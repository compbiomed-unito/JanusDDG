{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d0d193-11f5-4e42-b76f-99c49e887da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "from torch_geometric.utils import to_networkx\n",
    "#install required packages\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import Dataset\n",
    "import torch_geometric.utils as pyg_utils\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool,GATv2Conv\n",
    "from torch_geometric.nn.models import GCN, GAT\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch_geometric.utils import softmax\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import random\n",
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341de3e7-71a5-4d01-b23b-b03aa384584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.4.1+cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b180d05-7ebd-4b89-8213-2ee2a88debc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008404760555780284"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([0.7280, 0.7030, 0.7110, 0.715,0.72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4522ad10-397a-4b36-abe8-3c215b897876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)  # Python random\n",
    "    np.random.seed(seed)  # Numpy random\n",
    "    torch.manual_seed(seed)  # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU (un singolo dispositivo)\n",
    "    torch.cuda.manual_seed_all(seed)  # PyTorch GPU (tutti i dispositivi, se usi multi-GPU)\n",
    "    torch.backends.cudnn.deterministic = True  # Comportamento deterministico di cuDNN\n",
    "    torch.backends.cudnn.benchmark = False  # Evita che cuDNN ottimizzi dinamicamente (influisce su riproducibilità)\n",
    "\n",
    "# Imposta il seed\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd53fc53-6a66-4f06-aa44-c6e5ac8c52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "class DeltaDataset(Dataset):\n",
    "    def __init__(self, data, dim_embedding, inv = False):\n",
    "        self.data = data\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.inv = inv\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        if self.inv: \n",
    "            return {\n",
    "                'id': sample['id'],\n",
    "                'wild_type': torch.tensor(sample['mut_type'], dtype=torch.float32),    #inverto mut con wild \n",
    "                'mut_type': torch.tensor(sample['wild_type'], dtype=torch.float32),    #inverto mut con wild             \n",
    "                'length': torch.tensor(sample['length'], dtype=torch.float32),\n",
    "                'ddg': torch.tensor(-float(sample['ddg']), dtype=torch.float32),       # -ddg\n",
    "                #'alpha_vec': torch.tensor(-sample['alpha_vec'], dtype=torch.float32),  #-V\n",
    "                'pos_mut': torch.tensor(sample['pos_mut'], dtype=torch.int64),\n",
    "                #'hydra_slim': torch.tensor(sample['hydra_slim'], dtype=torch.int64),\n",
    "                }\n",
    "\n",
    "        else:\n",
    "            return {\n",
    "                'id': sample['id'],\n",
    "                'wild_type': torch.tensor(sample['wild_type'], dtype=torch.float32),\n",
    "                'mut_type': torch.tensor(sample['mut_type'],dtype=torch.float32),\n",
    "                'length': torch.tensor(sample['length'], dtype=torch.float32),\n",
    "                'ddg': torch.tensor(float(sample['ddg']), dtype=torch.float32),\n",
    "                #'alpha_vec': torch.tensor(sample['alpha_vec'], dtype=torch.float32),\n",
    "                'pos_mut': torch.tensor(sample['pos_mut'], dtype=torch.int64),\n",
    "                #'hydra_slim': torch.tensor(sample['hydra_slim'], dtype=torch.int64),\n",
    "\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37194b0e-d9a3-4928-aa15-b510cbe7a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "import random\n",
    "#from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = max(sample['wild_type'].shape[0] for sample in batch)  # Max sequence length in batch   700\n",
    "    max_features = max(sample['wild_type'].shape[1] for sample in batch)  # Max feature size\n",
    "\n",
    "    padded_batch = {\n",
    "        'id': [],\n",
    "        'wild_type': [],\n",
    "        'mut_type': [],\n",
    "        'length': [],\n",
    "        'ddg': [],\n",
    "        #'alpha_vec': [],\n",
    "        'pos_mut': [],\n",
    "        #'hydra_slim':[],\n",
    "    }\n",
    "\n",
    "    for sample in batch:\n",
    "        wild_type_padded = F.pad(sample['wild_type'], (0, max_features - sample['wild_type'].shape[1], \n",
    "                                                       0, max_len - sample['wild_type'].shape[0]))\n",
    "        mut_type_padded = F.pad(sample['mut_type'], (0, max_features - sample['mut_type'].shape[1], \n",
    "                                                     0, max_len - sample['mut_type'].shape[0]))\n",
    "\n",
    "        # hydra_slim_type_padded = F.pad(sample['hydra_slim'], (0, max_features - sample['hydra_slim'].shape[1], \n",
    "        #                                                0, max_len - sample['hydra_slim'].shape[0]))        \n",
    "\n",
    "        padded_batch['id'].append(sample['id'])  \n",
    "        padded_batch['wild_type'].append(wild_type_padded)  \n",
    "        padded_batch['mut_type'].append(mut_type_padded)  \n",
    "        padded_batch['length'].append(sample['length'])#append(torch.tensor(sample['length'], dtype=torch.float32))  \n",
    "        padded_batch['ddg'].append(sample['ddg'])#append(torch.tensor(float(sample['ddg']), dtype=torch.float32))\n",
    "        # padded_batch['hydra_slim'].append(hydra_slim_type_padded)\n",
    "        #padded_batch['alpha_vec'].append(sample['alpha_vec'])#append(torch.tensor(sample['alpha_vec'], dtype=torch.float32))  \n",
    "        #padded_batch['pos_mut'].append(sample['pos_mut'])#append(torch.tensor(sample['pos_mut'], dtype=torch.int64))  \n",
    "\n",
    "    # Convert list of tensors into a single batch tensor\n",
    "    padded_batch['wild_type'] = torch.stack(padded_batch['wild_type'])  # Shape: (batch_size, max_len, max_features)\n",
    "    padded_batch['mut_type'] = torch.stack(padded_batch['mut_type'])  \n",
    "    padded_batch['length'] = torch.stack(padded_batch['length'])  \n",
    "    padded_batch['ddg'] = torch.stack(padded_batch['ddg'])\n",
    "    # padded_batch['hydra_slim'] = torch.stack(padded_batch['hydra_slim'])\n",
    "    \n",
    "    #padded_batch['alpha_vec'] = torch.stack(padded_batch['alpha_vec'])  \n",
    "    #padded_batch['pos_mut'] = torch.stack(padded_batch['pos_mut'])  \n",
    "\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "\n",
    "def dataloader_generation(E_TYPE, train_path, validation_path, test_path, batch_size = 128, dataloader_shuffle = True, inv= False,sample_weights=None):\n",
    "    \n",
    "    EMBEDDING_TYPE = E_TYPE\n",
    "    \n",
    "    if EMBEDDING_TYPE == 'ESM2':\n",
    "\n",
    "        '''train formato da s2648 + UnionV e DA; 1000 dei DA sono usati nel validation insieme a s669 DA\n",
    "        '''\n",
    "        \n",
    "        dim_embedding = 1280\n",
    "        \n",
    "        dataset_train = []\n",
    "        dataset_validation = []\n",
    "        dataset_test = []\n",
    "\n",
    "        \n",
    "        for path in train_path:\n",
    "            with open(path, 'rb') as f:\n",
    "                dataset_train += pickle.load(f)\n",
    "        \n",
    "        for path in validation_path:\n",
    "            with open(path, 'rb') as f:\n",
    "                dataset_validation += pickle.load(f)\n",
    "        \n",
    "        for path in test_path:           \n",
    "            with open(path, 'rb') as f:\n",
    "                dataset_test += pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "    dataset_train = DeltaDataset(dataset_train, dim_embedding, inv = inv)  \n",
    "    dataset_test = DeltaDataset(dataset_test, dim_embedding, inv = inv)\n",
    "    dataset_validation = DeltaDataset(dataset_validation, dim_embedding, inv = inv)\n",
    "    print('ok fin qui')\n",
    "    \n",
    "    # # Creazione DataLoader\n",
    "    # #######\n",
    "    # sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    # dataloader_shuffle=False\n",
    "    ############\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=dataloader_shuffle, collate_fn=collate_fn)#, sampler=sampler)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=dataloader_shuffle, collate_fn=collate_fn)\n",
    "    dataloader_validation = DataLoader(dataset_validation, batch_size=batch_size, shuffle=dataloader_shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    return dataloader_train, dataloader_validation, dataloader_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b306576-bd45-49d6-88f3-c90a2a6a97f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok fin qui\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader  # Use standard PyTorch DataLoader\n",
    "import random\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "E_TYPE='ESM2'\n",
    "\n",
    "#WINDOWS\n",
    "# train_path = [f'Union_Varibench_S2648_fold_{i}_ESM2_LITE.pkl' for i in range(1,6)]\n",
    "# train_path_DA = [f'DA_th0.0_Union_Varibench_S2648_fold_{i}_ESM2_LITE.pkl' for i in range(1,6)]\n",
    "# validation_path = [f'Union_Varibench_S2648_fold_1_ESM2_LITE.pkl']+[f'DA_th0.0_Union_Varibench_S2648_fold_1_ESM2_LITE.pkl']\n",
    "\n",
    "# train_path = ['ddg_S2648_ESM2_ALL_LENGTH.pkl', 'ddg_DA_S2648_ESM2_ALL_LENGTH.pkl']\n",
    "# val_path = ['ddg_c2878_validation_ESM2_ALL_LENGTH.pkl']\n",
    "# test_path = ['ddg_s669_ESM2_ALL_LENGTH.pkl']\n",
    "folds = [0,1,2,3,4]\n",
    "\n",
    "Model_num = 0\n",
    "\n",
    "val_set = [folds[Model_num]]  # L'elemento corrente è il test set\n",
    "train_set = list(chain(folds[:Model_num], folds[Model_num+1:]))  # Tutti gli altri sono il training set\n",
    "\n",
    "\n",
    "#TRAIN PER JANUS300EPOCHE [f's2450_fold_{i}.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_inv.pkl' for i in [0,1,2,3,4]] #\n",
    "\n",
    "train_path = [f's2450_fold_{i}.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_inv.pkl' for i in [0,1,2,3,4]]#['DA_DOUBLE_s2450_sample2450.pkl']+[f's2450_fold_{i}.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_inv.pkl' for i in [0,1,2,3,4]] #+ ['ptmul_train.pkl']+['DA_s2450.pkl']#\n",
    "#[f's2450_fold_{i}.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_inv.pkl' for i in [0,1,2,3,4]]#['DA_th0.0_foldx_train.pkl'] + ['foldx_train.pkl'] + ['Double_mut_DA_0.0_foldx_train.pkl']+['test_TS16.pkl']#[f's2450_fold_{i}.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_inv.pkl' for i in [0,1,2,3,4]] + ['ptmul_train.pkl']+['DA_s2450.pkl'] #\n",
    "val_path = ['ptmul_test.pkl']#['s669_Castrense.pkl']#[f's2450_fold_{i}.pkl' for i in val_set]+[f's2450_fold_{i}_inv.pkl' for i in val_set]\n",
    "test_path = ['s669_Castrense.pkl']#['test_TS16.pkl']\n",
    "\n",
    "# ###################### PROVO A PESARE PER FREQUENZA\n",
    "# dataset_train_count=[]\n",
    "# for path in train_path:\n",
    "#     with open(path, 'rb') as f:\n",
    "#         dataset_train_count += pickle.load(f)\n",
    "\n",
    "# id_counts = Counter(sample['id'] for sample in dataset_train_count)\n",
    "\n",
    "# # Step 2: Calcola i pesi (inverso della frequenza)\n",
    "# weights = {id_: 1.0 / count for id_, count in id_counts.items()}\n",
    "\n",
    "# # Step 3: Crea una lista di pesi corrispondente all'ordine del dataset\n",
    "# sample_weights = torch.tensor([weights[sample['id']] for sample in dataset_train_count], dtype=torch.float)\n",
    "# ###########################\n",
    "\n",
    "\n",
    "\n",
    "#NOTA::: BATCH SIZE DI 6 DEVE ESSERE PER 0.55!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#!!!!!!!!!\n",
    "\n",
    "\n",
    "dataloader_train, dataloader_validation, dataloader_test = dataloader_generation(E_TYPE, train_path = train_path, validation_path = val_path,\n",
    "                                                                                 test_path = test_path, batch_size = 6,\n",
    "                                                                                 dataloader_shuffle = True, inv= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65e731d-c65c-4f99-88c6-c82520099f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAoaZJREFUeJzs3XVclecbx/HP4dBSJmKB3Z1MZ3fXnN26zZzYUzGnzsCYOWM6Z7fObrGwp5vdha1YiMT5/XH/BBmoBwSec+B6v17n5eHw8Jwvz5he3M9935fOYDAYEEIIIYRIJCy0DiCEEEIIEZekuBFCCCFEoiLFjRBCCCESFSluhBBCCJGoSHEjhBBCiERFihshhBBCJCpS3AghhBAiUbHUOkBCCwsL4969ezg6OqLT6bSOI4QQQggjGAwGXr58Sbp06bCw+PTYTJIrbu7du0fGjBm1jiGEEEKIWLh9+zYZMmT45DFJrrhxdHQE1MVxcnKK03MHBwezfft2qlatipWVVZyeW0SQ65wwzPE6HzhwgFq1apE9e3aOHz+udRyjmON1NldyrRNGfF3nFy9ekDFjxvB/xz8lyRU3729FOTk5xUtxY29vj5OTk/yPE4/kOicMc7zO9vb2AOj1+jj//zu+mON1NldyrRNGfF9nY6aUyIRiIUSiI/PphEjapLgRQgghRKKS5G5LCSESr9y5c7NkyRKzuSUlhIgfUtwIIRINV1dXmjVrpnUMzYWGhhIcHKx1DJMTHByMpaUlb9++JTQ0VOs4idaXXGdra+vPLvM2hhQ3QgiRSBgMBu7fv8/z58+1jmKSDAYDadOm5fbt2zIvKx59yXW2sLAgc+bMWFtbf1EGKW6EEInGo0eP8PX1xcnJicqVK2sdJ8G9L2zSpEmDvb29/AP+H2FhYbx69QoHB4c4GR0Q0YvtdX6/ya6/vz+ZMmX6op9fKW6EEInG2bNnadSoEfny5ePs2bNax0lQoaGh4YVNypQptY5jksLCwnj37h22trZS3MSjL7nOqVOn5t69e4SEhHzRMnL5ryuEEInA+zk27/f6EcIcvb8d9aVzojQtbvbv30+dOnVIly4dOp2OdevWffZr9u7dS5EiRbCxsSFbtmwsWLAg3nMKIcyDwWDQOoLm5FaUMGdx9fOraXHz+vVrChYsyPTp0406/vr169SqVYsKFSpw+vRpfvzxRzp27Mi2bdviOakQwpzIP/BCJG2azrmpUaMGNWrUMPr4WbNmkTlzZiZOnAioPS0OHDjApEmTqFatWnzFFEKYCRm5EUKAmU0oPnz4cJQVENWqVePHH3/86NcEBQURFBQU/vGLFy8AdX86rveBeH8+2V8ifsl1ThjmeJ1DQkLCn5tL7ri6zsHBwRgMBsLCwggLC4uLaCahXbt2PH/+nLVr137xud4Xv++vU0xlyZKFnj170rNnz48eo9frWb16NfXr1w9/bd68eaxYsSLe7zL891pVrFiRggULMmnSpHh93//61HX+6quv6N27N40aNYr2a8PCwjAYDAQHB6PX6yN9Lib/j5hVcXP//n1cXV0jvebq6sqLFy8IDAzEzs4uyteMGTOG4cOHR3l9+/btcTrx7t07Cx4/tsPe3prNm3dhZZV4/nIxVTt27NA6QpJgTtf577//BuDly5ds3rxZ4zQx86XX2dLSkrRp0/Lq1SvevXsXR6m0N2LECCDiF9O48PLly1h93c6dO7G3t/9slsDAwPBj3r59i7e3N7///nucfg/RCQ4OJiQkJPx9fv/9dywtLY1639q1a5M/f37GjBkT6/dfsmQJXbt2jfSajY0N9+/fD//4xx9/ZMCAAVSqVCnalVTv3r0jMDCQ/fv3R/plBeDNmzdGZzGr4iY2Bg4ciJeXV/jH71umV61aNU63aD95Epo0iVi2ZmNjwNkZnJzAySniubMzODsbIj13doaUKSFFCgMpU6rnNjZxFi3RCQ4OZseOHVSpUkU6+8Yjc7zOBQsWJG3atLi4uFCzZk2t4xglrq7z27dvuX37Ng4ODtja2sZhQm3F5d/TBoOBly9f4ujoGKt5WcZmsbOzCz92w4YNODs7U7Vq1Ri/X0xZWVlhaWkZ/t4xuXaWlpZYW1t/0fW2tbXFycmJc+fORdrn5sNzNmrUiB9//JGDBw9Sq1atKOd4+/YtdnZ2lC1bNsrPcUyKQ7MqbtKmTcuDBw8ivfbgwQOcnJyiHbUBVTXaRFMpWFlZxelf2CEh4OBg4NUr9T9MUJCOhw/h4UOAmP9P5OAAqVKpQuf9n++fu7qCmxukTav+dHWFL9zM0SzF9X9DET1zus4eHh507txZ6xix8qXXOTQ0FJ1Oh4WFRfhvxAYDxOCX3Thjbw8xqR1WrVrF8OHDuXLlCvb29hQuXJj169eTLFky2rZty/Pnz8NX0758+ZLvv/+edevW4eTkRL9+/Vi/fj2FChVi8uTJgPo56NixI5cuXWLNmjWkTJmSX3/9lZIlS9KuXTv2799PlixZmD9/PsWKFQvPsXr1ary9vbly5Qpubm50796d3r17h3/ew8ODH3/8MXwqxOXLl+nQoQNHjx4lS5YsTJkyBSDSf4MVK1ZQp06d8I/fvn1L0aJFKV26NL/99hsAV69epVChQkyZMoX27dsbdc1CQ0Pp27cv8+fPR6/X06FDB4DwnwGA8uXLR7ouM2bMYNKkSdy+fRtnZ2e+/vprVq1aRdu2bdm3bx/79u1j6tSpgFrA4+HhYfx/xP9/3zqdDjc3N168eIGTk1OU0RkLCwtq1qwZfl0+do7o/n+Iyf8fZlXceHp6Rhlq3rFjB56enholilC6NDx9GsLGjZv5+uuavHljRUAAkR4vXhDltYAAeP4cnjyJeISFwatX6nHjhnHvnzJlRLHz/s/06cHdHTJlUo+UKWP2F44Qwry9eaN+UUpor15BsmTGHevv70+zZs0YN24cDRo04OXLl/j6+n50criXlxcHDx5kw4YNuLq64u3tzcmTJylUqFCk4yZNmsTo0aMZMmQIkyZNolWrVnh6etK0aVN8fHwYOHAgrVu35t9//0Wn03HixAmaNGnCsGHD+Pbbbzl06BBdunQhZcqUtG3bNkqOsLAwGjZsiKurK35+fgQEBEQ7//PAgQO0atUq/GNbW1sWL15MyZIlqVWrFrVr16Zly5ZUqVLF6MIGYOLEiSxYsID58+eTO3duJk6cyNq1a6lYsWK0xx8/fpwePXqwaNEivvrqK54+fYqvry8AU6ZM4dKlS+TLly/8NmDq1KkBcPjMD1DLli2ZNWtW+MevXr0ic+bMhISEULRoUcaMGUPevHkjfU2JEiUYO3as0d9rbGha3Lx69YorV66Ef3z9+nVOnz5NihQpyJQpEwMHDuTu3bv88ccfAHz//fdMmzaNfv360b59e3bv3s2KFSvYtGmTVt9ChNev0a1cSapHj3Ap40lq9zSxOk1YmCp4njyBx4+j/vn4MTx4APfvg7+/+jM4OKIw+vffj5/bzi6i0Hn/yJwZsmdXjxQppPgR5u3x48ecOHECR0dHvvrqK63jCCP4+/sTEhJCw4YNcXd3ByB//vzRHvvy5UsWLlzIkiVLqFSpEqDmlaRLly7KsTVr1uS7774DwNvbm5kzZ1K8eHHq16+Pk5MT/fv3x9PTkwcPHpA2bVp8fHyoVKkSQ4YMASBHjhycO3eO8ePHR1vc7Ny5kwsXLrBt27bw9x89enSkFcDPnz8nICAgSr5ChQoxatQoOnbsSNOmTbl58yZ//fVXjK7b5MmTGThwIA0bNgTUauJPTVi+desWyZIlo3bt2jg6OuLu7k7hwoUBcHZ2xtraGnt7e9KmTRvp606fPv3JHB/ecsqZMyfz588nX758+Pv7M3PmTL766iv+/fdfMmTIEH5cunTpuH37NmFhYfG2U7Smxc3x48epUKFC+Mfv58a0adOGBQsW4O/vz61bt8I/nzlzZjZt2kSvXr2YMmUKGTJkYO7cuaaxDPzCBSzbtaMsQL9+apgkR46IyuHD55+ohC0sIHly9ciW7fNvGxYGz55FFDof/nnnDty8CbduqdcCA+HiRfWIjouLipctW0TUbNkgVy71OSFM3cmTJ6levTqFChXi1KlTWsfRnL29GkXR4n2NVbBgQSpVqkT+/PmpVq0aVatWpXHjxiRPnjzKsdeuXSM4OJgSJUqEv+bs7EzOnDmjHFugQIHw5+8XouTLly/Kaw8fPiRt2rScP3+eevXqRTpH6dKlmTx5MqGhoVFW7pw/f56MGTNGKlz+exchMDAQINo5UL1792bdunVMmzaNLVu2xKhlRkBAAP7+/pQsWTL8NUtLS4oVK/bREa8qVarg7u5OlixZqF69OtWrV6dBgwafXViTzZh/iP7P09MTT09PwsLCyJIlC5UrVyZv3rzMnj2bkSNHhh9nZ2dHWFgYQUFBH51S8qU0LW7Kly//yX0pott9uHz58qb5l1ZYGGHlyhH0zz/YvR9GOXxYPf4rQwbIn1898uVTf+bOHatZxBYWEfNxPvj/NoqgIFXs3LqlHjdvqse1a3Dlivrc8+dw7Jh6/Ff69Or8+fJB3rzqzzx5jB96FkIkPJ3O9P8f1ev17Nixg0OHDrF9+3Z+/fVXBg0ahJ+fH5kzZ471eT+cn/F+8nB0r8XnsvmUKVOi0+l49uxZlM89fPiQS5cuodfruXz5MtWrV4+3HACOjo6cPHmSvXv3sn37dry9vRk2bBjHjh3D5RO/vcb0ttSHrKysKFy4cKQ7NABPnz4lWbJk8VbYgJnNuTFpxYsTumMH2zdvpma5cljdvAmXLsHly5H/fPxYVRJ37sCWLRFfr9er0Z18+aBwYShWTD2i+e0lNmxsIGtW9YjOmzdw9aoqdC5fjvjz8mW4ezfi8eGop06nbmsVLBgRt2hRVWgJoQXZxM886XQ6SpcuTenSpfH29sbd3Z21a9dGWukKap8ZKysrjh07RqZMmQA1inHp0iXKli37RRly587NwYMHI7128OBBcuTIEWXU5v3xt2/fxt/fHzc3NwCOHDkS6Rhra2vy5MnDuXPnoqyWat++Pfnz56dDhw506tSJypUrkzt3bqOyOjs74+bmhp+fX/j3HRISwokTJyhSpMhHv87S0pLKlStTuXJlhg4diouLC7t376Zhw4ZYW1tH288pJrel/is0NJSzZ89GWbn4zz//hN8Siy9S3MSHZMnUv/gFC0b93LNnamLMP//A2bMRj+fP4fx59Vi5MuL4rFlV1VC8eEQFEQ+/itnbRwwm/VdAAJw7pyJ/+Hj4UI38XLsGH+6v5eEREfV99DhczSnEZ0n7BfPh5+fHrl27qFq1KmnSpMHPz49Hjx5F+w+9o6Mjbdq0oW/fvqRIkYI0adIwdOjQ8BU2X6J3794UL16ckSNH8u2333L48GGmTZvGjBkzoj2+cuXK5MiRgzZt2jB+/HhevHjBoEGDohxXrVo1Dhw4EGmy8fTp0zl8+DBnzpwhY8aMbNq0iRYtWnDkyJHwxpGf07NnT8aOHUv27NnJlSsXPj4+PH/+/KPH//XXX1y7do2yZcuSPHlyNm/eTFhYWPgtPQ8PD/z8/Lhx4wYODg6kSJECCwuLGN2WGjFiBKVKlSJLlizcvXuXmTNncvPmTTp27BjpOF9f33hfGi/FTUJLnhzKlFGP9wwGuHcvotA5cQKOH1dDKe8fy5erYy0toUiRiHOUKQP/n9UeX5ydwdNTPT708KEqck6dUnGPH1cjPjduqMeqVeo4CwsoUEBF/fpr9Wc08/+E+GIycmN+nJyc2L9/P5MnT+bFixe4u7szceLEj7bm8fHx4fvvv6d27drhS8Fv3779xXv7FClShBUrVuDt7c3IkSNxc3NjxIgR0U4mBrVkee3atXTo0IESJUrg4eHB1KlTo9xe6tChA8WKFSMgIABnZ2cuXLhA3759mTdvHhkzZgTUEu0CBQowZMgQfvnlF0AV6L///vtH37937974+/vTpk0bLCwsaN++PQ0aNCAgICDa411cXFizZg3Dhg3j7du3ZM+enaVLl4avZOrTpw9t2rQhT548BAYGxmop+LNnz+jUqRP379/HxcWFokWLcujQIfLkyRN+zN27dzl06BB//vlnjM4dY4YkJiAgwAAYAgIC4vzc7969M6xbt87w7t27uDnhkycGw/btBsPo0QZDw4YGQ/r0BoMqhSI/cuY0GDp2NBiWLjUYHj6Mm/eOpWfPDIZduwyGX34xGJo0MRg8PKKPnDmzwdCqlcGwYIHBcPduzN4jzq+ziJY5XuctW7YYAEORIkW0jmK0uLrOgYGBhnPnzhkCAwPjKJl5ePXqlcHZ2dkwd+7czx4bGhpqePbsmSE0NDQBkkVo3LixYfTo0UYff+3aNYOlpaXh0qVL8Zgq/nzqOvfr18/QqVOnj37tp36OY/Lvt4zcmLIUKaBKFfUAVRfcugUHDkQ8/vknYgnU3LnquMKFI76uTBlIwN1KXVygYkX1eO/ePTh4UMX19YW//4br19Vj0SJ1TN68Km7VqlC2rOlPghRCaOPUqVNcuHCBEiVKEBAQEL4vy39XOpmS8ePHs3HjRqOP37x5M507dyZ79uzxmEobadKkiTKXKj5IcWNOdDq1K5+7O7RooV57+hQOHYK9e2HHDjhzRt0nOnUKxo1ThU25clC3rnp8sNdAQkmXDr75Rj1AbWZ45Ajs26ciHz+upiH9+y9Mnqx2Wy5dGurUgXr1IEuWBI8szFTu3LmZMmVK+AZkInGaMGECFy9exNramqJFi+Lr60uqVKm0jvVRHh4edO/e3ejj/9ufKTH5cMfn+CTFjblLkQJq11YPUBva7NypqoYdO9SGN9u2qUfXrmq+Tr16qtApWFCTXfucnNQITdWq8PPPatX87t0q7vbtaon6nj3q4eWlFpDVratif7BTuhBRuLu706NHD61jiHhUuHBhTpw4oXUMYeKkuIkjBoOBcYfG4RSo8bKgtGmhZUv1MBjUcMimTbBhg9pz5+RJ9Rg6VA2JNG2qHvnyabY9ccqUESM7BoOalLx5s4q8b1/E6qzRo9V+O998Y0H69C7I3FEhhBDRiZ99j5Ogo3ePMnjvYHpc7EHxecWZeGgi917e0zaUTqeKlv791aQXf3+YN08Ng9jZqTXco0erpUz58sHIkWpjG40jZ88OPXvCrl3w6BH8+acqfBwc1F47kyfr6du3HLlzWzJokFpgJgQQ3i/HJDf6FEIkGClu4oiV3oo6OepgqbPk7wd/02dHHzL4ZKDKoiosPL2Q1+9eax1RtQ9v3x7Wr1dVw7JlUL++muRy7hx4e6uNBEuXVkXQy5daJyZ5cjW9aMUKtf/hunXQpEkYNjYhXLumC6/NCheGadPUFCSRdB05coSyZcuabWdwIUTckOImjhRxK8LqxquZn3c+06pPo3TG0hgwsPPaTtqub4vbRDe6bOrC3/f/1jqqkiwZfPut2n3vwQNYsACqV1c7JR86BB07qtbi7durUR8TuAdkY6Pm3fz5ZygLF27lzz9Dwmuz06ehe3c1eblZMzV/Jx53VRdCCGHCpLiJY06WTnQu0pkD7Q9wtcdVRlYYSdbkWXn57iUzj8+k0OxClJpbivmn5pvGaA6o9dtt2qh2ELdvw9ixagTn9Wv4/Xe1nDxPHpgxQ5sufNGwtQ2lSRMDa9equ21Tp6r50UFBakCqalW1ufP48TKak5QYTKAIF0JoT4qbeJQleRYGlx3Mpe6X2NlqJ9/k+QZLC0v87vrRYUMHMkzKQP8d/bn74q7WUSO4uak5OhcuqE1p2rVTozwXLqjVVunTqyVMV69qnTRcihRq1ObUKbW5c5cualflGzdUg/YMGeD779XcapE0SPuFxKNt27bUr19f6xiAWtI9efLkTx6j0+lYt25dpNfmzZsX7+0GIOq1Kl++fKS2D6agVKlSrF69Ot7fR4qbBGChs6BSlkqs+GYFd3rdYWylsWRJnoXnb58z7tA4PKZ40GptK07fP6111Ag6nRqxmT9f7cL3669qNOfFC5g0Sc36rVNH7a9jIr8t63Rqpfv06Wo0Z/58NZoTGAizZ6s505Urq8VjJhJZxDEZuUl8pkyZwoIFC7SOAcCxY8diPJ/r7du3DBkyhKFDh8ZTqo9bs2YNI0eONOrYuC6EVq9ejV6vj1KYDh48mAEDBsRrN3aQ4ibBuTq40r9Mfy53v8z6pusp616WkLAQ/jzzJ4VnF6bSH5XYfnW7af0l7eQE3bqppp5btkCNGqo6+OsvqFBBTUA2sYrBzk4NOp06pZaTN2qkelzt2qW2BCpcWE1SjqYJrkgEZOQm8XB2dsbFxUXrGACkTp0ae3v7GH3NqlWrcHJyonTp0vGU6uNSpEiBo6Njgr/vjRs38Pb25uuvv47yuRo1avDy5Uu2bNkSrxmkuNGIhc6Cujnrsq/tPo51OkazfM3Q6/Tsvr6ban9W46v5X7HtyjbTKnIsLNSk482bVbuHH35Qs3wPHzbZikGnU+0cVq1SK9/79FFLyv/+W82nzptXzaUODtY6qRBJ16pVq8ifPz92dnakTJmSypUr8/q1mpP431stL1++pEWLFiRLlgw3NzcmTZoUZdTBw8ODUaNG0bp1axwcHHB3d2fDhg08evSI5s2b4+TkRIECBTh+/HikHKtXryZv3rzY2Njg4eHBxIkTI33+v7elLl++TNmyZbG1tSVPnjzs2LEjyve2bNky6tSpE/7x27dvyZs3b6QRoKtXr+Lo6Mj8+fONvmahoaF4eXnh4uJCypQp6devX5R/L/57XWbMmEH27NmxtbXF1dWVxo0bA+oa79u3jylTpqDT6dDpdNy4ccPoLP/N1apVKwYMGEDmzJmjfF6v11OzZk2WLVsWq/MbS4obE1AsXTGWNFrCtZ7X+LHkj9ha2nLkzhGqL65umkUOqFtUM2aoBlF9+0atGJYtM7nlSu7uaoLxzZswbJhaZn7xohrhyZYN5syBkBCtU4ovkStXLkaPHs0PP/ygdRTTYDCohQEJ/YjB31f+/v40a9aM9u3bc/78efbu3UvDhg0/+neel5cXBw8eZMOGDezYsQNfX19OnjwZ5bhJkyZRunRpTp06Ra1atWjVqhVt2rShSZMmHD9+nKxZs9K6devw9zlx4gRNmjShadOmnD17lmHDhjFkyJCP3hILCwujYcOGWFtb4+fnx6xZs+jfv3+U4w4cOECxD7ZWt7W1ZfHixSxcuJD169cTGhpKy5YtqVKlCu3btzf6uk2cOJEFCxYwf/58Dhw4wNOnT1m7du1Hjz9+/Dg9evRgxIgRXLx4ka1bt1K2bFlA3frz9PSkU6dO+Pv74+/vH96x3MHB4ZOP77//PtL7jBgxgtSpU9OqVauPZilRogS+vr5Gf6+xYlSLz0TEHLqC+7/0N3ht9TLYjbIzMAwDwzCUnFPSsPva7jhKGg+ePDEYhg41GJInj2j9XaiQwbB5s8EQFhanbxVX1/nFC4Nh3DiDwdU1InKOHAbDihUGQwI3DTZJ5tgV3BzFa1fwV68ifrgT8vHqldG5T5w4YQAMN27ciPbzbdq0MdSrV89gMBgML168MFhZWRlWrlwZ/vnnz58b7O3tDT179gx/zd3d3dCyZcvwj/39/Q2AYfDgweHdqg8fPmwADP7+/gaDwWBo3ry5oUqVKpHeu2/fvoY8efJEOu+kSZMMBoPBsG3bNoOlpaXh7t274Z9/35V+7dq1BoPBYHj27JkBMOzfvz/K9zVu3DhDqlSpDN26dTO4ubkZHj9+/PmL9QE3NzfDuHHjwj8ODg42ZMiQIfxaGQwGQ7ly5cKvy+rVqw1OTk6GFy9eRHu+D4/90OXLlz/5ePDgQfixvr6+hvTp0xsePHhgePbsmaF169aR8ry3fv16g4WFRbRdw+OqK7iM3JigtA5pmVhtItd6XsOrlBd2lnb43fWj4h8Vqb2kNv8+NMFlPylSqOGQmzdhxAg1T+f0aahZUzXuPHxY64RRODqqQafr19Uc6VSp4NIlaNIEihdXfa6EEPGrYMGCVKpUifz58/PNN98wZ84cnj17Fu2x165dIzg4mBIlSoS/5uzsTM6cOaMcW6BAgfDnrq6uAOTLly/Kaw8fPgTg/PnzUebFlC5dmsuXLxMaza328+fPkzFjRtKlSxf+mqenZ6RjAgMDATVa81+9e/cmR44cTJs2jfnz55MyZcpov+foBAQE4O/vT8mSJcNfs7S0jDRC9F9VqlTB3d2dLFmy0KpVKxYvXsybN28++17ZsmX75CNNmjSAul3YqlUr5syZ89kmpnZ2doSFhREUFGTkdxxzUtyYsA+LnK7Fu2JpYcmmy5soMKsAnTZ00r69Q3QcHWHIkIgJLra2akn5V1+p3fVu3dI6YRR2dvDjjyrysGHqWzh5EqpVg1q11K0rYR6eP3/OyZMnuSj/0RR7e7U3VUI/YjDpVq/Xs2PHDrZs2UKePHn49ddfyZkzJ9evX/+ib93Kyir8+fsJ5tG9Fp+rdlKmTIlOp4u2WHv48CGXLl1Cr9dzOQHa3jg6OnLy5EmWLl2Km5sb3t7eFCxYkOfPn3/y64y9LXX16lVu3LhBnTp1sLa2JlWqVCxatIgNGzZgaWnJ1Q+2D3n69CnJkiXDzs4u3r5fKW7MQFqHtEyrOY1/u/xLw9wNCTOEMffUXHJOy8n4g+N5F/pO64hRpUypJrhcvgwdOqiZvcuWQa5cqmnnaxPZwPADjo4q2tWrqtixtFRzp/PlU1v7fObvAGEC9u3bR9GiRWnbtq3WUUyDTqf2qUroRwxXq+l0OkqXLs3w4cM5deoU1tbW0c4fyZIlC1ZWVhw7diz8tYCAAC5duvTFlyp37twcPHgw0msHDx4kR44c6PX6aI+/ffs2/v7+4a8dOXIk0jHW1tbkyZOHc+fORfn69u3bkz9/fhYuXEj//v05f/680VmdnZ1xc3PDz88v/LWQkJDPdku3tLSkcuXKjBs3jjNnznDjxg12794dnjW6EarTp09/8jFixAhAzXc7e/Ysp0+f5uTJk+zfv586depQoUIFTp8+HT6HB+Cff/6hcOHCRn+/sSHFjRnJkTIHq5us5kC7A5RMX5JX717Rb2c/Cs4qyK5ru7SOF70MGWDuXLW7XtmyatOZESMgZ061hMnUJkoDqVOr21T//qsWgYWERGztM2+eyc2TFsKs+fn5MXr0aI4fP86tW7dYs2YNjx49Infu3FGOdXR0pE2bNvTt25c9e/bw77//0qFDBywsLL54+X/v3r3ZtWsXI0eO5NKlSyxcuJBp06bRp0+faI+vXLkyOXLkoE2bNvz999/4+voyaNCgKMdVq1aNAwcORHpt+vTpHD58mIULF9KiRQvq169PixYtePfO+F9Ue/bsydixY1m3bh0XLlygS5cunxyF+euvv5g6dSqnT5/m5s2b/PHHH4SFhYXf0vPw8MDPz48bN27w+PHj8BEtY29L2draki9fvvBHnjx5cHFxwdHRkXz58mFtbR2exdfXN943NZTixgyVzlSaQx0OMb/ufFLbp+bC4wtUXlSZJiubmOatKlDLxPfuVQWNh4dq7/3NN2ojwFguOYxvOXLAxo2wdSvkzq0ad3bsCOXLqz6jwvQYTLBYFp/m5OTE/v37qVmzJjly5GDw4MFMnDiRGjVqRHu8j48Pnp6e1K5dm8qVK1O6dGly584d7byWmChSpAgrVqxg2bJl5MuXD29vb0aMGPHRUUALCwvWrl1LYGAgJUqUoGPHjvz8889RjuvQoQObN28mICAAgAsXLtC3b19mzJgRPpoxY8YMHj9+zJAhQ8K/TqfTfXLzwt69e4evAPP09MTR0ZEGDRp89HgXFxfWrFlDxYoVyZ07N7NmzWLp0qXkzZsXgD59+qDX68mTJw+pU6fmVjxNIbh79y6HDh2iXbt28XL+cJ+dcpzImMNqqZh4FvjM0H1zd4PFcAsDwzA4j3E2zDkxxxAWxyuU4lRgoMHg7W0wWFmplRX29mrZkpHXTYvr/O6dwTBhgooKKvqgQQbDmzcJFiHBmeNqqbVr1xoAg6enp9ZRjBavq6WSgFevXhmcnZ0Nc+fO/eyxoaGh4aulElLjxo0No0ePNvr4a9euGSwtLQ2XLl2Kx1Tx51PXuV+/foZOnTp99GtltZQAwMXWhak1pnKy80lKpC9BQFAAnTZ2otIflbj61HT6P0ViawvDh8OZM2ol1Zs3qglUiRLqNRNkZQW9e6sRm9q11aZ/P/8MBQrAf0achYYMMnKT6J06dYqlS5dy9epVTp48SYsWLQCoV6+exsk+bvz48Tg4OBh9/ObNm+ncuTPZs2ePx1TaSJMmjdEtIb6EFDeJRMG0BTnU/hA+VX2ws7Rjz4095J+ZH5/DPoQZTHSSSK5csGeP6jyeMqVaOl6smKoaTHQ3PXd32LABVq+GdOngyhU1lahPH3j7Vut04j1pv5C4TZgwgYIFC4bvZOzr6/vZ5cda8vDwoHv37kYf37VrV6ZPnx6PibTTu3fv8GX48UmKm0REb6Gnl2cv/unyDxUzVyQwJJDe23tT+Y/K3A64rXW86Ol00Latmr1bv74aEhk8WC0dj8HqgYSk00HDhipy27ZqTvTEiapp5weLOIQQ8aBw4cKcOHGCV69e8fTpU3bs2EH+/Pm1jiVMjBQ3iVCW5FnY2Wons2vPxt7KPnwUZ9k/8dvL44u4usKaNbBoEbi4qCqhcGHVjdxEbzW4uKhBpw0bVPzz58HTU91xM6H2WklKzpw5GTx4MG3atNE6ihBCQ1LcJFI6nY7ORTtz+rvT4XNxmq1uRos1LQh4G6B1vOjpdNCyJfzzj+o8HhQEPXqoEZ0nT7RO91F16qhRnKZNVVEzbBhUqgR37midLOnJkycPI0eOjNSUUAiR9Ehxk8hlT5mdA+0OMLTcUPQ6PUvOLqHIb0U45X9K62gflz49bNoEU6eCtbUaGilYEPbv1zrZR6VMCUuXwp9/qh6i+/ZBoULw119aJxNCiKRHipskwEpvxbDywzjQ/gDuzu5ce3YNz3mezD4+23RXl+h00L07+PmpDf/u3oUKFdQGgCa8i16LFqp1Q5EiarCpTh3o1QtisDeX+AIvX77k4sWL8bZHhxDCPEhxk4SUylCKk9+dpHaO2gSFBvH9pu9ptbYVr9690jraxxUqBMePq5m7YWEwdCj6Bg2wemW6mbNnh0OHVFEDMHkyVKwIH+zSLuLJ9u3byZUrV/jyYCFE0iTFTRKTwi4F65uuZ1zlceh1ehafXUyJOSW4/CT+G7fFmoODmrm7cCHY2mKxZQtl+/SBs2e1TvZRNjbg46PuqDk7w8GDULSoKnqEEELELylukiALnQV9S/dlb9u9pHNMx/nH5ykxtwQ7ru7QOtqntW4Nhw5h8PDA4f59LL/+WjXjNGF16qiFX3nzqpGb8uVh5kyTXQBm9kz2NquItbZt21K/fn2tYwBqv5rJkyd/8hidTse6desivTZv3rx476UEUa9V+fLl+fHHH+P9fWOiVKlSrF69Ot7fR4qbJKxMpjIc73ScUhlK8fztc2osrsFUv6mm/Q9E4cKEHD7Mw0KF0L15A82awcCBJj0PJ3t2OHJEtdIKDoYuXaBzZ/VcxA/ZxC/xmDJlyid7LCWkY8eOxXgl3tu3bxkyZAhDhw6Np1Qft2bNGqN3A46LQmjNmjUUK1aMFClSkD59eooUKcKiRYsiHTN48GAGDBgQ3pgzvkhxk8S5Obqxp80e2hRsQ6ghlJ5be9JxQ0eCQoK0jvZxKVNyeMgQQvv2VR+PHQtNmqg2DibKwQGWL4fx48HCQjVKr14dnj3TOlniYtKFuYgVZ2dnXFxctI4BQOrUqbG3t4/R16xatQonJydKly4dT6k+LkWKFDg6Oibo+w0aNIiDBw9y4MAB2rZtS7t27di2bVv4MTVq1ODly5ds2bIlXrNIcSOwtbTl93q/M7HqRCx0Fsw/PZ9qf1bj+dvnWkf7OL2esJ9/Vpv+WVurfgjly8P9+1on+yidTrVp2LBBFTu7d6uNmK9d0zpZ4iMjN+Zl1apV5M+fHzs7O1KmTBneVgGi3mp5+fIlLVq0IFmyZLi5uTFp0qQoow4eHh6MGjWK1q1b4+DggLu7Oxs2bODRo0c0b94cJycnChQowPHjxyPlWL16NXnz5sXGxgYPDw8mTpwY6fP/vS11+fJlypYti62tLXny5GHHjqi39pctW0adOnXCP3779i158+aNNAJ09epVHB0dmT9/vtHXLDQ0FC8vL1xcXEiZMiX9+vWLUtz/97rMmDGD7NmzY2tri6urK40bNwbUNd63bx9TpkxBp9Oh0+m4ceOG0Vk+fL8GDRqQO3duMmfOTI8ePShQoAAHPmjAp9frqVmzJsvieUqBFDcCUP8YeHl6san5JhytHdl3cx9l5pfhVoCJL6lt2RJ27lQbzRw7BiVLmvREY4BatVSzzQwZ4MIFFVkmGov4YDAYeP3udYI/YjKC5u/vT7NmzWjfvj3nz59n7969NGzY8KPn8PLy4uDBg2zYsIEdO3bg6+vLyZMnoxw3adIkSpcuzalTp6hVqxatWrWiTZs2NGnShOPHj5M1a1Zat24d/j4nTpygSZMmNG3alLNnzzJs2DCGDBny0VtiYWFhNGzYEGtra/z8/Jg1axb9+/ePctyBAwcoVqxY+Me2trYsXryYhQsXsn79ekJDQ2nZsiVVqlShffv2Rl+3iRMnsmDBAubPn8+BAwd4+vQpa9eu/ejxx48fp0ePHowYMYKLFy+ydetWypYtC6hbf56ennTq1Al/f3/8/f3JmDEjAA4ODp98fP/999G+n8FgYNeuXVy8eDH8fd4rUaIEvr6+Rn+vsWEZr2cXZqd6tur4tvOl5pKa/PvoXzznebK5+WYKpi2odbSP+/prNamlVi24dAlKl1bDI+XLa53sowoWVFv41Kmj9sWpWFFtAtiggdbJzFv27Nnp1asXWbJk0TqKSXgT/AaHMcZ3o44rrwa+Ipl1MqOO9ff3JyQkhIYNG+Lu7g7w0V5RL1++ZOHChSxZsoRKlSoB8Pvvv5MuXboox9asWZPvvvsOAG9vb2bOnEnx4sWpX78+Tk5O9O/fH09PTx48eEDatGnx8fGhUqVKDBkyBIAcOXJw7tw5xo8fT9u2baOcf+fOnVy4cIFt27aFv//o0aOpUaNG+DHPnz8nICAgSr5ChQoxatQoOnbsSNOmTbl58yZ/xXDHz8mTJzNw4EAaNmwIwKxZsyLd/vmvW7dukSxZMmrXro2joyPu7u4ULlwYULf+rK2tsbe3J23atJG+7vTp05/M4eTkFOnjgIAA0qdPT1BQEHq9nhkzZlClSpVIx6RLl47bt28TFhaGhUX8jLHIyI2IomDaghzucJg8qfNw7+U9vv79a3Ze26l1rE/Llg0OH1Ytul++VBNa/rNiwdSkS6c2Xa5bV3WaaNwY5s3TOpV5K1SoED4+PnTr1k3rKMJIBQsWpFKlSuTPn59vvvmGOXPm8Owjk9GuXbtGcHAwJUqUCH/N2dmZnDlzRjm2QIEC4c/fd6HOly9flNcePnwIwPnz56PMiyldujSXL18mNJpmcefPnydjxoyRChdPT89IxwQGBgJqtOa/evfuTY4cOZg2bRrz588nZcqU0X7P0QkICMDf35+SJUuGv2ZpaRlphOi/qlSpgru7O1myZKFVq1YsXryYN0bMU8yWLdsnH2nSpIl0vKOjIydPnmT37t2MGjUKLy8v9u7dG+kYOzs7wsLCCAqKv7mdMnIjopXJORMH2h2gwfIG7Lu5j5qLa7K00VIa5WmkdbSPS5ECtm1TK6jWrYNGjeC336BDB62TfVSyZGq60Pffq8KmY0d4/Bj69VNzdIT4EvZW9rwamPAbXtpbGT/pVq/Xs2PHDg4dOsT27dv59ddfGTRoEH5+fmTOnDnWGaysrMKfv5+DFd1r8blqJ2XKlOh0umiLtYcPH3Lp0iX0ej2XL1+mevXq8ZYDIoqOvXv3sn37dry9vRk2bBjHjh375IRtB4dPj/y1bNmSWbNmhX9sYWERXvSULl2aCxcuMGbMGMp/MJL+9OlTkiVLhp2d3Zd+Wx8lIzfio5LbJWdby200yduE4LBgmqxqwsLTC7WO9Wm2trBypSpowsJUtfDLLya9sYylJcyZA+9v1w8YAH37mnRkk/XmzRtu374d/tt4UqfT6UhmnSzBHzGd0K3T6ShdujTDhw/n1KlTWFtbRzt/JEuWLFhZWXHs2LHw1wICArh06dIXX6vcuXNz8ODBSK8dPHiQHDlyoNfroz3+9u3b+H+w9fiRI0ciHWNtbU2ePHk4d+5clK9v3749+fPnZ+HChfTv35/z588bndXZ2Rk3Nzf8/PzCXwsJCeHEiROf/DpLS0sqV67MuHHjOHPmDDdu3GD37t3hWaMboTp9+vQnHyNGjPjke0Y3QvPPP/+E3xKLLzJyIz7JxtKGJQ2X4GDlwPzT82m7vi2vg1/TpXgXraN93PtqIVUqVdgMGKDWXI8ZY7LDITqdWtGeOrVaUTVxoor8228Qzd+r4iM2btxI06ZNqVChQvhf2sK0+fn5sWvXLqpWrUqaNGnw8/Pj0aNH5M6dO8qxjo6OtGnThr59+5IiRQrSpEnD0KFDsbCw+OIVcr1796Z48eKMHDmSb7/9lsOHDzNt2jRmzJgR7fGVK1cmR44ctGnThvHjx/PixQsGDRoU5bhq1apx4MCBSKuWpk+fzuHDhzlz5gwZM2Zk06ZNtGjRgiNHjmBtbW1U3p49ezJ27FiyZ89Orly58PHx4fnz5x89/q+//uLatWuULVuW5MmTs3nzZsLCwsJv6Xl4eODn58eNGzdwcHAgRYoU4aMwxhozZgzFihUjc+bMPHnyBF9fXxYtWsTMmTMjHefr6xvvmxrKyI34LL2Fnjl159CzZE8Aum7uyi8HftE41We8rxYmTFAf//KLWQyH9O4NCxaogmb+fGjfHqL5ZUqIRMPJyYn9+/dTs2ZNcuTIweDBg5k4cWKkibkf8vHxwdPTk9q1a1O5cmVKly5N7ty5o53XEhNFihRhxYoVLFu2jHz58uHt7c2IESOinUwM6vbL2rVrCQwMpESJEnTs2JGff/45ynEdOnRg8+bNBAQEAHDhwgX69u3LjBkzwlckzZgxg8ePH4dPZgY1mvWpzQt79+4dvgLM09MTR0dHGnxiRYKLiwtr1qyhYsWK5M6dm1mzZrF06VLy5s0LQJ8+fdDr9eTJk4fUqVPHqvns69ev6dKlC/nz56d69eqsWbOGP//8k44dO4Yfc/fuXQ4dOkS7du1ifP4YMSQxAQEBBsAQEBAQ5+d+9+6dYd26dYZ3797F+blNQVhYmGHwrsEGhmFgGAbv3d6a5IjxdZ4+3WBQZY3B0LOnwRAWFq/54sKKFQaDXq8iN29uMAQHJ3wGc/x5Xrp0qQEwVKhQQesoRour6xwYGGg4d+6cITAwMI6SmYdXr14ZnJ2dDXPnzv3ssaGhoYZnz54ZQkNDEyBZhMaNGxtGjx5t9PHXrl0zWFpaGi5duhSPqeLPp65zv379DJ06dfro137q5zgm/37LyI0wmk6nY2TFkfxSWY3ajNg/ghH7Pn2/1SR06QKzZ6vnU6ZA9+4m3a4BVKuGFSvUHbYlS6BVKwgJ0TqV+ZBN/BKvU6dOsXTpUq5evcrJkyfDO8DXq1dP42QfN378+M9OzP3Q5s2b6dy5M9mzZ4/HVNpIkyaN0S0hvoTMuREx1q90P/Q6PX129GHo3qFYWVgx8OuBWsf6tM6dVaXQsSNMn64qhRkzVC8EE9WwIaxapQqdZcvU7anFi+GDBR9CJEkTJkzg4sWLWFtbU7RoUXx9fUmVKpXWsT7Kw8OD7t27G318165d4zGNtnr37p0g7yPFjYiV3l/1JiQshAG7BvDT7p+wtLCkb+m+Wsf6tPbt1WSWdu3USI6tLUyaZLKTjAHq1YM1a9Sq9pUrVWHzxx8yyfhjDCY+p0p8ucKFC392VZAQpvtrqzB5/cv0Z1SFUQD029mPSYcnaZzICG3aqJm6oG5ReXtrm8cItWurvXDe36L64QeTnxetObktJUTSJsWN+CKDyg5iWLlhAHht92LOiTnaBjJG27YwbZp6PmoUjBunaRxj1K6tbklZWKhV7n36SIETnaxZs9K5c2dq1aqldRTNyOiVMGdx9fMrt6XEF/Mu583bkLeMPTiW7zd9Twq7FKa9kzFA166qTcPAgWr3PAcHNfHYhDVpAq9eqf0JfXzAyQmGDtU6lWkpUaJEpK35k5L3u+++efMmXnd+FSI+vXv3DiDajRNjQoob8cV0Oh2jK43mSeAT5pycQ/M1zdlit4WKmStqHe3TBgxQBc7o0arYcXJSXcZNWPv2qsDp2ROGDVORe/XSOpUwBXq9HhcXl/Ddme3t7eX23H+EhYXx7t073r59G28NG0Xsr3NYWBiPHj3C3t4eS8svK0+kuBFxQqfTMbPWTJ4GPmX1+dXUW1aPPW32UCzdxxu5mYRRo1S1MHWqmmjs6gr/6WBranr0UDXZ4MHg5QVp06p2WgKCgoJ4/fo1VlZWODo6ah0nwb3v6CztJ6JnMBgIDAzEzs5OCr949CXX2cLCgkyZMn3xfx8pbkSc0VvoWdxwMc+XPGfX9V3UWFwD33a+5EqVS+toH6fTqRVTjx7B0qVq/fX+/RDPfU++1E8/qchTpqg50q6uUNHEB8oSwqpVq2jZsiVVqlRh+/btWsdJcDqdDjc3N9KkSUNwcLDWcUxOcHAw+/fvp2zZspGaaIq49SXX2draOk5G1aS4EXHKxtKGtd+upeIfFTl+7zg1FtfgSIcjuDq4ah3t4yws4Pff4cED2L0bataEQ4fgCzoSxzedTs27uXdPLRFv0AB8faFAAa2TCVOg1+u/eM5CYqTX6wkJCcHW1laKm3hkCtdZbjqKOOdo48jm5pvJmjwrN57foO6yurwJfqN1rE+zsVEbyhQoAPfvQ/Xq8Pix1qk+ycJC7XlTtiy8eAE1akAs2sEkKrJSSAgBUtyIeJI6WWo2t9hMCrsUHL17lJZrWhIaZuIdIJ2dYcsWyJQJLl2CunUhMFDrVJ9kawvr1kHevGoUp0YN+ERj4CRD5lMIkbRpXtxMnz4dDw8PbG1tKVmyJEePHv3k8ZMnTyZnzpzY2dmRMWNGevXqxdu3bxMorYiJHClzsL7peqz11qy9sJZ+O/ppHenz0qWDrVsheXI4fFituzbx0YDkyVVNlj49nDunloxLHyohRFKmaXGzfPlyvLy8GDp0KCdPnqRgwYJUq1btozP9lyxZwoABAxg6dCjnz59n3rx5LF++nJ9++imBkwtjlclUhgX1FgDgc8SH6UenaxvIGLlzR2wJvHQp/Pyz1ok+K2NG+OsvsLeHHTuS7vJwuS0lhACNJxT7+PjQqVMn2rVrB8CsWbPYtGkT8+fPZ8CAAVGOP3ToEKVLl6Z58+aAakbWrFkz/Pz8PvoeQUFBBAUFhX/84sULQM3mjuvVBO/PJ6sUImucqzGXy11m6L6h9Njag8zOmamSJfbLrRPkOpcpg+7XX7H84QcYMoSQrFkxNG4cf+8XB/LmhYULdXzzjSXTpkGOHKF8/33su5+b489zyAdDVuaS2xyvs7mSa50w4us6x+R8OoNGv+q8e/cOe3t7Vq1aRf369cNfb9OmDc+fP2f9+vVRvmbJkiV06dKF7du3U6JECa5du0atWrVo1arVR0dvhg0bxvDhw6M9l729fZx9P+LTDAYDv97+ld1Pd5NMn4wJOSbgZuOmdazPyjdvHlk3biTE2pqDo0fzPFs2rSN91urV2Vm0KA8WFmEMHXqEggUfaR0pwZw7d45t27bh7u5Ow4YNtY4jhIhDb968oXnz5gQEBODk5PTJYzUrbu7du0f69Ok5dOgQnp6e4a/369ePffv2fXQ0ZurUqfTp0weDwUBISAjff/89M2fO/Oj7RDdykzFjRh4/fvzZixNTwcHB7NixgypVqsgyw2gEhQRReXFl/O76kStlLg60PYCTTcz/GyTodQ4NRd+wIRZbtmBIl46QgwfV5BYTZjBA+/Z6Fi+2wMXFgK9vCDlzxvw88vOcMOQ6Jxy51gkjvq7zixcvSJUqlVHFjVntc7N3715Gjx7NjBkzKFmyJFeuXKFnz56MHDmSIUOGRPs1NjY22NjYRHndysoq3n644/Pc5szKyoq1366l2JxiXHhygXYb27Gu6TosdLGb+pUg19nKCpYtA09PdOfOYfXtt2qTv2h+pkzJvHlw/TocOqSjUSMrjh5Vi8FiQ36eE4Zc54Qj1zphxPV1jsm5NJtQnCpVKvR6PQ8ePIj0+oMHD8K3EP+vIUOG0KpVKzp27Ej+/Plp0KABo0ePZsyYMYSFxX5ugUg4bo5urP12LTZ6GzZe2sjQPWbQ+dHJCTZuVMuSjh5V/Q9MnI0NrF2rJhpfuqR2MU4K/4uEhoYSFBQkcyqESOI0K26sra0pWrQou3btCn8tLCyMXbt2RbpN9aE3b95E2Zb5/S6cskrCfJRIX4Lf6vwGwCjfUaz8d6XGiYyQJYtaOaXTwW+/wdy5Wif6rDRp1KIva2tYvx7GjtU6Ufz7888/sbW1pV69elpHEUJoSNOl4F5eXsyZM4eFCxdy/vx5fvjhB16/fh2+eqp169YMHDgw/Pg6deowc+ZMli1bxvXr19mxYwdDhgyhTp06stW4mWldsDVepbwAaL+hPRceX9A4kRGqVVONNkF1Ef/MnkymoHhxmDFDPR88GLZt0zaPEEIkBE3n3Hz77bc8evQIb29v7t+/T6FChdi6dSuurqoP0a1btyKN1AwePBidTsfgwYO5e/cuqVOnpk6dOvxsBvuQiKh+qfILJ++fZO+NvTRe0Ri/jn4ks06mdaxPGzAAjh1T2wI3agQnTqghEhPWoYOqw377TXUPP3HCpNtmfREZwRVCgAnsUNytWzdu3rxJUFAQfn5+lCxZMvxze/fuZcGCBeEfW1paMnToUK5cuUJgYCC3bt1i+vTpuLi4JHxw8cUsLSxZ2mgpaR3S8u+jf/nur+9M/x8nCwtYuBBy5IA7d6BpU7PYDnjqVChRAp49U43P35h4q68vJe0XhEjaNC9uRNKW1iEtyxsvR6/Ts/jsYmafmK11pM9zclKzdR0cYM8eGDZM60SfZWMDq1ZB6tRw+jT07Kl1IiGEiD9S3AjNlXUvy5hKYwDoubUnx+8d1ziREfLkgTlz1PPRo1XPAxOXMaNa1a7TqfnQS5ZonSjumfzInxAiQUhxI0xCn6/6UD9Xfd6FvqPxisY8C3ymdaTPa9oUOndWu+a1aAH+/lon+qyKFeH9llDffaeWiSdGcltKiKRNihthEnQ6Hb/X+52sybNyM+AmnTZ2Mo/fwidPhgIF4NEjVeCEhmqd6LO8vaF8eXj1SnUQf/tW60Rxx93dnQYNGnx0OwkhRNIgxY0wGS62LixrvAwrCytWn1/Nbyd+0zrS59nZwYoVkCyZmn8zcqTWiT5Lr4fFi9X8m7//Bi8vrRPFnYoVK7JmzRoGDRqkdRQhhIakuBEmpVi6YuHzb37c9iP/PvxX40RGyJkTZv9/IvSIEbB7t7Z5jJAuHSxapJ7PnKnqMyGESCykuBEmp5dnL6plrcbbkLc0Xd2UwOBArSN9XosWakMZgwFatYInT7RO9FnVqsH7PTI7dYKbN7XNI4QQcUWKG2FyLHQWLKy/ENdkrvzz8B96b++tdSTjTJ0KuXLBvXsRE41N3IgR4OkJL16omswMpgx90rx589Dr9dJ+QYgkToobYZJcHVz5o8EfAMw8PpO159dqnMgI9vZqMouVFaxZA7//rnWiz7K0hD//VFv2+PrCL79onejLGAwGwsLCzGMyuhAi3khxI0xW1axV6fdVPwA6buyI/0vTX2pNkSIRk4p79IDLl7XNY4QsWeDXX9XzoUPhuBlsMySEEJ8ixY0waSMrjqSIWxGeBj6lw4YO5vEbeZ8+aq3169fQsiUEB2ud6LPatIFvvlGdJJo3V9HN0fufD9nnRoikTYobYdKs9dYsarAIG70NW65sMY/l4Xo9/PEHuLiojpUjRmid6LN0Opg1C9KnV4NNvXppnUgIIWJPihth8vKkzsPYymMB8NruxeWnpn+rh4wZI5aHjx4Nhw9rm8cIKVKomkynU50lNm6U0Q8hhHmS4kaYhR4le1Axc0XeBL+h/cb2hBrMYFlPkyZqCVJYGLRtC4Gmv6S9YsWITf26dNHz4oWVtoFiSW5LCZG0SXEjzIKFzoLf6/2Ok40Tfnf9WPNgjdaRjDNlitox79IlGDxY6zRGGTVKrWh/8EDHnDkFtI4TIxkyZKBq1aoUKlRI6yhCCA1JcSPMRibnTEyvOR2AZfeXcer+KY0TGSF58oju4ZMmwYED2uYxgq0tLFwIFhYGfH0zsHat+YyC1KhRg23btjFs2DCtowghNCTFjTArLfK3oEHOBoQSSudNnQkONf2VSNSsCe3bq0392rUzi6VIJUpAnz5hAHTrpufRI40DCSFEDEhxI8yKTqdjarWpOOod+fvB3/xy0Ex2nfPxgQwZ4MoV+OknrdMYZciQMDJlesGjRzq6ddM6jRBCGE+KG2F2XB1c6ZihIwAj9o3gn4f/aJzICM7OMG+eej51Kuzbp20eI9jYQI8eJ9HrDaxYAStXap3o8+bPn4+TkxMtWrTQOooQQkNS3AizVNalLLWz1yY4LJj269sTEhaidaTPq1pV9ZwCdXvq1Stt8xghW7YA+vVTt6e6dIGHDzUO9Bnv3r3j5cuXBJrByjQhRPyR4kaYJZ1Ox7Tq03C2cebYvWP4HPbROpJxJkwAd3e4fh3699c6jVEGDQqjQAF4/Bh++MG0+4GaxQ7WQoh4J8WNMFvpHNMxqdokALz3eHPx8UWNExnB0THi9tSMGWaxesraGhYsUE0216yBtWbQw1T2uREiaZPiRpi1toXaUi1rNYJCg2i/oT2hYWawuV+lStChg3reqRMEBWmbxwiFC0cMNHXrBs+faxpHCCE+SYobYdZ0Oh2/1fkNB2sHDt0+xLSj07SOZJzx48HVFS5cgDFjtE5jlMGDIXt28PeHAQO0ThM9uS0lhAApbkQikMk5E+OrjAfgp90/cfP5TY0TGSF5crVqClTvqXPntM1jBFtb+O3/fUtnzwZfX23zfIrclhIiaZPiRiQKnYt2pqx7Wd4Ev6Hr5q7m8Rv8N99A7doQHKxuT4WFaZ3os8qXj7ij1rmz6d1Rc3Nzo0yZMuTKlUvrKEIIDUlxIxIFC50Fs2rNwsrCik2XN7HmvBn0ntLp1KRiBwc4dCiii7iJ+/CO2ujRWqeJrEGDBvj6+jJq1CitowghNCTFjUg0cqfOzYAyajJI9y3dCXgboHEiI2TMGFEh9O8Pd+9qm8cIH95RGzMG/v1X2zxCCPFfUtyIROWnr38iW4ps+L/yZ/Bu8+jCTZcuULIkvHyJufQ5+PCOWufOZnFHTQiRhEhxIxIVW0tbZtWaBcD0Y9M5eveoxomMoNerzuGWlrBundpMxsT9947arFlaJ1Lmz5+Pm5sb3333ndZRhBAakuJGJDqVslSiVYFWGDDQeWNn82jNkD9/xEYyPXuaRWuGD++o/fQTPHigbR6AN2/ecP/+fZ7LRjxCJGlS3IhEaWLViaSwS8HfD/5mypEpWscxzqBBkDkz3LkDI0ZoncYoXbpAkSIQEAB9+2qdRva5EUIoUtyIRCl1stThe9947/U2j71v7OwiZupOmmQWM3X1epg5U92mWrTILJqdCyGSACluRKLVrlC78L1vem7tqXUc49SuDfXqQUiIGhYxg5GIEiUimp137aomGWtNNvETImmT4kYkWjqdjpm1ZmJpYcn6i+vZcnmL1pGMM2WKGsXZvx8WL9Y6jVFGj4ZUqdRg0xQN7wLKbSkhBEhxIxK5PKnz0LOkGrXpsbUHQSEmtqVudNzdYcgQ9bx3b7PoUpkiBfzyi3o+bJiaNiSEEFqR4kYkekPLDcXNwY0rT68w4dAEreMYp3dvyJkTHj5UHSvNQNu28NVX8Po19OqlTYbUqVNTuHBhPDw8tAkghDAJUtyIRM/RxpEJVVVR87Pvz+YxudjaGqZPV89nzoQTJ7TNYwQLC7X3jYUFrFoF27cnfIZmzZpx8uRJxo4dm/BvLoQwGVLciCShWb5mlHMvR2BIIF7bvbSOY5xKlaBpU7X9b5cuZrENcMGC0L27et61K7x9q20eIUTSJMWNSBJ0Oh3Tak5Dr9Oz5vwatl/VYFghNiZOBEdHOHoUFizQOo1RRowANze4cgV8fLROI4RIiqS4EUlGvjT56F5CDSt039LdPCYXp0sHQ4eq5wMHqt3yTJyTk+ocDmoVVUL2Av3999/JmjUrvbSa9COEMAlS3IgkZVj5Ybgmc+XSk0tMOjJJ6zjG6d4dcuRQk4tHjtQ6jVGaN4+YXDxgQMK9b0BAANeuXePhw4cJ96ZCCJMjxY1IUpxtncN3Lh65fyS3A25rnMgI1tZqx2JQm8hcvKhtHiPodCqqTgd//gmHDyfM+8o+N0IIkOJGJEEtC7SkTKYyvAl+w8BdA7WOY5yaNdUjJES7ddYxVKwYtGunnvfokbDzoWWHYiGSNiluRJKj0+mYXG0yOnQsPruYI3eOaB3JOJMmgZUVbNkCmzdrncYoo0erOTjHj8PChVqnEUIkFVLciCSpaLqitC3UFoCeW3sSZjD9ZdbkyAE9/98jq1cvePdO2zxGcHUFb2/1fOBAePEift9PbksJIUCKG5GEja40GgdrB47ePcriM+bRw4khQ1TFcOlSRAdxE/d+PvSDBwk3H1puSwmRtElxI5KstA5pGfT1IAAG7BrAq3evNE5kBCcnGDNGPR8xAu7f1zaPEf47H/rSpfh7r+TJk5MjRw7Spk0bf28ihDB5UtyIJO3HUj+S2SUz917e45cDv2gdxzht2kDx4vDyJfz0k9ZpjPJ+PnRwMHjF4wbR7dq14+LFi4x/v9GOECJJkuJGJGm2lrbhfacmHJ5gHn2nLCwibkn9/jscO6ZtHiNNmgSWlrBpk5oTLYQQ8UWKG5HkNcjVgPIe5Xkb8pZ+O/tpHcc4pUpBq1bqec+eYAYTaT+cD/3jj2YxH1oIYaakuBFJnk6nY1K1SejQseLfFfje9NU6knHGjoVkydQOeStWaJ3GKEOGQJo0at7NrFlxf/6FCxdSoEABBg0aFPcnF0KYDSluhAAKpS1ExyIdAfhx24/msTQ8XTro9/+RpgEDzKIFt7NzxIqp4cPh2bO4Pf+jR484e/Ysd+7cidsTCyHMihQ3QvzfqIqjcLJx4qT/SRaeNpMd53r3hvTp4cYN+PVXrdMYpX17yJsXnj6FUaPi9tyyz40QAqS4ESJcmmRpGFJ2CAADdw3kZdBLjRMZIVky+Pln9XzUKHj0SNs8RrC0hAlqDje//gpXr8b9e8g+N0IkbVLcCPGBHiV7kDV5Vh68fsCEQxO0jmOcVq2gcGG1/e/w4VqnMUr16lCtmloanpBdw4UQSYPmxc306dPx8PDA1taWkiVLcvTo0U8e//z5c7p27Yqbmxs2NjbkyJGDzWbSZ0eYPmu9NWMrjwXU0vB7L+9pnMgIFhYwcaJ6PmsWXLigbR4jTZigoq9aBQcPxs055baUEAI0Lm6WL1+Ol5cXQ4cO5eTJkxQsWJBq1arx8OHDaI9/9+4dVapU4caNG6xatYqLFy8yZ84c0qdPn8DJRWLWKHcjPDN48ib4Dd57vLWOY5wKFaBOHQgNhb59tU5jlHz5oEMH9dzLK267hsttKSGSNk2LGx8fHzp16kS7du3IkycPs2bNwt7envnz50d7/Pz583n69Cnr1q2jdOnSeHh4UK5cOQoWLJjAyUViptPpwjf2m39qPmcfnNU4kZHGj1cTWv76C3bv1jqNUUaMAAcHOHoUli//8vM5OjqSIUMGkidP/uUnE0KYLUut3vjdu3ecOHGCgQMHhr9mYWFB5cqVOXz4cLRfs2HDBjw9PenatSvr168nderUNG/enP79+6PX66P9mqCgIIKCgsI/fvH/tsTBwcEEBwfH4XdE+Pni+rwisoS4zsXTFqdhroasubCGvtv7srHpxnh7rziTJQsWnTujnzEDg5cXIUeOwEf+vzBGQlznlCmhb18Lhg7VM3Cggdq1Q7C1jf35OnbsSMeOakm/ufx/KH9vJBy51gkjvq5zTM6nWXHz+PFjQkNDcXV1jfS6q6srFz4yZ+DatWvs3r2bFi1asHnzZq5cuUKXLl0IDg5m6NCh0X7NmDFjGB7NJMvt27djb2//5d9INHbs2BEv5xWRxfd1rqKrwgbdBrZd28bo5aMp5FgoXt8vLliXKkXlBQuw+vtv/unXj1uVKn3xOeP7OufKpSdlykrcvGlHt26XaNjwSry+n6mSvzcSjlzrhBHX1/nNmzdGH6szaDQD7969e6RPn55Dhw7h6ekZ/nq/fv3Yt28ffn5+Ub4mR44cvH37luvXr4eP1Pj4+DB+/Hj8/f2jfZ/oRm4yZszI48ePcXJyitPvKTg4mB07dlClShWsrKzi9NwiQkJe5947evPrsV/JnyY/R9sfRW8R+5GQhGIxcSL6gQMxuLkRcu6cWi4eCwl5nf/8U0f79pY4Oho4fz6ENGni9e1Mivy9kXDkWieM+LrOL168IFWqVAQEBHz23+9Yjdxcv34dX19fbt68yZs3b0idOjWFCxfG09MTWyPHlFOlSoVer+fBgweRXn/w4AFp06aN9mvc3NywsrKKdAsqd+7c3L9/n3fv3mFtbR3la2xsbLCxsYnyupWVVbz9cMfnuUWEhLjOQ8sP5Y8zf3D24VmWnV9G20Jt4/X94sSPP8Ls2ehu3MBq8mQYNuyLTpcQ17lNG5g+HU6c0PHzz1bMmBG78yxatIgZM2ZQu3Zts2vBIH9vJBy51gkjrq9zTM4VownFixcvpkSJEmTNmpX+/fuzbt06fH19mTt3LtWrV8fV1ZUuXbpw8+bnOytbW1tTtGhRdu3aFf5aWFgYu3btijSS86HSpUtz5coVwj5YVnHp0iXc3NyiLWyE+FIp7VMyuOxgAAbvHsybYOOHRTVja6v6ToFab33/vrZ5jPDhavbffoOLF2N3nrt373LkyBGuXbsWd+GEEGbH6OKmcOHCTJ06lbZt23Lz5k38/f05ceIEBw4c4Ny5c7x48YL169cTFhZGsWLFWLly5WfP6eXlxZw5c1i4cCHnz5/nhx9+4PXr17Rr1w6A1q1bR5pw/MMPP/D06VN69uzJpUuX2LRpE6NHj6Zr166x+NaFME63Et1wd3bn7su7TDo8Ses4xmnSBIoXh9ev1ZIkM1CuXMRq9g/+t48R2edGCAExKG7Gjh2Ln58fXbp0IWPGjFE+b2NjQ/ny5Zk1axYXLlwgS5Ysnz3nt99+y4QJE/D29qZQoUKcPn2arVu3hk8yvnXrVqS5NBkzZmTbtm0cO3aMAgUK0KNHD3r27MkA2eJUxCNbS1vGVBoDwNiDY3nw6sFnvsIE6HRqaTiooZBLl7TNY6SxY9Uoztq1cOhQ7M8j+9wIkbQZXdxUq1bN6JOmTJmSokWLGnVst27duHnzJkFBQfj5+VGyZMnwz+3du5cFCxZEOt7T05MjR47w9u1brl69yk8//fTRZeBCxJVv831L8XTFefXuFcP3mUeLA8qVg1q11FDITz9pncYoefLA/wdu6dcPZCBGCBEbsdrE778Fx3shISGRbiMJkVhY6CzCN/b77cRvXHhsHi0OwodCVq+GI0e0TmOU4cPBzk61ZNgYw+2F5LaUEAJiWdz06NGDb775hmfPnoW/dvHiRUqWLMnSpUvjLJwQpqSse1nq5axHqCGUfjv6aR3HOPnyqaVIYDZDIenTqwVfoJpqhoTE/BxyW0qIpC1Wxc2pU6e4c+cO+fPnZ8eOHUyfPp0iRYqQK1cu/v7777jOKITJ+KXyL+h1ejZe2siBWwe0jmOcESPUCipfX9WawQz07692Lz5/Hj4yUBwtW1tbkidPHm8bdAohzEOsipusWbNy8OBBGjZsSPXq1enVqxdz585l8eLFODs7x3VGIUxGzlQ56VhEbe/ff2d/87gNkiED9Oypnsd2KCSBOTvDYLUCn6FDwdiNSb28vHj69ClTpkyJv3BCCJMX68aZmzZtYtmyZXh6euLi4sK8efO4d+9eXGYTwiQNLTcUeyt7Dt0+xIaLG7SOY5wBAyBFCjh3DhYu1DqNUX74ATw84N49mDxZ6zRCCHMSq+Lmu+++45tvvqF///74+vpy5swZrK2tyZ8/PytWrIjrjEKYFDdHN3qV6gXAwF0DCQkz/ZEQXFzg/Y693t7GD4VoyMYGfv5ZPf/lF3j8WNs8QgjzEavi5uDBg/j5+dG7d290Oh1p06Zl8+bNjBgxgvbt28d1RiFMTt+v+pLSLiXnH59n4WnzGAmha1dwd1dDIWZy26ZpUyhSBF68gFGjPn/8n3/+SaVKlfDx8Yn/cEIIkxWr4ubEiRMULFgwyutdu3blxIkTXxxKCFPnbOvMoK/VSMjQvUMJDA7UOJERbGwiKoSxY81iKMTCQo3aAMyYAZ/rqnD9+nV2797Nxdj2bxBCJAqxKm6ia0T5Xs6cOWMdRghz0qV4l/C2DL8e/VXrOMZp3hwKFVJDIe/v+Zi4ypWhalUIDo6YZPwxZjHBWwgR74wubqpXr84RIzYBe/nyJb/88gvTp0//omBCmDobSxtGVhgJwJgDY3ga+FTjREawsIBx49Tz6dPh+nVt8xjpl19UR4mlS8GYwWHZ50aIpM3o4uabb76hUaNG5MmTh/79+7Ny5UoOHjzIiRMn2LlzJ1OnTqVJkya4ublx8uRJ6tSpE5+5hTAJzfM3J3+a/Dx/+5yxB8ZqHcc4VaqohzFDISaiUCFo0UI979/fLPYiFEJoyOjipkOHDly7do2ffvqJc+fO0blzZ77++muKFy9OtWrVmDNnDpkyZeLYsWMsX76cTJkyxWduIUyC3kLP2MqqqJnqN5XbAbc1TmSk9xNZliyBkye1zWKkkSPB2hp27YLt26M/Rm5LCSEghnNubGxsaNmyJRs3buTZs2c8e/aMe/fu8fbtW86ePcuECRPInTt3fGUVwiTVyFaDcu7lCAoNYtjeYVrHMU7hwpGHQsyAhwd066ae9+8PYWEfP1ZuSwmRtMV6Ez8AZ2dn0qZNi5WVVVzlEcLs6HQ6fqmsRkIW/L2Ac4/OaZzISKNGqaGQnTthxw6t0xjlp5/U7sV//63m3/yXXq/H2toaS0vLhA8nhDAZRv8NsGGD8Tux1q1bN1ZhhDBXJTOUpGHuhqw5v4afdv3EuqbrtI70eR4e0KWL2v63f3+oVElNODZhKVOqzZYHDlTThRo3Vivc3xs8eDCDzWQekRAi/hhd3NSvXz/SxzqdLtL97Q+HgUNDQ788mRBmZnTF0ay/sJ71F9dz8NZBSmcqrXWkzxs0CObNg1OnYPlyaNZM60Sf1aMH/Por3LgBs2ZFtM0SQoj3jP41LSwsLPyxfft2ChUqxJYtW3j+/DnPnz9n8+bNFClShK1bt8ZnXiFMVs5UOelQuANgRk01U6WKmHMzeDC8e6dtHiPY28OwYer5yJEQEKBpHCGECYrVGPSPP/7IlClTqFatGk5OTjg5OVGtWjV8fHzo0aNHXGcUwmwMLT8UO0s7Dt4+yMZLG7WOY5wff4S0adX2v7/9pnUao7RrBzlzwpMnMGFCxOtLliyhbt26zJo1S7twQgjNxaq4uXr1Ki4uLlFed3Z25saNG18YSQjzlc4xHT+W+hFQTTVDw8zgFm2yZBFDISNGwMuXmsYxhqUljBmjnvv4wP376vnFixfZuHEjZ8+e1S6cEEJzsSpuihcvjpeXFw8ePAh/7cGDB/Tt25cSJUrEWTghzFG/0v1IYZeCc4/O8cfff2gdxzjt20OOHPDoEUycqHUao9SvD6VKqQbnI0ao18ziVqAQIt7FqriZP38+/v7+ZMqUiWzZspEtWzYyZcrE3bt3mTdvXlxnFMKsuNi6hDfV9N7rbR5NNa2sYPRo9XzCBPjgFxdTpdNF7EX4229w6dKHn5N9boRIymJV3GTLlo0zZ86wceNGevToQY8ePfjrr784e/Ys2bJli+uMQpidLsW7kMk5E3de3GHa0WlaxzFOw4ZQogS8fq1m6pqBsmWhVi0IDTWbThJCiAQQ600tdDodVatWDS9uqlSpIr8tCfF/tpa2jCiv7pWMOTCGZ4HPNE5kBJ0uoqnm7Nlw5Yq2eYw0ZoyKvnIl3L0rt6WEEDHY52bq1Kl07twZW1tbpk6d+sljZcWUENCyQEsmHJ7APw//4ZeDv4T3oDJp5cpBzZqweTP6oUMjWjSYsPz5oXVrWLhQ9Z0CuS0lRFJndHEzadIkWrRoga2tLZMmTfrocTqdToobIVBNNcdUGkOdpXWY4jeF7iW6k94pvdaxPm/MGNiyBYuVK3EpWVLrNEYZMQKWLYObN7VOIoQwBUbflrp+/TopU6YMf/6xx7Vr1+ItrBDmplb2Wnyd6Wvehrw1n6aaBQpAq1YA5Fm4EMxgBVKmTO+bag6nQIEQfHwma5xICKGlL24kYzAYZPmlEB/xYVPN+afnc+HxBY0TGWnECAzW1qQ+exbdzp1apzHKwIHg5GTBmTN6VqzQax1HCKGhWBc3f/zxB/nz58fOzg47OzsKFCjAokWL4jKbEImCZ0ZP6ueqT5ghjJ92/aR1HOO4uxP2ww8A6H/6CcLCNA70ee+baoJaORUUpG0eIYR2YlXc+Pj48MMPP1CzZk1WrFjBihUrqF69Ot9///0n5+MIkVSNrjgaC50Fay+s5fDtw1rHMUpY//4E29uj+/tvNaHFDKRPvxxb22bcuLGQ2bO1TiOE0Eqsiptff/2VmTNn8ssvv1C3bl3q1q3LuHHjmDFjxmdXUgmRFOVOnZt2hdoB5tVU83KDBur5oEFmMRRy8eIZ3r5dBpxi5Eh48ULrREIILcSquPH39+err76K8vpXX32Fv7//F4cSIjEaVn4Ytpa2+N7yZfPlzVrHMcq1OnUwuLnBjRuYw1DI+6IxeXJ4/DhyU00hRNIR6x2KV6xYEeX15cuXkz179i8OJURilMEpAz1KqG0SzKWpZqitLaFDhqgPzGgopHRptc/NxIkRTTWFEElHrIqb4cOH4+3tTfXq1Rk5ciQjR46kevXqDB8+nBHvO9gJIaIYUGYALrYunH14lsVnF2sdxyiGtm1VU00zGgrJmhVKlozcVFMIkXTEqrhp1KgRfn5+pEqVinXr1rFu3TpSpUrF0aNHafD+Hr0QIorkdskZWGYgAEP2DOFtyFuNExnB0lJt7AcmPxTy/rbUh00158yBy5c1DCWESHCxXgpetGhR/vzzT06cOMGJEyf4888/KVy4cFxmEyJR6l6iO+kd03Mr4BYzj83UOo5xGjQwq6EQnU5HuXKqqWZIiDTVFCKp+aJN/B4+fMg///zDmTNnIj2EEB9nZ2XH8PLDARjlO4qAtwEaJzLCh0Mhv/0Gly5pm8dI75tqrlgBx45pnUYIkVBiVdycOHGCfPny4ebmRoECBShUqFD4Q0ZvhPi8NoXakCtVLp4GPmX8ofFaxzHO+6GQ0FCTHQoZOnQoAQEB/Pzzz4Bqqvn/ThL0728WnSSEEHEgVsVN+/btyZEjB4cOHeLatWvSW0qIGLK0sGRMJTWPxeewD/4vzWQLhfdDIStXwtGjWqeJwtbWFicnJ+zs7MJfGzECrK1hzx7Yvl3DcEKIBBOr4ubatWuMGzeOkiVL4uHhgbu7e6SHEOLz6uWsh2cGTwJDAhmxz/TnsQBqKKR1a/XcTIZC3N3fN9VU7RnMoJOEEOILxaq4qVSpEn///XdcZxEiSfmwqeack3O49MQ85rEwYgTY2MDevbBtm9ZpIlm1ahUdOnSIsg/XTz+BkxOcPm02nSSEEF8gVsXN3LlzmT9/PsOHD2f16tVs2LAh0kMIYZyv3b+mdo7ahBpCGbzbNOexRJEpU8RQSP/+JjUUcuzYMebPn8/R/9wyS5lSRQWz6SQhhPgClrH5osOHD3Pw4EG2bNkS5XM6nY7QUNPfeVUIUzG64mg2XdrEynMrOXr3KCXSl9A60ucNHAhz58KZM7BkCbRsqXUigE/27OrZE6ZNi+gk0aNHwuUSQiSsWI3cdO/enZYtW+Lv709YWFikhxQ2QsRMftf8tC6o5rEM2DnAPJpqpkypJrCAWjllYkMhOp0uymvJksGwYeq5GXWSEELEQqyKmydPntCrVy9cXV3jOo8QSdLw8sOx1luz58Yetl81kyU9PXpAunRw8ybMNI/NCNu3j+gkMXGi1mmEEPElVsVNw4YN2bNnT1xnESLJcndxp1txNY+l/87+hBlMZx7LR9nbw3C1GSGjRkGA9psRfm7Uy4w6SQghvkCs5tzkyJGDgQMHcuDAAfLnz4+VlVWkz/eQm9lCxNhPX//E3FNz+fvB3yz7ZxnN8zfXOtLntW2rqoQLF1RTzZEjtU4ERH9b6r33nST8/FTc6dMTMJgQIkHEerWUg4MD+/btY9q0aUyaNCn8MXny5DiOKETSkNI+Jf1LqyU9g3cP5l3oO40TGeHDoRAfH/A3/c0I/9tJQppqCpH4xKq4+XBH4v8+ZIdiIWKvZ8mepHVIy/Xn15l9fLbWcYxTrx54eppEU80hQ4Zw7949Bn+mPUS5clCzpjTVFCKx+qLGmUKIuJXMOhnDyg0DYMT+EbwIMoMlPTodjB2rns+Zo2lTTScnJ9zc3HBycvrssR821Tx+PAHCCSESTKzm3Hh5eUX7uk6nw9bWlmzZslGvXj1SpEjxReGESIraF27PxMMTufz0MhMPTWR4heFaR/q8smWhdm346y+1S97KlVon+qwCBVRTzT/+UBv87dypih0hhPmL1cjNqVOnmDdvHr/99hv79u1j3759zJkzh3nz5rFr1y68vLzIli0b586di+u8QiR6VnorRlcaDcDEwxN58OqBxomM9H4oZNUqNVtXA2vXrqVHjx5G75T+vqnm7t2wY0c8hxNCJJhYFTf16tWjcuXK3Lt3jxMnTnDixAnu3LlDlSpVaNasGXfv3qVs2bL06tUrrvMKkSQ0yt2I4umK8zr4NSP3m8YKpM/Klw/atFHPNWqqeeDAAX799VcOHTpk1PHu7tC1q3puYp0khBBfIFbFzfjx4xk5cmSk+9rOzs4MGzaMcePGYW9vj7e3NydOnIizoEIkJR821Zx9YjZXn17VOJGRhg9XTTX37YOtWxP87WOzu7M01RQi8YlVcRMQEMDDhw+jvP7o0SNe/H9PcxcXF969M4OlrEKYqAqZK1AtazVCwkIYvMdMlvRkygTdu6vn/fuDGbRjSZUqoqnm4MEgf20JYf5ifVuqffv2rF27ljt37nDnzh3Wrl1Lhw4dqF+/PgBHjx4lR44ccZlViCRnbGW1CmnZP8s4fs9MlvQMHAjOznD2rGqqqYFPbeIXnZ49wc0Nrl9XTTWFEOYtVsXN7NmzqVSpEk2bNsXd3R13d3eaNm1KpUqVmDVrFgC5cuVi7ty5cRpWiKSmUNpCtCygOm733t7bPJpqpkihChxQQyFv3ybYW8f2+iRLBkOHqucjRkhTTSHMXayKGwcHB+bMmcOTJ084deoUp06d4smTJ/z2228kS5YMgEKFClGoUKG4zCpEkvRzxZ+xtbRl/839bLho3CogzfXoAenTw61bmjTVjOnIDUhTTSESky/axM/BwYECBQpQoEABHBwcYn2e6dOn4+Hhga2tLSVLluTo0aNGfd2yZcvQ6XTht8KESIwyOWeiVym18rDfzn4EhwZrnMgIdnYm11Tzc6ysYLRagc/EifDATFbgCyGiMrq4adiwYfhk4YYNG37yERPLly/Hy8uLoUOHcvLkSQoWLEi1atWinbD8oRs3btCnTx++/vrrGL2fEOZoQJkBpEmWhktPLjHr+Cyt4xinTRvInRuePoVx4xLkLQcNGsSVK1fo3bt3rL6+YUMoUQJev9a8k4QQ4gsYXdw4OzuHD/U6Ozt/8hETPj4+dOrUiXbt2pEnTx5mzZqFvb098+fP/+jXhIaG0qJFC4YPH06WLFli9H5CmCMnGyeGl1cjIcP3Def52+faBjLGh001J02Ce/fi/S1TpUpF1qxZSZkyZay+XqeLqMNmz1bNzoUQ5sfo9gu///57+PMZM2YQFhYWPr/mxo0brFu3jty5c1OtWjWj3/zdu3ecOHGCge8nHwIWFhZUrlyZw4cPf/TrRowYQZo0aejQoQO+vr6ffI+goCCCgoLCP34/+hQcHExwcNwO778/X1yfV0SWVK9zm/xtmHJkCheeXGDkvpGMrTg2Xt8vTq5zjRroPT2xOHyYMG9vQjWYfxNTX30FtWvr+esvC3r3DmPduvhdzp5Uf561INc6YcTXdY7J+WLVW6pevXo0bNiQ77//nufPn1OqVCmsrKx4/PgxPj4+/PDDD0ad5/Hjx4SGhuLq6hrpdVdXVy585FemAwcOMG/ePE6fPm3Ue4wZM4bhw6P25tm+fTv29vZGnSOmdsg+7gkiKV7nxk6NGfVkFFP9ppIzICeuNq6f/6Iv9KXXOUXdunx9+DC633/HN39+Xnp4xE2waBw9epSLFy+SL18+ChcuHOvz1KjhwJYtFdi82YIxY45QsOCjOEwZvaT486wVudYJI66v85s3b4w+NlbFzcmTJ5k0aRIAq1atwtXVlVOnTrF69Wq8vb2NLm5i6uXLl7Rq1Yo5c+aQKlUqo75m4MCBkRp9vnjxgowZM1K1alWjOgfHRHBwMDt27KBKlSpYWVnF6blFhKR8nWsYanBo6SF239jNTnayuObieHuvOLvONWsSduwYFmvWUH7jRkI3b463DpU7duxg9erVZM+enZo1a37Ruc6fNzBtGqxa5Um/fiHo9XEU8j+S8s9zQpNrnTDi6zq/iMEeDbEqbt68eYOjoyOgRkAaNmyIhYUFpUqV4ubNm0afJ1WqVOj1eh78Z1nCgwcPSJs2bZTjr169yo0bN6hTp074a2H/bwZjaWnJxYsXyZo1a6SvsbGxwcbGJsq5rKys4u2HOz7PLSIk1evsU82HwrMLs/L8Sry+8qJUhlLx+n5xcp3Hj4e//sJi1y4sduyAWrXiJtx/WFioaYR6vf6LMw8bBn/+CWfP6li82IoOHeIg4Cck1Z9nLci1ThhxfZ1jcq5YLQXPli0b69at4/bt22zbto2qVasC8PDhwxiNhlhbW1O0aFF27doV/lpYWBi7du3C09MzyvG5cuXi7NmznD59OvxRt25dKlSowOnTp8mYMWNsvh0hzErBtAVpW6gtYEYb+2XJorYBBujdG8xgzkPKlODtrZ4PHgwvX2qbRwhhvFgVN97e3vTp0wcPDw9KliwZXohs3749xve5vby8mDNnDgsXLuT8+fP88MMPvH79mnbt2gHQunXr8AnHtra25MuXL9LDxcUFR0dH8uXLh7W1dWy+HSHMzsgKI7G3sufQ7UOsPr9a6zjGGTQIUqeGixdhVvwsZ39f6MVmE7/odO0K2bLB/fsJtppdCBEHYlXcNG7cmFu3bnH8+HG2ftD5t1KlSuFzcYz17bffMmHCBLy9vSlUqBCnT59m69at4ZOMb926hb+/f2xiCpFopXdKTx/PPgD039mfd6Fm0O3R2Tli85hhw+DZM03jGMPaOqKomTABbt/WNo8Qwjix3qE4bdq0FC5cOPweN0CJEiXIlStXjM/VrVs3bt68SVBQEH5+fpQsWTL8c3v37mXBggUf/doFCxawbt26GL+nEOaub+m+pHVIy7Vn15h+dLrWcYzTsSPkzas29hs5Mt7eJq5GbgDq14eyZVWLrJ9+irPTCiHi0Re1XxBCaMfB2oFRFUYBMGL/CJ68eaJxIiNYWkY0bpo2DS5f1jaPEXQ68PFRf/75Jxw7pnUiIcTnSHEjhBlrW6gtBVwL8Pztc7z3eGsdxzjVqkGNGmpScd++cXrqAQMG8Pfff9OlS5c4PW/RotC6tXru5QXmMIdbiKRMihshzJjeQs+U6lMAmHViFmcenNE4kZEmTAC9Htavhz174uy06dOnp0CBAtFuJfGlfv5Z9QM9cABWm8kcbiGSKiluhDBz5T3K0zhPY8IMYfy49UfzWBqeJw9895167uUFofHb4iAupE8P/fqp5337QmCgtnmEEB8nxY0QicD4KuOxtbRlz409rDm/Rus4xhk+XK2gOn0aPrFoICa2bNnCyJEj2bdvX5yc77/69YOMGeHGDTX4JIQwTVLcCJEIeLh40PcrNX+lz44+BAabwbBCqlQRu+QNHAjPn3/xKTdu3Ii3tzd74vBW14fs7dVmy6Aant+6FS9vI4T4QlLcCJFI9C/dnwxOGbjx/AYTD0/UOo5xunWDnDnh0SM1kmMGmjRRS8MDA+N8PrQQIo5IcSNEIpHMOhnjKqsd58YcGMOdF3c0TmQEa2uYOlU9//VX+PffLzpdQsw30ulUZAsLWLEC9u6N97cUQsSQFDdCJCJN8zWldMbSvAl+Q/+d/bWOY5yqVdVOeaGh0KNHnKyzjstN/KJTsCB8/7163qMHhITE69sJIWJIihshEhGdTsfUGlPRoWPJ2SUcvHVQ60jG8fEBW1vYvfuL1lkn5EqxESMgRQo4exZmz06wtxVCGEGKGyESmSJuRehQuAMAPbb2IMwQpnEiI2TOHLHOundvePPmi04X3yM3oLqGv+8gMWQIPDGDDaKFSCqkuBEiEfq50s842Thx0v8kv5/6Xes4xunfHzJlUkuQfvlF6zRG6dwZChRQPUCHDNE6jRDiPSluhEiE0iRLw9ByQwEYuGsgzwJNvwM39vbq9hSo4ub69Rifol+/fhw6dIh27drFcbjoWVpGzIeePVtt2SOE0J4UN0IkUt1KdCN3qtw8evOIwbsHax3HOA0bQqVKEBSkdi6OoSxZsuDp6UnGjBnjIVz0ypWDb7+FsLA4mw8thPhCUtwIkUhZ662ZUWsGADOPz+TEvRMaJzKCTgdTpqi+U+vWwfbtWicyyvjxqu+Ury8sXap1GiGEFDdCJGLlPcrTPH9zDBjosrmLeUwuzpsXundXz3v0gHfvjP7SHTt24OPjg5+fXzyFi17GjDBokHru5QUBAQn69kKI/5DiRohEbkKVCTjZOHH07lHmnpyrdRzjDBsGadLAxYswaZLRX7Zy5Up69+7Nzp074y/bR/TpAzlywIMHMNhM7gIKkVhJcSNEIufm6MaI8iMANbn48ZvHGicygrNzRBOn4cNVp0oTZ2MDM9RdQGbMgBNmcBdQiMRKihshkoCuJbpS0LUgTwOfMmDnAK3jGKdVKzVbNzBQ3aYyYqZuQm7iF51KlaBZMzW5+Icf1KbLQoiEJ8WNEEmApYVl+OTieafmcfj2YY0TGUGng5kzwcoK/vpLTTA2+kvjfxO/j/HxAScnOHYMfvtNsxhCJGlS3AiRRHyV8SvaFVL7v3TZ3IWQMDNoiJQ7d8TOxT16wMuXnzxc65EbgLRp4eef1fOBA9UcHCFEwpLiRogk5JfKv5DcNjmn759m+tHpWscxzqBBkCUL3LmjJhobQcuRG1C3pIoWVaum+vbVNIoQSZIUN0IkIamTpWZMpTEADNo9iFsBtzROZAQ7O5j+/0JsyhSz2AZYr1d31HQ6WLQI9u7VOpEQSYsUN0IkMZ2KdqJMpjK8Dn5Nl01dTOJWzmdVrw7ffKNm6H7/vZqxG433y8CbNWuWwAGjKl5cRQU1khMUpG0eIZISKW6ESGIsdBb8Vvs3rCys2HR5EyvPrdQ6knEmTQJHR/Dzgzlzoj0kd+7cVKpUCQ8Pj4TN9hGjR4OrK1y4AGPGaJ1GiKRDihshkqDcqXPz09c/AdBjSw/zaKyZPj2MGqWe9+8P9+5pm8cILi7w66/q+ejR8O+/msYRIsmQ4kaIJGpgmYHkSpWLB68f0G9HP63jGKdrVyhWTM3U7do1yt43e/fuZfbs2Zw6dUqjgFE1bgx160JwMHTsKHvfCJEQpLgRIomysbRhTh11e2fuqbnsu7FP40RG0Oth3jywtFT73qxeHenTf/zxB99//z3bTajhpk6ndix2coIjRyJ2MRZCxB8pboRIwspkKsN3Rb8DoPNfnXkb8lbjREYoUEBtIANq9ObpU23zGCF9evjlF/V84EC4eVPbPEIkdlLcCJHEja08FjcHNy49ucTP+3/WOo5xBg1SG/w9fKjacP+fKa/86twZvv4aXr9Wq6hMOKoQZk+KGyGSOBdbF36toWa9jj04ljMPzmicyAg2Nur2lE4HCxfC1q2RPq31Jn7RsbBQi7ysrVXcJUu0TiRE4iXFjRCChrkbUj9XfULCQmi7ri3BocFaR/o8T0/o2VM9/+47ePnSpEduAHLmBG9v9bxnT3j0SNs8QiRWUtwIIdDpdMysNZMUdik4df8UYw6YyaYso0aBhwfcugU//RT+simO3LzXrx/kzw9Pnqh2WUKIuCfFjRACgLQOaZlWYxoAI/eP5PSD09oGMkayZBEb+k2fbhZdKq2sYP58tfBr2TJYvdp0CzEhzJUUN0KIcE3zNaVBrgaEhIXQcWNHgsPM4PZU5crQvj0YDPT891/WL19OgwYNtE71ScWKRSz46t5dz/PnNtoGEiKRkeJGCBHu/e2plHYpOfPwDKserNI6knEmToQMGSh8+zZ19+0jW7ZsWif6rCFDoGBBePxYx8yZBWX1lBBxSIobIUQkrg6uTK+punCverCKU/dNZ7ffj3JxUfd6QO2SZ0Kb+H2MtbVa6GVlZcDPz43Fi+X2lBBxRYobIUQUTfI2oWGuhoQSSoeNHXgX+k7rSJ9XpQoHGzViEXCuVSt4Zvr9sgoWhCFDVIfzXr303LmjcSAhEgkpboQQUeh0OqZWm4qT3ol/Hv3DiH0jtI5klNnW1rQGtjx8CN27ax3HKH36hJE9+zMCAnR07Cib+wkRF6S4EUJEK02yNHyf8XsAxhwYw6HbhzROZARLS/WnTgeLF8Mq058zZGkJPXuexNbWwLZtEYu/hBCxJ8WNEOKjvnL5iub5mhNmCKPlmpa8CHqhdaRPCt/Er2JF9ef338P9+9oFMlKGDK8YOVLdnvLygmvXNA4khJmT4kYI8UlTqk7B3dmd68+v02OLeew6p6taFQoVUjvldepkFvd6uncPo2xZ1XuqRQsICdE6kRDmS4obIcQnOds6s6jBIix0Fiz8eyEr/12pdaSPCh+5sbSEP/5QS5L++gtmz9Y2mBEsLFRkZ2c4cgRGmMc0JyFMkhQ3QojP+tr9awaUHgDAd399x50Xpr2sR6fTqR4Ho0erF3r1gn//1TaUEdzdYdYs9fznn8HXV9s8QpgrKW6EEEYZVn4YxdIV49nbZ7Rd15YwQ5jWkT6vVy+oWhXevoVmzSAwUOtEn9W0KbRpA2Fh0LIlPH+udSIhzI8UN0IIo1jprVjccDH2Vvbsur6L8QfHax0pii5durBkyRJq1KihXrCwUDvlpUkDZ89C377aBjTSr79C1qyqH+h335nFlCEhTIoUN0IIo+VImYMp1acAMGj3IJNbHv7VV1/RrFkzcuXKFfFi2rSqwAHVXHPDBm3CxYCjIyxZoqYOrVgBCxZonUgI8yLFjRAiRjoU7kCzfM0INYTSdFVTngY+1TrS51WvrtZYA7RrB3fvapvHCCVKREwq7tYNzp3TNo8Q5kSKGyFEjOh0OmbVnkW2FNm4/eI27da3i1ilpLGjR4+yZs0arly5EvWTo0dDkSLw9Ck0b24Wa6379VNNz9+8gW++UcvEhRCfJ8WNECLGnGycWNF4BdZ6azZc3MAUvylaRwJg0qRJNGrUiE2bNkX9pI0NLF0KDg6wf79qy23i9Hr48091Z+3cOejSRebfCGEMKW6EELFS2K0wPlV9AOi3ox/H7h7TOJERcuSAefPU87FjYeNGbfMYwdUVli2L2Afn99+1TiSE6ZPiRggRa12Kd6FR7kYEhwXzzcpvePLmidaRPq9JE+jx/52WW7eG69e1zWOEcuVg5Ej1vGtXOHNG2zxCmDopboQQsabT6Zhbdy5Zk2flZsBNmq9pTmhYqGZ53s/90el0nz5w/HgoWVJtIvPNNxAUFP/hvtCAAWpe9Nu3KvLLl1onEsJ0SXEjhPgiLrYurPl2DXaWdmy/up2he4dqHenzrK3VGuuUKeHECbXZn4mzsIBFiyB9erh0Cdq2lfk3QnyMFDdCiC9WwLUAc+rMAeBn359Zf2G9pnk+O3IDkCmTmq2r08HMmRF74ZiwVKlg1SqwsoI1a2DMGK0TCWGapLgRQsSJFgVa0KOEmsvSel1rLj25pHEiI1SvDt7e6vl338HRo9rmMUKpUmovQoDBg2HzZm3zCGGKpLgRQsSZCVUnUCZTGV4EvaDh8oa8evcqQd//u+++Y+7cuVSsWNH4L/L2hnr11LybBg3A3z/+AsaRTp2gc2d1W6p5c7h8WetEQpgWkyhupk+fjoeHB7a2tpQsWZKjn/jtac6cOXz99dckT56c5MmTU7ly5U8eL4RIOFZ6K1Y0XkFah7T8++hfWq1tlaANNitUqECHDh3Imzev8V/0fo117txw7x40amQWE4ynTgVPTwgIgPr1ZYKxEB/SvLhZvnw5Xl5eDB06lJMnT1KwYEGqVavGw4cPoz1+7969NGvWjD179nD48GEyZsxI1apVuWsG26kLkRS4ObqxpskarPXWrLuwjiG7TX+zPJycYP16cHGBw4fVemsTn61rYwOrV4Obm9rgr21b1UlcCAGWWgfw8fGhU6dOtGvXDoBZs2axadMm5s+fz4ABA6Icv3jx4kgfz507l9WrV7Nr1y5at24d5figoCCCPvgt7MWLFwAEBwcTHBwcl99K+Pni+rwiMrnOCeNLrnOxtMWYVXMW7Te2Z/SB0eRIkYPm+ZrHdcQoTp8+zcOHD8mVKxeZMmWK2Rd7eKD780/0deuimzeP0Pz5CevSJX6CfuBLrnOqVLB8uY7KlfWsWaNj4MBQRo2SCudj5O+OhBFf1zkm59MZNGwK8+7dO+zt7Vm1ahX169cPf71NmzY8f/6c9es/v+Li5cuXpEmThpUrV1K7du0onx82bBjDhw+P8vqSJUuwt7f/ovxCiE9bdG8Rqx+uxkpnxahso8iZLGe8vt+4ceM4dOgQnTt3pmbNmrE6R7a1a8m7cCFhFhb4DR7MwyJF4jhl3NuzJyNTpqic3bufpFKl2xonEiLuvXnzhubNmxMQEICTk9Mnj9V05Obx48eEhobi6uoa6XVXV1cuXLhg1Dn69+9PunTpqFy5crSfHzhwIF7vuwGjRm7e38r63MWJqeDgYHbs2EGVKlWwsrKK03OLCHKdE0ZcXOfqhuq8W/2OjZc24nPPh4NtD5LJOYYjKjHwxx9/AJA3b95YFzfUqEFYWBgWixZRatIkQvbsgQIF4jBlZHFxnWvWBDu7UMaO1TNrVmHq1StA2bKmfVtNC/J3R8KIr+v8/s6LMTS/LfUlxo4dy7Jly9i7dy+2trbRHmNjY4ONjU2U162srOLthzs+zy0iyHVOGF96nZc0WkLp+aU58+AMDVc1xLedL042cfuLxXvv97fR6/Vf9rMxdy7cuYNuzx6s6tcHPz9Ily5uQn7El17nn3+Gq1dh5UodTZpYcuQIZM8ehwETEfm7I2HE9XWOybk0nVCcKlUq9Ho9Dx48iPT6gwcPSJs27Se/dsKECYwdO5bt27dTIB5/qxJCfBkHawc2NN2AazJXzjw4Q6MVjXgX+i5e39OoTfw+xdpazdbNlQvu3IHateFVwi5rjykLC7UPYYkS8PSpivz0qdaphNCGpsWNtbU1RYsWZdeuXeGvhYWFsWvXLjw9PT/6dePGjWPkyJFs3bqVYsWKJURUIcQXcHdxZ1PzTSSzSsbOazvpuKEj8THdL07PmTw5bNoEqVPDqVNqQ5lQ7fpmGcPOTi36ypRJtWioWxcCA7VOJUTC03wpuJeXF3PmzGHhwoWcP3+eH374gdevX4evnmrdujUDBw4MP/6XX35hyJAhzJ8/Hw8PD+7fv8/9+/d5ZeK/VQmR1BVNV5RVTVah1+lZdGYRg3YPirf3+uKRm/eyZIENG8DWFjZuNIsl4mnTqprMxQUOHoSmTSEkROtUQiQszYubb7/9lgkTJuDt7U2hQoU4ffo0W7duDZ9kfOvWLfw/2DF05syZvHv3jsaNG+Pm5hb+mDBhglbfghDCSNWzVQ/vQTXmwBhmHpupcSIjlCoV0YNq9uyIdg0mLF8+VZPZ2Kg/f/jB5GsyIeKUSUwo7tatG926dYv2c3v37o308Y0bN+I/kBAi3rQr3I7bL24zdO9Qum3pRppkaWiUp1GcnLt9+/aUK1eO0qVLx8n5wjVqpJprfv89jBqlblX16BG37xHHvv4ali1T0efOVSM6I0dqnUqIhKH5yI0QIukZUnYInYp0IswQRrPVzdh2ZVucnLdWrVr06NEjfhYZfPddRHXQsycsWRL37xHH6tdXNRmomux9w00hEjspboQQCU6n0zGz1ky+yfMNwWHBNFjeAN+bvlrH+rxBgyJGbNq0gS1btM1jhM6d4f0+pt26qRVVQiR2UtwIITSht9DzZ8M/qZGtBoEhgdRaUovj945/0TnPnTuHr69vpHl6cUqng0mT1MqpkBBo2BB2746f94pDQ4ZA9+7qefv2sHy5tnmEiG9S3AghNGOtt2Z1k9WUcy/Hy3cvqf5ndf59+G+szzdo0CDKli3Lxo0b4zDlf1hYwO+/q41k3r6FOnVg//74e784oNPB5MnQsaNqrtmiBaxbp3UqIeKPFDdCCE3ZWdmxodkGiqcrzpPAJ1ReVJnzj85rHevTrK1h1SqoXh3evFH9Dw4d0jrVJ1lYwKxZ0LKl2q6nSROzuKsmRKxIcSOE0JyTjRNbWmyhgGsB7r+6T4WFFTj36FyMz5OgfYBtbGDNGqhcGV6/VoXO0aMJ9/6xoNerQadvvoHgYHVXbccOrVMJEfekuBFCmISU9inZ1XoXhdIW4sHrB5RfUJ5/Hv6jdaxPe78lcPny8PIlVK0Kx45pneqTLC1h8WJ1N+39XbXNm7VOJUTckuJGCGEyUtmnYlfrXRROW5hHbx5RYWEFzjw4Y/TXvx+5ibMdio1hb692Ly5TBgICoFIlk5+DY2UFK1dCvXoQFKSWjMscHJGYSHEjhDApKexSsKv1Loq6FeXxm8dUXFiRk/4ntY71aQ4OagJLhQpqBKd6ddgWN3v3xBcbG1XgNGmiblE1biyrqETiIcWNEMLkJLdLzs7WO8MnGZdfUJ69N/ZqHevTHBxUU6datVS3yrp1TX44xMpK3aJq1UpNMm7eHP74Q+tUQnw5KW6EECbJxdaFna13Rlomvv7C+k9+TatWrRg9ejTFixdPoJT/YWenJhk3bgzv3qk/Fy/WJouRLC1hwYKIZeJt2oCPj9aphPgyUtwIIUyWk40TW1tupV7OegSFBtFoRSMWnF7w0eMbN27MwIEDKVSoUIJljMLaGpYuVVVCaKhae23i1YKFheoJ+uOP6uPevaFvX1XsCGGOpLgRQpg0W0tbVjVZRdtCbQk1hNJufTsmHJqQsMu+Y8rSEubPVz2oQFULvXqZdLVgYaFqsF9+UR9PmKDqs+BgbXMJERtS3AghTJ6lhSXz6s6jt2dvAPru6Eu3zd0ICQuJdNzVq1c5efIkjx8/1iJmZBYWqlXDhAnq48mToWlTtf7aROl00K+f6j+l18Off6ql4q9eaZ1MiJiR4kYIYRYsdBaMrzKeCVUmoEPHjOMzqLu0Li+CXoQf8+OPP1K0aFE2bNigYdIP6HRq1GbJkoj119WqwdOnWif7pNat1ep2e3u16KtMGbh1S+tUQhhPihshhNnQ6XT0/qo3q5usxs7Sji1XtvD1719zO+C21tE+rVkzVSU4Oak9cEqWhPOm3WKiRg3VEzRNGvj7byhRAo4c0TqVEMaR4kYIYXYa5G7Avrb7cE3mypkHZyg5tyTH7h4z7Xk4FSrAgQPg7g5XrkCpUia/NXDJkqqjRIEC8OCB2oh56VKtUwnxeVLcCCHMUvH0xfHr6Ee+NPnwf+UfaQQnQXcojon8+VV7hq+/hhcvoHZtLHx8wISLMnd3VZPVqaN2M27eHIYMMem50UJIcSOEMF/uLu4cbH8wfKn4+1YN/51obFJSp4adO6FTJzAY0A8YQJEpU1R3cRPl6Ahr16rl4QCjRqm9Ck186pBIwqS4EUKYNScbJ9Z8u4YR5UeEvzbu4Dj8X/prmOozrK3VxjJTp2LQ68m4dy+WZcrApUtaJ/sovR7GjVMrqWxtYetWKFoUTpp4ZwyRNElxI4QwexY6C4aUG0KxdMUAuPL0CoVnF2bXtV0aJ/sEnQ66dyd061beurig++cfVS2sWKF1sk9q3VpNLM6SBW7cgK++gnnztE4lRGRS3AghEo0eHXrQpXcXsubOyoPXD6iyqAqDdw826dtUhnLl2OvjQ1jZsmpDmW+/hR491AQXE1WwIJw4ETEPp2NHVfS8ePH5rxUiIUhxI4RINFq1asX0CdM5M+IMnYt0xoCBn31/pvyC8twKMN2NWoJSpCB061YYMEC98OuvajWVCS8Xd3FRfUF//lntV7hoERQurFZXCaE1KW6EEImOvZU9s+vMZnnj5TjZOHHw9kEKzSrEyn9Xah3t4ywtYcwYtXteypRw+jQUKQLTp5vsaioLC/jpJ9i3DzJlgmvXoHRp9W2EhmqdTiRlUtwIIRKNO3fucPHiRQICAgBokrcJp747RfF0xXn29hlNVjWh+ermPA004WU+tWvD2bNqJ+O3b6FbN/XagwdaJ/uoMmXURn9NmkBIiCp4KlVSxY4QWpDiRgiRaHTq1IlcuXKxfv368NeyJM/CgfYHGFJ2CHqdnqX/LCXfjHxsurRJw6Sf4eamNvibMgVsbNTzvHlVGwcTHcVxcYFly+D33yFZMjWakz8/TJsme+KIhCfFjRAi0bPWWzOiwggOdzhMrlS58H/lT+2ltemwvgPPAp9pHS96FhZqYvGJE2oG75Mn0KIF1K0Ld+9qnS5aOh20batGccqVU1v3dO8OFSvC1atapxNJiRQ3QohE43PtF4qnL87JzifxKuWFDh3zT88n1/RcLD271HRbN+TNq2bpjhihmm/+9RfkyQNz5pjsKE7WrKov1bRpqvnmvn2qhcPEiRAcrHU6kRRIcSOESHQ+1X7BzsqOidUm4tvOlzyp8/Dw9UOar2lOjcU1uPbMRCeJWFurngenTqkOli9eQOfOULYsnDmjdbpoWVhA165q+lD58moUp08ftZXPwYNapxOJnRQ3QohEIyajL6UzlebUd6cYVWEUNnobtl3dRt4ZeRm5bySBwYHxmPIL5M0Lhw6pIRB7e9X0qUgR+PFH+P8kalOTJQvs2gVz50KKFKrYKVMGOnSAx4+1TicSKyluhBCJjrGNM6311gwqO4izP5ylYuaKvA15i/deb3JPz83Kf1ea5q0qvR68vODCBWjcWK25njIFcuaEP/4wydm7FhaqmLl4Uf0JMH8+5MihosutKhHXpLgRQiR52VNmZ2ernSxvvJyMThm5GXCTJquaUG5BOU75n9I6XvQyZoSVK2HbNlUlPHgAbdpA8eKwZ4/W6aKVKpUawTlwQM3BefZMDTrlywcbNpjsFCJhhqS4EUIkGg0bNqRXr17kypUrxl+r0+lokrcJF7pdYHj54dhZ2uF7y5civxWhxZoWXH1qost9qlZV827GjlXtu0+eVMuT6tSBc+e0Thet0qXVIrDZsyFNGtUvtF49tTfO8eNapxOJgRQ3QohE47vvvsPHx4dixYrF+hz2VvZ4l/PmYreLNMvXDIAlZ5eQa3ouumzqYprdxm1soH9/td66Wze12/Fff6mNZtq1M8l12JaWak705cswcKD6FvbsUQNP9eub7DxpYSakuBFCiGhkdM7IkkZLONn5JNWzVSckLISZx2eSdWpW+mzvY5pFTurUqi/Vv/9CgwZq/s2CBWo+TocOJrllsJMTjB6tphC1aqXm56xfr7b2adLEZAefhImT4kYIkWg8fPiQ27dv8/r16zg7Z2G3wmxpsYU9bfZQKkMpAkMCmXh4IpmnZKbrpq7cfH4zzt4rzuTIAWvWwJEjUKOGmnQ8f74qcjp2hOvXtU4YhYeHmg/977+qMTqoKUX58kHLlvDPP5rGE2ZGihshRKLRokULMmXKFKn9Qlwp71GeQ+0Psan5Jr7K+BVBoUHMOD6DbL9mo/369lx6cinO3/OLlSypWjccPgzVq6vGT/PmQfbs0KyZSU5wyZVLtXE4c0YNPhkMsHixusNWq5baEFAmHovPkeJGCCGMpNPpqJm9JgfaHWBPmz1UylyJkLAQfj/9O7mm5aLu0rrsurbL9JaQlyoFW7aoPXKqVVMjOcuWqQku5curTuQmtoQ8f341+HTiBHzzjbpdtXmziluqFKxeLZ3HxcdJcSOESDQSqqjQ6XSU9yjPztY7OdzhMLVz1MaAgY2XNlJ5UWUKzCrAnBNzeBP8JkHyGM3TE7ZuVTsdt2qlZvXu26f6VeXJAzNnqt2PTUiRIrBihdoj54cfwNZWdaNo3BiyZYNffoFHj7ROKUyNFDdCiETH2E384kKpDKXY2GwjF7peoGvxriSzSsY/D/+h81+dyTgpI/129OPi44sJlscohQqpCS7Xr0O/fmpW78WL0KULpEunljGdOKF1ykiyZYMZM+DmTdWJIkUKuHEDBgyADBnUvJxDh+SWlVCkuBFCiDiQM1VOptWcxh2vO0ysOpHMLpl5GviU8YfGk2t6LsrML8Pvp37n1btXWkeNkCGDGvq4cwcmT1YTXl6/Vk05ixVTjzlzTGo0J00a1UP09m01R7pYMXj3Ts3LKV1a1W1Tp8poTlInxY0QItEwhbkuLrYueHl6cbn7ZdY3XU+dHHXQ6/QcvH2Q9hva4zbRjU4bOrH/5n7CDCYyz8XREXr2VOuu9+2D5s1Vs84TJ9QojqsrNG0KmzaZTK8Ee3u1hc+xY+rRvr26ZXXmjPpW0qVTGwOuXg1BQVqnFQlNihshRKKTkLelPkZvoaduzrpsaLaBW71uMabSGLKlyMard6+Ye2ou5RaUI9OkTPTb2Y8rb66YRGGGTqc6jS9eDHfvwvjxavn427ewfDnUrg3p00OPHuDnZzL3gIoVU4vA7t1TozbFiqmFYRs2qLk5bm5qvs6ePTpCQ7X/2RDxT4obIUSiUbt2bTp37kzWrFm1jhJJOsd0DCgzgEvdLrG3zV7aFWqHs40zd1/eZfLRyfS51Ie8s/IyePdgjt87bhqFTqpU0KcPnD+vhkZ69lT3hB49UhsFlioFmTKp1/fvN4mlS8mTQ/fuKu6//6pNm9OnVz2sZs2CatUsad++Gl26WLBjh8kMQol4oDOYxP9FCefFixc4OzsTEBCAk5NTnJ47ODiYzZs3U7NmTaysrOL03CKCXOeEIdc5fr0NecvWK1tZfGYxGy5s4J3hXfjn0jump27OutTLWY8KmStgrbfWMOkHQkJgxw5YtEgtH3/1wfyhNGlU34S6daFCBXXfyASEhsLu3Wrgae1aA0+fRozcpEypBqNq1YIqVcDFRbuciUl8/d0Rk3+/pbiJQ/KPQcKQ65ww5DonjODgYFZvXM27LO/YcGkDW69s5XVwxA7LjtaOVM1alapZq1IlSxUyJ8+sYdoPvH2rCp3Vq9X9n2fPIj5nYwPlyqndkatXV7e2TOBW4Zs3wYwff4w7d0qybp2ex48jPqfXQ5kyULOmeuTNaxKRzZIpFDeWcfauQgihsRcvXhASEoKDgwPW1iYy2mEEO70djfI2onWh1rwNecvu67tZf2E9Gy5t4P6r+6w+v5rV51cDkC1FNqpmqUqVrFWo4FEBZ1tnbULb2qrO43XqqPs7e/fC2rVqp72bN2H7dvXo1Uv1VqhRQ3UrL1tWjfJowMoKChV6xE8/hTFzph5fXzVHetMm1dtq3z716N9f3XGrXFkNQpUvrxaWCfMhxY0QItGoW7cu+/btY/ny5TRp0kTrOLFia2lLzew1qZm9JjMNMzl+7zjbrmxj+7XtHLlzhCtPr3Dl6RVmHJ+BXqeniFsRymQqw9eZvqZMpjKkTpY64UNbWan7OlWqqEnGFy6oHZG3bFHzcW7cUBsEzpypjs+TR43slC+v/nR1TfDIlpaqcKlQASZMUD1Ft2xRtdnu3XDrllpqPn++Oj5btohCp3x5tRpLmC4pboQQwkRZ6Cwokb4EJdKXYEi5IbwIesHeG3vZfnU7269u5/LTyxy7d4xj944x6cgkAHKmzBle6JRIX4KcqXJioUvAtSM6HeTOrR5eXmpezp49sG2bGhb55x+15PzcuYhiJ0cOtXtyyZLqkT+/KpgSUJYs0LWregQGqqh79qjHiRNw5Yp6zJmjjvfwUHOq3z8KFVJ344RpkOJGCJFoJPYphE42TtTNWZe6OesCcCvgFgduHcD3pi8Hbh/gn4f/cPHJRS4+ucjcU3MBcLB2oKhbUYqnK06xdMUonr44mV0yJ9xyeQeHiNtXAI8fg6+vuo21b5/amObSJfVYuFAdY2sLRYuqQqdECShYUDX71OsTJLKdnZoqVL26+jggAA4ciCh2Tp1Sg1E3bqgWXaC2BSpSREUuXFgVO7lzq9dFwpPiRgiR6JjCPjcJIZNzJprnb87/2rv3oKjOuw/g373vwi4g13UVFBNAK0ZUgqPmjX9otY1majNjb6ZxzEz/eXGqYcapSUMynTRSk4lDYhypnWn/yNQmvUxsTdJOKTXUBBWE4uAbxBsUFLmJsAsLeznnef847OIqphpgj3v4fpzfnD3PHo4/H9H98ZznnOcHS34AAOgf6UdNRw1O/uckaq7VoOFGA4b8Q6j+TzWq/1Md/rokaxKWpC9RIkPZ5qfnR2f+Tmqqstz3t7+t7Pf3K6uWnzmjRG0tMDAAfP65EiE2G5CfrxQ6oXjsMSBx+nNOTFTuqNq0Sdl3u5XbzU+fHo++vvHXISaTMjE5VOwUFCgp866s6cfihohII5JtydicuxmbczcDACRZQnNfM+quK5euznaexbnucxgYHcDJ9pM42X4y4uuzErPChU5uSi5yU3KRl5KH1LjU6SsYk5MjKwdZBi5dGi926uuBpibA6x1/HHFE0lnKEElenrJ8RCiczmm73SkhAVi3TglAmWZ09apSo509CzQ2KjE4OP76dk6nkvLCheNX8BYtUubxzJC6fNqxuCEizdD6ZakHZdAbkJ+ej/z0fOxYtgMA4Av60NzXjKbuJjT1jEV3E657rqN9sB3tg+34+NLHEedJsiaFi53c5Fw8mvwo5ifNx/yk+ciwZ0ztnB69XilU8vKA555T2iQJuHIFOHcuMjo6lJm/7e3KnJ7bJSQo1UNODpCdDSxYAF1mJmw9Pcr5pnBOj04HPPKIEs8+q7QJoVy2ChU3jY3K5ayODqCrS4kTJyLP43Ao048WLLg7srKUSdB0f9hVRKQ5M+Wy1FdhMVpQ4CxAgbMgor1/pB/ne86jqbsJzX3NuHjzIi7evIj2wXYMjA6g9notaq/X3n0+gwVZiVnhYicUmQmZcDlcmO2YjTjTJB/oZzAon/q5ucDWrbcl3a9MTL5wQVnV/MIFJa5eVa4d1dYqMcYIYAMAUVysVAvZ2UrMm6c8ynjOHOWe7zlzlOJoEnS68dOHrsABymhOS4vy4OfmZiXd5maldvN4lIGqiRZkNxiUNBcsUCYzh1K9PRITOfITwuKGiDRjw4YNmDdvHrKystROJeYk25Lx5Lwn8eS8JyPaRwIjuHLrSrjYabnZgtZbrWgbaEOHuwM+yYdL/Zdwqf/SPc+dZE2Cy+FSih377PBrl8MFp92JtLg0pMWnIcma9GCjQMnJypP3nngist3nU25tClUNra1AayvE1asQbW3QBwJK+5Ur9z633X53weNyAWlpynN6QtuUlAea6JyYqMyRLiqaOOXLl5Xa7PZobVXeD+3fS1xcZLHjcikp3hmpqVG/GS3qWNwQkWa8/PLLaqegOTaTLXxp604BKYDrnutoG2i7K665r6HT04mR4AgGRgcwMDqAL3q/+NLfy6AzICUuBalxqUiLS4vcxivbWdZZSLImRYTVaI0crbNYlJm8ixdHnD8YCOCT48fxVEEBTB0d4aIH7e3KQqGhGBxUbmFvaVHiy+h0SpF1e8GTlqbErFnjkZQUuR8XFzHMco+UASjTkG7cUAqbK1fG0712bTz6+5VpSaEbz/6bUMqhSEtT2m5P8c6w22NnZOihKG4OHTqEN998E11dXVi6dCkOHjyIojvL2tv84Q9/QGlpKdra2pCTk4P9+/fjqaeeimLGRERkMpjCl6EmIoTAoG8QNzw30OnpjIwhZds11IU+bx/cPjckIaFnuAc9wz0PlofedFfBEwqH2QG72Q672Q6bwYbLg5fh90pImpuE+AW5sJuXh9+3m+2wGW3QDQ8rS4xfuxZZ9Ny4oSwc2tsL9PQoFYUQwM2bSjQ3P0DSpvGCJ7RNSFAm3tjtSoy91jscmGO3Y47djv/JdQDL7ZHHWa3wjujCKYeis3M81VD09SnFUn+/Ehcu3H/KRmNkjZaUpKQcCodD2cbH63DjRjrU/FhWvbj54IMPUFJSgoqKCqxcuRLl5eXYuHEjWlpakD7BI7pramrw/e9/H2VlZdi8eTOOHj2KLVu2oKGhAfn5d/9kQUQzh8/ngxACJpMJhig9E4XuTafThYuMRWmLvvRYX9CHmyM30Tvciz5vH3q9Y9s79gdGBzDoGwyPBslCRkAOoNfbi15v733l9Xb72/fOGTrYzXbEm+NhM9pgM9lgTbbCmm6F7XEbrEYHrMY02EwrYNWbYQ0CNr+AdSSgxLAPtmEfrJ4RWIdGYR4agdnjhcnjhXlwCKbBIZj9EkxyAGapF6abvTD3ACYZMEuASRrbysrr+xooMRgQZ7PhUZsNj1qtym3zoQjt59qApTbIFit8OhuGZRuGgla4AzYM+Gy4NWqDx2fG4IgSA14zbg0rcdNjhlcywx80I9Bngr/PjG6Y0QEz/HeEgB6AEbm5eSgtva+/jmmhenFz4MAB/OhHP8KOHcpM/oqKCnz88cf49a9/jb179951/Ntvv41vfOMb2LNnDwDgtddeQ2VlJd59911UVFRENXcierisW7cOn3/+Of70pz/hmWeeUTsdegAWoyU8D+d+CSEw5B8KFzoThcfvwZB/CEP+IXh8HrR1tsGWaMNwYDjcPuwfDi9WKiDg8Xvg8Xu++h9GDyBxLCbJKHQwy3ql+JF1MEkCpqCAQRYwSAIGGTDKEgxiCAZ5CAaBsTaEXxvE2L4MGLz3eN+kTB0y2MbfTxRAsgzkCEA/QRgmaIPQATBCFvMB/N/kO+Cr9ptqvzMAv9+P+vp6vPjii+E2vV6P9evX49SpUxN+zalTp1BSUhLRtnHjRhw7dmzC430+H3w+X3jf7XYDUFYtDQQCk/wTRAqdb6rPS5HYz9ERi/0cuhU8GAzGTN6x2M8PE6veCmecE8445389NhAIoLKyEl//+tfvWq1aFjK8AW+44BkKDMEX9GEkMIJRaRSjwVGMBEfgC/owGhzfHw2OKseNvb79uJHgCAJSAAE5AL/kD2+DclDZlwLwy2PbsffvFNQJBA0SEDMDkQJAAI/3XcP/TtNn7P1Qtbjp6+uDJEnIuGPRtIyMDFy4x4XArq6uCY/v6uqa8PiysjL87Gc/u6v973//O+LiJnl74j1UVlZOy3kpEvs5OmKpn/v7+wEADQ0NsMTYQj+x1M+x7qv0tXns15fSATCNxVcghIAECZKQEJADkCAhKAcRFEpIQgq/liFDFnJ4Kwkpok0S0oO9j7H9CdoApfgTY79CxwihvIaQIORQKPuu7Iwp/572er33fazql6Wm24svvhgx0uN2u5GZmYkNGzYgYZLPMbjTl/1UQFOH/RwdsdjP+/fvBwCsWLEiZm4yiMV+jlXs6+iYrn4OXXm5H6oWN6mpqTAYDOju7o5o7+7uhtM58RCj0+l8oOMtFsuEP8GZTKZp++aeznPTOPZzdMRSP4duBzYajTGTc0gs9XOsY19Hx1T384Ocawqfmf3gzGYzVqxYgaqqqnCbLMuoqqrCqlWrJvyaVatWRRwPKEOM9zqeiGYOLr9ARMBDcFmqpKQE27dvR2FhIYqKilBeXo7h4eHw3VPPPfcc5syZg7KyMgDArl27sHbtWrz11lvYtGkT3n//fZw9exZHjhxR849BRA8RLr9ANLOpXtx897vfRW9vL1555RV0dXWhoKAAf/vb38KThtvb26HXjw8wrV69GkePHsXLL7+Ml156CTk5OTh27BifcUNEWLt2LZxOJ2bPnq12KkSkItWLGwDYuXMndu7cOeF7n3766V1tW7duxdbbF08jIgLCI7xENLOpOueGiIiIaKqxuCEiIiJNYXFDRJqxevVqGAwGHD9+XO1UiEhFLG6ISDNkWYYsy2qnQUQqY3FDRJrB59wQEcDihog0iM+5IZrZWNwQERGRprC4ISLN4GUpIgJY3BCRBvGyFNHM9lA8oTiaQj/ZPcjS6fcrEAjA6/XC7XZzxdlpxH6Ojljs5+XLlyMhIQE2m21a/o1Ph1js51jFvo6O6ern0L/p+xmh1YkZNo577do1ZGZmqp0GERERfQUdHR2YO3fulx4z44obWZbR2dkJh8Mx5UPXbrcbmZmZ6OjoQEJCwpSem8axn6OD/Rwd7OfoYV9Hx3T1sxACHo8HLpcrYkHticy4y1J6vf6/VnyTlZCQwH84UcB+jg72c3Swn6OHfR0d09HPiYmJ93UcJxQTERGRprC4ISIiIk1hcTOFLBYLXn31VVgsFrVT0TT2c3Swn6OD/Rw97OvoeBj6ecZNKCYiIiJt48gNERERaQqLGyIiItIUFjdERESkKSxuiIiISFNY3EyRQ4cOYf78+bBarVi5ciVqa2vVTklzysrK8Pjjj8PhcCA9PR1btmxBS0uL2mlp2i9+8QvodDrs3r1b7VQ06fr163j22WeRkpICm82GJUuW4OzZs2qnpSmSJKG0tBTZ2dmw2Wx45JFH8Nprr3EF+Un617/+haeffhoulws6nQ7Hjh2LeF8IgVdeeQWzZ8+GzWbD+vXrcenSpajlx+JmCnzwwQcoKSnBq6++ioaGBixduhQbN25ET0+P2qlpSnV1NYqLi3H69GlUVlYiEAhgw4YNGB4eVjs1Taqrq8Mvf/lLPPbYY2qnokm3bt3CmjVrYDKZ8Ne//hVffPEF3nrrLcyaNUvt1DRl//79OHz4MN599100Nzdj//79eOONN3Dw4EG1U4tpw8PDWLp0KQ4dOjTh+2+88QbeeecdVFRU4MyZM4iPj8fGjRsxOjoanQQFTVpRUZEoLi4O70uSJFwulygrK1MxK+3r6ekRAER1dbXaqWiOx+MROTk5orKyUqxdu1bs2rVL7ZQ05yc/+Yl44okn1E5D8zZt2iSef/75iLZnnnlGbNu2TaWMtAeA+PDDD8P7siwLp9Mp3nzzzXDbwMCAsFgs4ne/+11UcuLIzST5/X7U19dj/fr14Ta9Xo/169fj1KlTKmamfYODgwCA5ORklTPRnuLiYmzatCni+5qm1l/+8hcUFhZi69atSE9Px7Jly/CrX/1K7bQ0Z/Xq1aiqqsLFixcBAOfOncNnn32Gb37zmypnpl2tra3o6uqK+P8jMTERK1eujNrn4oxbOHOq9fX1QZIkZGRkRLRnZGTgwoULKmWlfbIsY/fu3VizZg3y8/PVTkdT3n//fTQ0NKCurk7tVDTt6tWrOHz4MEpKSvDSSy+hrq4OP/7xj2E2m7F9+3a109OMvXv3wu12Y+HChTAYDJAkCa+//jq2bdumdmqa1dXVBQATfi6G3ptuLG4oJhUXF+P8+fP47LPP1E5FUzo6OrBr1y5UVlbCarWqnY6mybKMwsJC7Nu3DwCwbNkynD9/HhUVFSxuptDvf/97/Pa3v8XRo0exePFiNDY2Yvfu3XC5XOxnDeNlqUlKTU2FwWBAd3d3RHt3dzecTqdKWWnbzp078dFHH+HEiROYO3eu2uloSn19PXp6erB8+XIYjUYYjUZUV1fjnXfegdFohCRJaqeoGbNnz8bXvva1iLZFixahvb1dpYy0ac+ePdi7dy++973vYcmSJfjhD3+IF154AWVlZWqnplmhzz41PxdZ3EyS2WzGihUrUFVVFW6TZRlVVVVYtWqViplpjxACO3fuxIcffoh//vOfyM7OVjslzVm3bh2amprQ2NgYjsLCQmzbtg2NjY0wGAxqp6gZa9asuetRBhcvXsS8efNUykibvF4v9PrIjzqDwQBZllXKSPuys7PhdDojPhfdbjfOnDkTtc9FXpaaAiUlJdi+fTsKCwtRVFSE8vJyDA8PY8eOHWqnpinFxcU4evQo/vznP8PhcISv3SYmJsJms6mcnTY4HI675jDFx8cjJSWFc5um2AsvvIDVq1dj3759+M53voPa2locOXIER44cUTs1TXn66afx+uuvIysrC4sXL8a///1vHDhwAM8//7zaqcW0oaEhXL58Obzf2tqKxsZGJCcnIysrC7t378bPf/5z5OTkIDs7G6WlpXC5XNiyZUt0EozKPVkzwMGDB0VWVpYwm82iqKhInD59Wu2UNAfAhPGb3/xG7dQ0jbeCT5/jx4+L/Px8YbFYxMKFC8WRI0fUTklz3G632LVrl8jKyhJWq1UsWLBA/PSnPxU+n0/t1GLaiRMnJvz/ePv27UII5Xbw0tJSkZGRISwWi1i3bp1oaWmJWn46IfiYRiIiItIOzrkhIiIiTWFxQ0RERJrC4oaIiIg0hcUNERERaQqLGyIiItIUFjdERESkKSxuiIiISFNY3BAREZGmsLghIiIiTWFxQ0RERJrC4oaIiIg0hcUNEcW83t5eOJ1O7Nu3L9xWU1MDs9mMqqoqFTMjIjVw4Uwi0oRPPvkEW7ZsQU1NDfLy8lBQUIBvfetbOHDggNqpEVGUsbghIs0oLi7GP/7xDxQWFqKpqQl1dXWwWCxqp0VEUcbihog0Y2RkBPn5+ejo6EB9fT2WLFmidkpEpALOuSEizbhy5Qo6OzshyzLa2trUToeIVMKRGyLSBL/fj6KiIhQUFCAvLw/l5eVoampCenq62qkRUZSxuCEiTdizZw/++Mc/4ty5c7Db7Vi7di0SExPx0UcfqZ0aEUUZL0sRUcz79NNPUV5ejvfeew8JCQnQ6/V47733cPLkSRw+fFjt9IgoyjhyQ0RERJrCkRsiIiLSFBY3REREpCksboiIiEhTWNwQERGRprC4ISIiIk1hcUNERESawuKGiIiINIXFDREREWkKixsiIiLSFBY3REREpCksboiIiEhT/h8fQaMICHNfEgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#funzione sigmoide per modificare la matrice delle distanze\n",
    "def sigmoid(x, dist=5):\n",
    "    return 1/(1+np.exp(x-dist))\n",
    "\n",
    "# Crea un intervallo di x, ad esempio da -10 a 10\n",
    "x_values = np.linspace(0, 10, 400)\n",
    "# Calcola i valori della sigmoide per ogni x\n",
    "sigmoid_5 = sigmoid(x_values, dist=5)\n",
    "sigmoid_4 = sigmoid(x_values, dist=4)\n",
    "sigmoid_3 = sigmoid(x_values, dist=3)\n",
    "# Crea il grafico\n",
    "plt.plot(x_values, sigmoid_5, label=\"sigmoid(x, dist=5)\", color='b')\n",
    "plt.plot(x_values, sigmoid_4, label=\"sigmoid(x, dist=4)\", color='r')\n",
    "plt.plot(x_values, sigmoid_3, label=\"sigmoid(x, dist=3)\", color='g')\n",
    "plt.axline(xy1=(5,0),slope=90,color='black', ls='--')\n",
    "\n",
    "# Aggiungi etichette agli assi e un titolo\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')\n",
    "# Aggiungi una legenda\n",
    "plt.legend()\n",
    "# Mostra il grafico\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "062d0d54-2d6e-478e-b273-f7d4b7d2c93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos = 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Embedding Value (Sum)')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#embedding\n",
    "\n",
    "# Ottieni un iteratore dal DataLoader\n",
    "data_iter = iter(dataloader_train)\n",
    "# Ottieni il primo batch\n",
    "batch = next(data_iter)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.lineplot(batch['wild_type'][0,:,:].sum(dim=1) - batch['mut_type'][0,:,:].sum(dim=1) )\n",
    "#plt.axline(xy1=(batch['pos_mut'][0], 0), slope=90, color='red', linestyle='--')\n",
    "pos = 31\n",
    "print(f'{pos = }')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Embedding Value (Sum)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8b46a1f-ded3-4ab0-bbcb-d91ab2483241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def output_model_from_batch(batch, model, device, train=True):\n",
    "\n",
    "    '''Dato un modello pytorch e batch restituisce: output_modello, True labels'''\n",
    "    \n",
    "    x_wild = batch['wild_type'].float().to(device)\n",
    "    x_mut = batch['mut_type'].float().to(device)\n",
    "    labels = batch['ddg'].float().to(device)\n",
    "    length = batch['length'].to(device)\n",
    "    # try:\n",
    "    #     hydra_slim = batch['hydra_slim'].to(device)\n",
    "    #     output_ddg = model(x_wild, x_mut, length,hydra_slim=hydra_slim, train = train)\n",
    "    # except:\n",
    "        \n",
    "        \n",
    "    output_ddg = model(x_wild, x_mut, length, train = train)\n",
    "    \n",
    "    return output_ddg, labels\n",
    "\n",
    "\n",
    "# def output_model_from_batch_HYDRA_SLIM(batch, model, device, train=True):\n",
    "\n",
    "#     '''Dato un modello pytorch e batch restituisce: output_modello, True labels'''\n",
    "    \n",
    "#     x_wild = batch['wild_type'].float().to(device)\n",
    "#     x_mut = batch['mut_type'].float().to(device)\n",
    "#     length = batch['length'].to(device)\n",
    "\n",
    "#     hydra_slim = batch['hydra_slim'].float().to(device)\n",
    "#     output_ddg = model(x_wild, x_mut, length, train = train)\n",
    "\n",
    "#     HYDRA1 = model(x_wild, hydra_slim, length, train = train)\n",
    "#     HYDRA2 = model(hydra_slim, x_mut, length, train = train)\n",
    "#     HYDRA_TOT = HYDRA1+HYDRA2\n",
    "    \n",
    "#     return HYDRA_TOT, output_ddg\n",
    "\n",
    "\n",
    "def training_and_validation_loop_ddg(model, dataloader_train, dataloader_test, dataloader_validation, path_save_fig, epochs=20, lr =0.001, patience=10):\n",
    "            \n",
    "    criterion =nn.MSELoss()# nn.HuberLoss()#nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    \n",
    "    pearson_r_train = []\n",
    "    pearson_r_test = []\n",
    "    #pearson_r_test_inv =[]\n",
    "    pearson_r_validation = []\n",
    "    \n",
    "    loss_ddg_train = []\n",
    "    loss_ddg_test = []\n",
    "    loss_ddg_validation = []\n",
    "\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "            \n",
    "        # Training Loop\n",
    "        model.train()\n",
    "        preds_ddg_train = []\n",
    "        preds_dgw_train = []\n",
    "        preds_dgm_train = []\n",
    "        preds_coerenza_train = []\n",
    "\n",
    "        labels_tot_epoch = []\n",
    "\n",
    "        for i, batch in enumerate(dataloader_train):\n",
    "            train = True\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output_ddg, labels = output_model_from_batch(batch, model, device, train=train)\n",
    "\n",
    "            # output_ddg_HYDRA_SLIM, labels_HYDRA_SLIM = output_model_from_batch_HYDRA_SLIM(batch, model, device, train=train)\n",
    "            \n",
    "            \n",
    "            if isinstance(output_ddg, list):\n",
    "                # Compute the loss for each output and sum them\n",
    "                loss_list = [criterion(output_aa, labels) for output_aa in output_ddg]\n",
    "                loss_ddg = torch.stack(loss_list).sum()\n",
    "                output_ddg  = torch.mean(torch.stack(output_ddg), dim=0)\n",
    "            else: \n",
    "                loss_ddg = criterion(output_ddg, labels)  #usa se NON uso hydra\n",
    "            \n",
    "            tot_loss = loss_ddg \n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            tot_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Collect predictions\n",
    "            preds_ddg_train.extend(output_ddg.cpu().reshape(-1).tolist())\n",
    "            labels_tot_epoch.extend(labels.cpu().tolist())\n",
    "\n",
    "        # Calculate and print train metrics\n",
    "        train_loss = mean_squared_error(preds_ddg_train, labels_tot_epoch)\n",
    "        train_correlation = pearsonr(preds_ddg_train, labels_tot_epoch)[0]\n",
    "        train_spearman = spearmanr(preds_ddg_train, labels_tot_epoch)[0]\n",
    "        \n",
    "        loss_ddg_train.append(train_loss)\n",
    "        pearson_r_train.append(train_correlation)\n",
    "        \n",
    "        # Validation Loop\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "                \n",
    "        all_preds_validation = []\n",
    "        all_labels_validation = []\n",
    "        all_preds_test = []\n",
    "        all_labels_test = []\n",
    "      \n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            train = False\n",
    "            for i, batch in enumerate(dataloader_test):\n",
    "\n",
    "                output_ddg,labels = output_model_from_batch(batch, model, device, train=train) \n",
    "                    \n",
    "                all_preds_test.extend(output_ddg.cpu().reshape(-1).tolist())\n",
    "                all_labels_test.extend(labels.cpu().tolist())\n",
    "            \n",
    "            # Calculate validation metrics\n",
    "            test_loss = mean_squared_error(all_preds_test, all_labels_test)\n",
    "            loss_ddg_test.append(test_loss)\n",
    "            \n",
    "            test_correlation, _ = pearsonr(all_preds_test, all_labels_test)\n",
    "            pearson_r_test.append(test_correlation)\n",
    "\n",
    "            for i, batch in enumerate(dataloader_validation):\n",
    "                output_ddg,labels = output_model_from_batch(batch, model, device, train=train)\n",
    "\n",
    "                all_preds_validation.extend(output_ddg.cpu().reshape(-1).tolist())\n",
    "                all_labels_validation.extend([x for x in labels.cpu().tolist()]) #MESSO UN -  se DEF AL CONTRARIO\n",
    "            \n",
    "            # Calculate validation metrics\n",
    "            val_loss = mean_squared_error(all_preds_validation, all_labels_validation)\n",
    "            loss_ddg_validation.append(val_loss)\n",
    "            \n",
    "            val_correlation, _ = pearsonr(all_preds_validation, all_labels_validation)\n",
    "            pearson_r_validation.append(val_correlation)\n",
    "\n",
    "        \n",
    "        if val_correlation >= max(pearson_r_validation): \n",
    "            best_model = copy.deepcopy(model)\n",
    "            print(f'\\033[91mEpoch {epoch+1}/{num_epochs}')\n",
    "            print(f'Train -      Loss: {train_loss:.4f}, Pearson r: {train_correlation:.4f}, Rho spearman: {train_spearman:.4f}')\n",
    "            print(f'Validation - Loss: {val_loss:.4f}, Pearson r: {val_correlation:.4f}, Rho spearman: {spearmanr(all_preds_validation, all_labels_validation)[0]:.4f}',)        \n",
    "            print(f'Test -       Loss: {test_loss:.4f}, Pearson r: {test_correlation:.4f}, Rho spearman: {spearmanr(all_preds_test, all_labels_test)[0]:.4f}\\033[0m\\n')\n",
    "      \n",
    "\n",
    "        else:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "            print(f'Train -      Loss: {train_loss:.4f}, Pearson r: {train_correlation:.4f}, Rho spearman: {train_spearman:.4f}')\n",
    "            print(f'Validation - Loss: {val_loss:.4f}, Pearson r: {val_correlation:.4f}, Rho spearman: {spearmanr(all_preds_validation, all_labels_validation)[0]:.4f}',)        \n",
    "            print(f'Test -       Loss: {test_loss:.4f}, Pearson r: {test_correlation:.4f}, Rho spearman: {spearmanr(all_preds_test, all_labels_test)[0]:.4f}\\n')\n",
    "                  \n",
    "        if epoch > (np.argmax(pearson_r_validation) + patience):\n",
    "            print(f'\\033[91mEarly stopping at epoch {epoch+1}\\033[0m')\n",
    "            break\n",
    "\n",
    "        # if (epoch == 100) or (epoch == 150) or (epoch == 200) or (epoch == 250) or (epoch == 300):\n",
    "        #     torch.save(model, f'JanusDDG_{epoch}_ensamble.pth')\n",
    "    \n",
    "    pearson_max_val = np.max(pearson_r_validation)\n",
    "\n",
    "    return pearson_r_train, pearson_r_validation, pearson_r_test, loss_ddg_train, loss_ddg_validation, loss_ddg_test, pearson_max_val, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e35b0-73e5-455a-8047-ff4d81a531af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "929f7e6d-973d-4fb4-bf11-e89af78108e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Attention_DDG(nn.Module):\n",
    "    \n",
    "    def __init__(self, base_module, cross_att=False, dual_cross_att= False,**transf_parameters):\n",
    "        super().__init__()\n",
    "        self.base_ddg = base_module(**transf_parameters, cross_att=cross_att, dual_cross_att= dual_cross_att).to(device)\n",
    "    \n",
    "    def forward(self, x_wild, x_mut,length, hydra_slim=None, train = True):\n",
    "\n",
    "        delta_x_dir = x_wild - x_mut\n",
    "        output_TCA_dir = self.base_ddg(delta_x_dir, x_wild, length)\n",
    "    \n",
    "        # delta_x_inv = x_mut - x_wild\n",
    "        # output_TCA_inv = self.base_ddg(delta_x_inv, x_mut, length) #PROVAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "            \n",
    "            \n",
    "        return output_TCA_dir #(output_TCA_dir-output_TCA_inv)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c98febd2-26c0-4c75-9768-9a9871b263e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def apply_masked_pooling(position_attn_output, padding_mask):\n",
    "\n",
    "    # Convert mask to float for element-wise multiplication\n",
    "    padding_mask = padding_mask.float()\n",
    "\n",
    "    # Global Average Pooling (GAP) - Exclude padded tokens\n",
    "    # Sum only over valid positions (padding_mask is False for valid positions)\n",
    "    sum_output = torch.sum(position_attn_output * (1 - padding_mask.unsqueeze(-1)), dim=1)  # (batch_size, feature_dim)\n",
    "    valid_count = torch.sum((1 - padding_mask).float(), dim=1)  # (batch_size,)\n",
    "    gap = sum_output / valid_count.unsqueeze(-1)  # Divide by number of valid positions\n",
    "\n",
    "    # Global Max Pooling (GMP) - Exclude padded tokens\n",
    "    # Set padded positions to -inf so they don't affect the max computation\n",
    "    position_attn_output_masked = position_attn_output * (1 - padding_mask.unsqueeze(-1)) + (padding_mask.unsqueeze(-1) * (- 1e10))\n",
    "    gmp, _ = torch.max(position_attn_output_masked, dim=1)  # (batch_size, feature_dim)\n",
    "\n",
    "    return gap, gmp\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=3700):\n",
    "        super(SinusoidalPositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape (1, max_len, embedding_dim)\n",
    "        self.register_buffer('pe', pe)  # Salvato come tensore fisso (non parametro)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class TransformerRegression(nn.Module):\n",
    "    def __init__(self, input_dim=1280, num_heads=8, dropout_rate=0., num_experts=1, f_activation = nn.ReLU(), kernel_size=20, cross_att = True,\n",
    "                dual_cross_att=True):\n",
    "        \n",
    "        super(TransformerRegression, self).__init__()\n",
    "\n",
    "        self.embedding_dim = input_dim\n",
    "        self.act = f_activation\n",
    "        self.max_len = 3700 #lunghezza massima proteina\n",
    "        out_channels = 128  #num filtri conv 1D\n",
    "        kernel_size = 20\n",
    "        padding = 0\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=self.embedding_dim, \n",
    "                                             out_channels=out_channels, \n",
    "                                             kernel_size=kernel_size, \n",
    "                                             padding=padding) \n",
    "        \n",
    "        self.conv1d_wild = nn.Conv1d(in_channels=self.embedding_dim, \n",
    "                                             out_channels=out_channels, \n",
    "                                             kernel_size=kernel_size, \n",
    "                                             padding=padding)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(out_channels)\n",
    "        self.norm2 = nn.LayerNorm(out_channels)\n",
    "        \n",
    "        # Cross-attention layers\n",
    "        self.positional_encoding = SinusoidalPositionalEncoding(out_channels, 3700)\n",
    "        self.speach_att_type = True\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=out_channels, num_heads=num_heads, dropout=dropout_rate, batch_first=True )\n",
    "        self.inverse_attention = nn.MultiheadAttention(embed_dim=out_channels, num_heads=num_heads, dropout=dropout_rate, batch_first =True)\n",
    "        \n",
    "        dim_position_wise_FFN = out_channels*2\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(dim_position_wise_FFN)\n",
    "        self.router = nn.Linear(dim_position_wise_FFN, num_experts) #dim_position_wise_FFN*2\n",
    "\n",
    "        self.pw_ffnn = nn.Sequential(\n",
    "            nn.Linear(dim_position_wise_FFN, 512),\n",
    "            self.act,\n",
    "            nn.Linear(512, dim_position_wise_FFN)\n",
    "            )\n",
    "        \n",
    "\n",
    "        self.Linear_ddg = nn.Linear(dim_position_wise_FFN*2, 1)\n",
    "\n",
    "            \n",
    "\n",
    "    def create_padding_mask(self, length, seq_len, batch_size):\n",
    "        \"\"\"\n",
    "        Create a padding mask for multihead attention.\n",
    "        length: Tensor of shape (batch_size,) containing the actual lengths of the sequences.\n",
    "        seq_len: The maximum sequence length.\n",
    "        batch_size: The number of sequences in the batch.\n",
    "        \n",
    "        Returns a padding mask of shape (batch_size, seq_len).\n",
    "        \"\"\"\n",
    "        mask = torch.arange(seq_len, device=length.device).unsqueeze(0) >= length.unsqueeze(1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, delta_w_m, x_wild, length):\n",
    "            \n",
    "            delta_w_m = delta_w_m.transpose(1, 2)  # (batch_size, feature_dim, seq_len) -> (seq_len, batch_size, feature_dim)\n",
    "            C_delta_w_m = self.conv1d(delta_w_m)\n",
    "            C_delta_w_m = C_delta_w_m.transpose(1, 2)  # (seq_len, batch_size, feature_dim) -> (batch_size, seq_len, feature_dim)\n",
    "            C_delta_w_m = self.positional_encoding(C_delta_w_m)\n",
    "            \n",
    "            x_wild = x_wild.transpose(1, 2)  # (batch_size, feature_dim, seq_len) -> (seq_len, batch_size, feature_dim)\n",
    "            C_x_wild = self.conv1d_wild(x_wild)\n",
    "            C_x_wild = C_x_wild.transpose(1, 2)  # (seq_len, batch_size, feature_dim) -> (batch_size, seq_len, feature_dim)\n",
    "            C_x_wild = self.positional_encoding(C_x_wild)            \n",
    "            \n",
    "            batch_size, seq_len, feature_dim = C_x_wild.size()\n",
    "\n",
    "            padding_mask = self.create_padding_mask(length, seq_len, batch_size)        \n",
    "                    \n",
    "            if self.speach_att_type:\n",
    "                print('ATTENTION TYPE: Dual cross Attention\\n q = wild , k = delta, v = delta and q = delta , k = wild, v = wild \\n ----------------------------------')\n",
    "                self.speach_att_type = False\n",
    "                \n",
    "            direct_attn_output, _ = self.multihead_attention(C_x_wild, C_delta_w_m, C_delta_w_m, key_padding_mask=padding_mask)\n",
    "            direct_attn_output += C_delta_w_m \n",
    "            direct_attn_output = self.norm1(direct_attn_output)                        \n",
    "            \n",
    "            inverse_attn_output, _ = self.inverse_attention(C_delta_w_m, C_x_wild, C_x_wild, key_padding_mask=padding_mask)                   \n",
    "            inverse_attn_output += C_x_wild  \n",
    "            inverse_attn_output = self.norm2(inverse_attn_output)\n",
    "            \n",
    "            attn_output = torch.cat([direct_attn_output, inverse_attn_output], dim=-1)\n",
    "\n",
    "            output = self.pw_ffnn(attn_output)\n",
    "    \n",
    "            position_attn_output = attn_output + output\n",
    "    \n",
    "            position_attn_output = self.norm3(position_attn_output)\n",
    "    \n",
    "            gap, gmp = apply_masked_pooling(position_attn_output, padding_mask)\n",
    "    \n",
    "            # Concatenate GAP and GMP\n",
    "            pooled_output = torch.cat([gap, gmp], dim=-1)  # (batch_size, 2 * feature_dim)\n",
    "    \n",
    "            # Pass through FFNN to predict DDG\n",
    "            x = self.Linear_ddg(pooled_output)        \n",
    "            \n",
    "            return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e868852-c219-4b8f-af00-87d01884432a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "245621e5-bf63-4d4f-a923-f1bac00c67ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDGemb \n",
      " ----------------------------------\n",
      "ATTENTION TYPE: Dual cross Attention\n",
      " q = wild , k = delta, v = delta and q = delta , k = wild, v = wild \n",
      " ----------------------------------\n",
      "\u001b[91mEpoch 1/300\n",
      "Train -      Loss: 1.8881, Pearson r: 0.6380, Rho spearman: 0.6373\n",
      "Validation - Loss: 4.7021, Pearson r: 0.5590, Rho spearman: 0.4886\n",
      "Test -       Loss: 2.3662, Pearson r: 0.5075, Rho spearman: 0.5273\u001b[0m\n",
      "\n",
      "Epoch 2/300\n",
      "Train -      Loss: 1.1301, Pearson r: 0.8035, Rho spearman: 0.7935\n",
      "Validation - Loss: 4.6910, Pearson r: 0.5548, Rho spearman: 0.5308\n",
      "Test -       Loss: 1.9932, Pearson r: 0.5073, Rho spearman: 0.5099\n",
      "\n",
      "\u001b[91mEpoch 3/300\n",
      "Train -      Loss: 0.7244, Pearson r: 0.8791, Rho spearman: 0.8684\n",
      "Validation - Loss: 4.5552, Pearson r: 0.5702, Rho spearman: 0.5547\n",
      "Test -       Loss: 1.9255, Pearson r: 0.5208, Rho spearman: 0.5209\u001b[0m\n",
      "\n",
      "Epoch 4/300\n",
      "Train -      Loss: 0.4434, Pearson r: 0.9279, Rho spearman: 0.9228\n",
      "Validation - Loss: 4.9797, Pearson r: 0.5280, Rho spearman: 0.5389\n",
      "Test -       Loss: 2.0201, Pearson r: 0.5272, Rho spearman: 0.5220\n",
      "\n",
      "Epoch 5/300\n",
      "Train -      Loss: 0.2842, Pearson r: 0.9544, Rho spearman: 0.9500\n",
      "Validation - Loss: 4.9808, Pearson r: 0.5275, Rho spearman: 0.5209\n",
      "Test -       Loss: 1.9790, Pearson r: 0.5222, Rho spearman: 0.5227\n",
      "\n",
      "Epoch 6/300\n",
      "Train -      Loss: 0.2013, Pearson r: 0.9679, Rho spearman: 0.9628\n",
      "Validation - Loss: 4.8949, Pearson r: 0.5365, Rho spearman: 0.5245\n",
      "Test -       Loss: 1.9673, Pearson r: 0.5183, Rho spearman: 0.5221\n",
      "\n",
      "Epoch 7/300\n",
      "Train -      Loss: 0.1481, Pearson r: 0.9765, Rho spearman: 0.9732\n",
      "Validation - Loss: 4.9720, Pearson r: 0.5295, Rho spearman: 0.5291\n",
      "Test -       Loss: 1.9745, Pearson r: 0.5116, Rho spearman: 0.5253\n",
      "\n",
      "Epoch 8/300\n",
      "Train -      Loss: 0.1284, Pearson r: 0.9796, Rho spearman: 0.9767\n",
      "Validation - Loss: 5.1056, Pearson r: 0.5126, Rho spearman: 0.5252\n",
      "Test -       Loss: 1.9768, Pearson r: 0.5208, Rho spearman: 0.5263\n",
      "\n",
      "Epoch 9/300\n",
      "Train -      Loss: 0.1025, Pearson r: 0.9838, Rho spearman: 0.9816\n",
      "Validation - Loss: 5.0850, Pearson r: 0.5122, Rho spearman: 0.5104\n",
      "Test -       Loss: 1.9471, Pearson r: 0.5249, Rho spearman: 0.5320\n",
      "\n",
      "Epoch 10/300\n",
      "Train -      Loss: 0.0856, Pearson r: 0.9865, Rho spearman: 0.9844\n",
      "Validation - Loss: 5.0761, Pearson r: 0.5193, Rho spearman: 0.5252\n",
      "Test -       Loss: 2.0183, Pearson r: 0.5181, Rho spearman: 0.5302\n",
      "\n",
      "Epoch 11/300\n",
      "Train -      Loss: 0.0821, Pearson r: 0.9870, Rho spearman: 0.9855\n",
      "Validation - Loss: 4.9465, Pearson r: 0.5284, Rho spearman: 0.5243\n",
      "Test -       Loss: 1.9760, Pearson r: 0.5162, Rho spearman: 0.5276\n",
      "\n",
      "Epoch 12/300\n",
      "Train -      Loss: 0.0746, Pearson r: 0.9882, Rho spearman: 0.9857\n",
      "Validation - Loss: 5.0786, Pearson r: 0.5178, Rho spearman: 0.5226\n",
      "Test -       Loss: 1.9743, Pearson r: 0.5166, Rho spearman: 0.5276\n",
      "\n",
      "Epoch 13/300\n",
      "Train -      Loss: 0.0611, Pearson r: 0.9904, Rho spearman: 0.9889\n",
      "Validation - Loss: 5.2239, Pearson r: 0.5050, Rho spearman: 0.5152\n",
      "Test -       Loss: 2.0027, Pearson r: 0.5115, Rho spearman: 0.5257\n",
      "\n",
      "Epoch 14/300\n",
      "Train -      Loss: 0.0589, Pearson r: 0.9907, Rho spearman: 0.9888\n",
      "Validation - Loss: 5.0484, Pearson r: 0.5181, Rho spearman: 0.5108\n",
      "Test -       Loss: 2.0093, Pearson r: 0.5159, Rho spearman: 0.5328\n",
      "\n",
      "Epoch 15/300\n",
      "Train -      Loss: 0.0562, Pearson r: 0.9911, Rho spearman: 0.9890\n",
      "Validation - Loss: 5.1527, Pearson r: 0.5112, Rho spearman: 0.5323\n",
      "Test -       Loss: 2.0782, Pearson r: 0.5269, Rho spearman: 0.5354\n",
      "\n",
      "Epoch 16/300\n",
      "Train -      Loss: 0.0566, Pearson r: 0.9911, Rho spearman: 0.9897\n",
      "Validation - Loss: 5.1745, Pearson r: 0.5069, Rho spearman: 0.5317\n",
      "Test -       Loss: 1.9792, Pearson r: 0.5349, Rho spearman: 0.5472\n",
      "\n",
      "Epoch 17/300\n",
      "Train -      Loss: 0.0466, Pearson r: 0.9927, Rho spearman: 0.9909\n",
      "Validation - Loss: 4.8606, Pearson r: 0.5387, Rho spearman: 0.5496\n",
      "Test -       Loss: 1.9676, Pearson r: 0.5272, Rho spearman: 0.5348\n",
      "\n",
      "Epoch 18/300\n",
      "Train -      Loss: 0.0423, Pearson r: 0.9933, Rho spearman: 0.9917\n",
      "Validation - Loss: 4.8710, Pearson r: 0.5362, Rho spearman: 0.5513\n",
      "Test -       Loss: 1.9663, Pearson r: 0.5175, Rho spearman: 0.5295\n",
      "\n",
      "Epoch 19/300\n",
      "Train -      Loss: 0.0463, Pearson r: 0.9927, Rho spearman: 0.9904\n",
      "Validation - Loss: 4.9498, Pearson r: 0.5269, Rho spearman: 0.5424\n",
      "Test -       Loss: 1.9288, Pearson r: 0.5286, Rho spearman: 0.5416\n",
      "\n",
      "Epoch 20/300\n",
      "Train -      Loss: 0.0436, Pearson r: 0.9931, Rho spearman: 0.9918\n",
      "Validation - Loss: 4.8561, Pearson r: 0.5317, Rho spearman: 0.5260\n",
      "Test -       Loss: 1.9204, Pearson r: 0.5249, Rho spearman: 0.5417\n",
      "\n",
      "Epoch 21/300\n",
      "Train -      Loss: 0.0449, Pearson r: 0.9929, Rho spearman: 0.9908\n",
      "Validation - Loss: 5.1066, Pearson r: 0.5104, Rho spearman: 0.5240\n",
      "Test -       Loss: 1.9760, Pearson r: 0.5342, Rho spearman: 0.5489\n",
      "\n",
      "Epoch 22/300\n",
      "Train -      Loss: 0.0426, Pearson r: 0.9933, Rho spearman: 0.9917\n",
      "Validation - Loss: 4.8365, Pearson r: 0.5400, Rho spearman: 0.5591\n",
      "Test -       Loss: 1.9470, Pearson r: 0.5324, Rho spearman: 0.5423\n",
      "\n",
      "Epoch 23/300\n",
      "Train -      Loss: 0.0361, Pearson r: 0.9943, Rho spearman: 0.9928\n",
      "Validation - Loss: 4.8676, Pearson r: 0.5428, Rho spearman: 0.5468\n",
      "Test -       Loss: 2.0001, Pearson r: 0.5272, Rho spearman: 0.5405\n",
      "\n",
      "Epoch 24/300\n",
      "Train -      Loss: 0.0333, Pearson r: 0.9948, Rho spearman: 0.9932\n",
      "Validation - Loss: 4.9124, Pearson r: 0.5330, Rho spearman: 0.5456\n",
      "Test -       Loss: 1.9156, Pearson r: 0.5369, Rho spearman: 0.5457\n",
      "\n",
      "Epoch 25/300\n",
      "Train -      Loss: 0.0352, Pearson r: 0.9945, Rho spearman: 0.9929\n",
      "Validation - Loss: 4.9329, Pearson r: 0.5258, Rho spearman: 0.5448\n",
      "Test -       Loss: 1.9144, Pearson r: 0.5325, Rho spearman: 0.5490\n",
      "\n",
      "Epoch 26/300\n",
      "Train -      Loss: 0.0348, Pearson r: 0.9945, Rho spearman: 0.9932\n",
      "Validation - Loss: 4.9227, Pearson r: 0.5267, Rho spearman: 0.5505\n",
      "Test -       Loss: 1.8851, Pearson r: 0.5414, Rho spearman: 0.5509\n",
      "\n",
      "Epoch 27/300\n",
      "Train -      Loss: 0.0303, Pearson r: 0.9952, Rho spearman: 0.9939\n",
      "Validation - Loss: 4.8169, Pearson r: 0.5400, Rho spearman: 0.5395\n",
      "Test -       Loss: 1.9210, Pearson r: 0.5317, Rho spearman: 0.5402\n",
      "\n",
      "Epoch 28/300\n",
      "Train -      Loss: 0.0326, Pearson r: 0.9949, Rho spearman: 0.9936\n",
      "Validation - Loss: 4.8161, Pearson r: 0.5433, Rho spearman: 0.5656\n",
      "Test -       Loss: 1.9570, Pearson r: 0.5344, Rho spearman: 0.5494\n",
      "\n",
      "Epoch 29/300\n",
      "Train -      Loss: 0.0322, Pearson r: 0.9949, Rho spearman: 0.9934\n",
      "Validation - Loss: 4.8204, Pearson r: 0.5371, Rho spearman: 0.5379\n",
      "Test -       Loss: 1.8994, Pearson r: 0.5387, Rho spearman: 0.5544\n",
      "\n",
      "Epoch 30/300\n",
      "Train -      Loss: 0.0301, Pearson r: 0.9953, Rho spearman: 0.9940\n",
      "Validation - Loss: 4.8324, Pearson r: 0.5387, Rho spearman: 0.5471\n",
      "Test -       Loss: 1.9402, Pearson r: 0.5337, Rho spearman: 0.5484\n",
      "\n",
      "Epoch 31/300\n",
      "Train -      Loss: 0.0285, Pearson r: 0.9955, Rho spearman: 0.9941\n",
      "Validation - Loss: 4.8885, Pearson r: 0.5302, Rho spearman: 0.5196\n",
      "Test -       Loss: 1.8935, Pearson r: 0.5439, Rho spearman: 0.5536\n",
      "\n",
      "Epoch 32/300\n",
      "Train -      Loss: 0.0252, Pearson r: 0.9960, Rho spearman: 0.9948\n",
      "Validation - Loss: 4.8387, Pearson r: 0.5340, Rho spearman: 0.5315\n",
      "Test -       Loss: 1.9041, Pearson r: 0.5360, Rho spearman: 0.5526\n",
      "\n",
      "Epoch 33/300\n",
      "Train -      Loss: 0.0238, Pearson r: 0.9963, Rho spearman: 0.9955\n",
      "Validation - Loss: 4.7818, Pearson r: 0.5442, Rho spearman: 0.5598\n",
      "Test -       Loss: 1.9310, Pearson r: 0.5369, Rho spearman: 0.5529\n",
      "\n",
      "Epoch 34/300\n",
      "Train -      Loss: 0.0277, Pearson r: 0.9956, Rho spearman: 0.9945\n",
      "Validation - Loss: 4.9706, Pearson r: 0.5164, Rho spearman: 0.5240\n",
      "Test -       Loss: 1.9160, Pearson r: 0.5378, Rho spearman: 0.5585\n",
      "\n",
      "Epoch 35/300\n",
      "Train -      Loss: 0.0276, Pearson r: 0.9957, Rho spearman: 0.9944\n",
      "Validation - Loss: 4.7946, Pearson r: 0.5423, Rho spearman: 0.5420\n",
      "Test -       Loss: 1.9153, Pearson r: 0.5381, Rho spearman: 0.5511\n",
      "\n",
      "Epoch 36/300\n",
      "Train -      Loss: 0.0257, Pearson r: 0.9960, Rho spearman: 0.9949\n",
      "Validation - Loss: 4.8232, Pearson r: 0.5391, Rho spearman: 0.5322\n",
      "Test -       Loss: 1.9466, Pearson r: 0.5376, Rho spearman: 0.5525\n",
      "\n",
      "Epoch 37/300\n",
      "Train -      Loss: 0.0214, Pearson r: 0.9966, Rho spearman: 0.9955\n",
      "Validation - Loss: 4.8445, Pearson r: 0.5376, Rho spearman: 0.5331\n",
      "Test -       Loss: 1.9405, Pearson r: 0.5381, Rho spearman: 0.5552\n",
      "\n",
      "Epoch 38/300\n",
      "Train -      Loss: 0.0201, Pearson r: 0.9968, Rho spearman: 0.9959\n",
      "Validation - Loss: 4.8403, Pearson r: 0.5347, Rho spearman: 0.5419\n",
      "Test -       Loss: 1.8953, Pearson r: 0.5428, Rho spearman: 0.5621\n",
      "\n",
      "Epoch 39/300\n",
      "Train -      Loss: 0.0233, Pearson r: 0.9963, Rho spearman: 0.9953\n",
      "Validation - Loss: 4.9941, Pearson r: 0.5199, Rho spearman: 0.5171\n",
      "Test -       Loss: 1.9555, Pearson r: 0.5400, Rho spearman: 0.5600\n",
      "\n",
      "Epoch 40/300\n",
      "Train -      Loss: 0.0243, Pearson r: 0.9962, Rho spearman: 0.9950\n",
      "Validation - Loss: 4.8858, Pearson r: 0.5281, Rho spearman: 0.5266\n",
      "Test -       Loss: 1.8938, Pearson r: 0.5440, Rho spearman: 0.5631\n",
      "\n",
      "Epoch 41/300\n",
      "Train -      Loss: 0.0201, Pearson r: 0.9968, Rho spearman: 0.9960\n",
      "Validation - Loss: 4.8944, Pearson r: 0.5255, Rho spearman: 0.5182\n",
      "Test -       Loss: 1.8893, Pearson r: 0.5391, Rho spearman: 0.5585\n",
      "\n",
      "Epoch 42/300\n",
      "Train -      Loss: 0.0200, Pearson r: 0.9969, Rho spearman: 0.9960\n",
      "Validation - Loss: 4.7844, Pearson r: 0.5397, Rho spearman: 0.5382\n",
      "Test -       Loss: 1.9005, Pearson r: 0.5342, Rho spearman: 0.5513\n",
      "\n",
      "Epoch 43/300\n",
      "Train -      Loss: 0.0193, Pearson r: 0.9970, Rho spearman: 0.9962\n",
      "Validation - Loss: 4.9305, Pearson r: 0.5235, Rho spearman: 0.5195\n",
      "Test -       Loss: 1.9313, Pearson r: 0.5380, Rho spearman: 0.5578\n",
      "\n",
      "Epoch 44/300\n",
      "Train -      Loss: 0.0201, Pearson r: 0.9968, Rho spearman: 0.9960\n",
      "Validation - Loss: 4.8316, Pearson r: 0.5372, Rho spearman: 0.5355\n",
      "Test -       Loss: 1.9430, Pearson r: 0.5360, Rho spearman: 0.5532\n",
      "\n",
      "Epoch 45/300\n",
      "Train -      Loss: 0.0188, Pearson r: 0.9970, Rho spearman: 0.9964\n",
      "Validation - Loss: 4.8370, Pearson r: 0.5362, Rho spearman: 0.5346\n",
      "Test -       Loss: 1.8984, Pearson r: 0.5439, Rho spearman: 0.5593\n",
      "\n",
      "Epoch 46/300\n",
      "Train -      Loss: 0.0201, Pearson r: 0.9968, Rho spearman: 0.9960\n",
      "Validation - Loss: 4.7387, Pearson r: 0.5486, Rho spearman: 0.5410\n",
      "Test -       Loss: 1.8900, Pearson r: 0.5498, Rho spearman: 0.5680\n",
      "\n",
      "Epoch 47/300\n",
      "Train -      Loss: 0.0187, Pearson r: 0.9971, Rho spearman: 0.9964\n",
      "Validation - Loss: 4.8105, Pearson r: 0.5386, Rho spearman: 0.5365\n",
      "Test -       Loss: 1.8695, Pearson r: 0.5512, Rho spearman: 0.5695\n",
      "\n",
      "Epoch 48/300\n",
      "Train -      Loss: 0.0166, Pearson r: 0.9974, Rho spearman: 0.9967\n",
      "Validation - Loss: 4.7754, Pearson r: 0.5443, Rho spearman: 0.5412\n",
      "Test -       Loss: 1.9226, Pearson r: 0.5398, Rho spearman: 0.5573\n",
      "\n",
      "Epoch 49/300\n",
      "Train -      Loss: 0.0172, Pearson r: 0.9973, Rho spearman: 0.9966\n",
      "Validation - Loss: 4.8156, Pearson r: 0.5353, Rho spearman: 0.5216\n",
      "Test -       Loss: 1.8796, Pearson r: 0.5455, Rho spearman: 0.5648\n",
      "\n",
      "Epoch 50/300\n",
      "Train -      Loss: 0.0179, Pearson r: 0.9972, Rho spearman: 0.9965\n",
      "Validation - Loss: 4.7821, Pearson r: 0.5458, Rho spearman: 0.5429\n",
      "Test -       Loss: 1.9451, Pearson r: 0.5400, Rho spearman: 0.5571\n",
      "\n",
      "Epoch 51/300\n",
      "Train -      Loss: 0.0176, Pearson r: 0.9972, Rho spearman: 0.9965\n",
      "Validation - Loss: 4.7436, Pearson r: 0.5460, Rho spearman: 0.5429\n",
      "Test -       Loss: 1.9092, Pearson r: 0.5435, Rho spearman: 0.5641\n",
      "\n",
      "Epoch 52/300\n",
      "Train -      Loss: 0.0161, Pearson r: 0.9975, Rho spearman: 0.9968\n",
      "Validation - Loss: 4.6376, Pearson r: 0.5609, Rho spearman: 0.5617\n",
      "Test -       Loss: 1.8863, Pearson r: 0.5441, Rho spearman: 0.5638\n",
      "\n",
      "Epoch 53/300\n",
      "Train -      Loss: 0.0159, Pearson r: 0.9975, Rho spearman: 0.9969\n",
      "Validation - Loss: 4.7583, Pearson r: 0.5418, Rho spearman: 0.5320\n",
      "Test -       Loss: 1.8750, Pearson r: 0.5415, Rho spearman: 0.5587\n",
      "\n",
      "Epoch 54/300\n",
      "Train -      Loss: 0.0155, Pearson r: 0.9976, Rho spearman: 0.9968\n",
      "Validation - Loss: 4.7606, Pearson r: 0.5407, Rho spearman: 0.5349\n",
      "Test -       Loss: 1.8942, Pearson r: 0.5376, Rho spearman: 0.5569\n",
      "\n",
      "Epoch 55/300\n",
      "Train -      Loss: 0.0157, Pearson r: 0.9975, Rho spearman: 0.9968\n",
      "Validation - Loss: 4.6828, Pearson r: 0.5529, Rho spearman: 0.5577\n",
      "Test -       Loss: 1.9016, Pearson r: 0.5413, Rho spearman: 0.5623\n",
      "\n",
      "Epoch 56/300\n",
      "Train -      Loss: 0.0146, Pearson r: 0.9977, Rho spearman: 0.9971\n",
      "Validation - Loss: 4.7264, Pearson r: 0.5467, Rho spearman: 0.5435\n",
      "Test -       Loss: 1.8835, Pearson r: 0.5437, Rho spearman: 0.5642\n",
      "\n",
      "Epoch 57/300\n",
      "Train -      Loss: 0.0160, Pearson r: 0.9975, Rho spearman: 0.9967\n",
      "Validation - Loss: 4.7932, Pearson r: 0.5369, Rho spearman: 0.5414\n",
      "Test -       Loss: 1.8748, Pearson r: 0.5454, Rho spearman: 0.5656\n",
      "\n",
      "Epoch 58/300\n",
      "Train -      Loss: 0.0164, Pearson r: 0.9974, Rho spearman: 0.9966\n",
      "Validation - Loss: 4.8132, Pearson r: 0.5341, Rho spearman: 0.5221\n",
      "Test -       Loss: 1.8855, Pearson r: 0.5410, Rho spearman: 0.5605\n",
      "\n",
      "Epoch 59/300\n",
      "Train -      Loss: 0.0151, Pearson r: 0.9976, Rho spearman: 0.9971\n",
      "Validation - Loss: 4.6778, Pearson r: 0.5539, Rho spearman: 0.5404\n",
      "Test -       Loss: 1.9141, Pearson r: 0.5353, Rho spearman: 0.5555\n",
      "\n",
      "Epoch 60/300\n",
      "Train -      Loss: 0.0137, Pearson r: 0.9978, Rho spearman: 0.9972\n",
      "Validation - Loss: 4.8591, Pearson r: 0.5274, Rho spearman: 0.5225\n",
      "Test -       Loss: 1.9059, Pearson r: 0.5419, Rho spearman: 0.5593\n",
      "\n",
      "Epoch 61/300\n",
      "Train -      Loss: 0.0133, Pearson r: 0.9979, Rho spearman: 0.9972\n",
      "Validation - Loss: 4.8212, Pearson r: 0.5340, Rho spearman: 0.5136\n",
      "Test -       Loss: 1.8820, Pearson r: 0.5431, Rho spearman: 0.5649\n",
      "\n",
      "Epoch 62/300\n",
      "Train -      Loss: 0.0138, Pearson r: 0.9978, Rho spearman: 0.9973\n",
      "Validation - Loss: 4.8237, Pearson r: 0.5354, Rho spearman: 0.5341\n",
      "Test -       Loss: 1.9006, Pearson r: 0.5429, Rho spearman: 0.5641\n",
      "\n",
      "Epoch 63/300\n",
      "Train -      Loss: 0.0143, Pearson r: 0.9978, Rho spearman: 0.9972\n",
      "Validation - Loss: 4.7329, Pearson r: 0.5461, Rho spearman: 0.5438\n",
      "Test -       Loss: 1.8902, Pearson r: 0.5440, Rho spearman: 0.5668\n",
      "\n",
      "Epoch 64/300\n",
      "Train -      Loss: 0.0141, Pearson r: 0.9978, Rho spearman: 0.9973\n",
      "Validation - Loss: 4.7865, Pearson r: 0.5390, Rho spearman: 0.5411\n",
      "Test -       Loss: 1.8946, Pearson r: 0.5428, Rho spearman: 0.5614\n",
      "\n",
      "Epoch 65/300\n",
      "Train -      Loss: 0.0136, Pearson r: 0.9979, Rho spearman: 0.9974\n",
      "Validation - Loss: 4.7074, Pearson r: 0.5496, Rho spearman: 0.5456\n",
      "Test -       Loss: 1.8882, Pearson r: 0.5471, Rho spearman: 0.5671\n",
      "\n",
      "Epoch 66/300\n",
      "Train -      Loss: 0.0129, Pearson r: 0.9980, Rho spearman: 0.9975\n",
      "Validation - Loss: 4.8213, Pearson r: 0.5346, Rho spearman: 0.5323\n",
      "Test -       Loss: 1.8741, Pearson r: 0.5480, Rho spearman: 0.5685\n",
      "\n",
      "Epoch 67/300\n",
      "Train -      Loss: 0.0124, Pearson r: 0.9980, Rho spearman: 0.9978\n",
      "Validation - Loss: 4.7390, Pearson r: 0.5437, Rho spearman: 0.5324\n",
      "Test -       Loss: 1.8687, Pearson r: 0.5446, Rho spearman: 0.5663\n",
      "\n",
      "Epoch 68/300\n",
      "Train -      Loss: 0.0131, Pearson r: 0.9979, Rho spearman: 0.9975\n",
      "Validation - Loss: 4.6788, Pearson r: 0.5519, Rho spearman: 0.5449\n",
      "Test -       Loss: 1.8641, Pearson r: 0.5477, Rho spearman: 0.5673\n",
      "\n",
      "Epoch 69/300\n",
      "Train -      Loss: 0.0131, Pearson r: 0.9979, Rho spearman: 0.9974\n",
      "Validation - Loss: 4.7319, Pearson r: 0.5447, Rho spearman: 0.5358\n",
      "Test -       Loss: 1.8702, Pearson r: 0.5475, Rho spearman: 0.5680\n",
      "\n",
      "Epoch 70/300\n",
      "Train -      Loss: 0.0138, Pearson r: 0.9978, Rho spearman: 0.9972\n",
      "Validation - Loss: 4.6444, Pearson r: 0.5562, Rho spearman: 0.5539\n",
      "Test -       Loss: 1.8842, Pearson r: 0.5393, Rho spearman: 0.5602\n",
      "\n",
      "Epoch 71/300\n",
      "Train -      Loss: 0.0123, Pearson r: 0.9981, Rho spearman: 0.9977\n",
      "Validation - Loss: 4.7269, Pearson r: 0.5496, Rho spearman: 0.5429\n",
      "Test -       Loss: 1.9343, Pearson r: 0.5365, Rho spearman: 0.5540\n",
      "\n",
      "Epoch 72/300\n",
      "Train -      Loss: 0.0116, Pearson r: 0.9982, Rho spearman: 0.9979\n",
      "Validation - Loss: 4.6861, Pearson r: 0.5521, Rho spearman: 0.5482\n",
      "Test -       Loss: 1.8844, Pearson r: 0.5438, Rho spearman: 0.5606\n",
      "\n",
      "Epoch 73/300\n",
      "Train -      Loss: 0.0132, Pearson r: 0.9979, Rho spearman: 0.9976\n",
      "Validation - Loss: 4.7998, Pearson r: 0.5380, Rho spearman: 0.5227\n",
      "Test -       Loss: 1.9251, Pearson r: 0.5445, Rho spearman: 0.5644\n",
      "\n",
      "Epoch 74/300\n",
      "Train -      Loss: 0.0109, Pearson r: 0.9983, Rho spearman: 0.9981\n",
      "Validation - Loss: 4.7953, Pearson r: 0.5401, Rho spearman: 0.5354\n",
      "Test -       Loss: 1.9032, Pearson r: 0.5487, Rho spearman: 0.5698\n",
      "\n",
      "Epoch 75/300\n",
      "Train -      Loss: 0.0078, Pearson r: 0.9988, Rho spearman: 0.9986\n",
      "Validation - Loss: 4.8126, Pearson r: 0.5341, Rho spearman: 0.5243\n",
      "Test -       Loss: 1.8917, Pearson r: 0.5430, Rho spearman: 0.5625\n",
      "\n",
      "Epoch 76/300\n",
      "Train -      Loss: 0.0090, Pearson r: 0.9986, Rho spearman: 0.9984\n",
      "Validation - Loss: 4.7347, Pearson r: 0.5448, Rho spearman: 0.5342\n",
      "Test -       Loss: 1.8772, Pearson r: 0.5420, Rho spearman: 0.5641\n",
      "\n",
      "Epoch 77/300\n",
      "Train -      Loss: 0.0117, Pearson r: 0.9982, Rho spearman: 0.9978\n",
      "Validation - Loss: 4.6712, Pearson r: 0.5524, Rho spearman: 0.5500\n",
      "Test -       Loss: 1.8941, Pearson r: 0.5364, Rho spearman: 0.5549\n",
      "\n",
      "Epoch 78/300\n",
      "Train -      Loss: 0.0138, Pearson r: 0.9978, Rho spearman: 0.9974\n",
      "Validation - Loss: 4.6309, Pearson r: 0.5612, Rho spearman: 0.5580\n",
      "Test -       Loss: 1.8995, Pearson r: 0.5428, Rho spearman: 0.5627\n",
      "\n",
      "Epoch 79/300\n",
      "Train -      Loss: 0.0126, Pearson r: 0.9980, Rho spearman: 0.9976\n",
      "Validation - Loss: 4.8465, Pearson r: 0.5325, Rho spearman: 0.5409\n",
      "Test -       Loss: 1.9189, Pearson r: 0.5370, Rho spearman: 0.5607\n",
      "\n",
      "Epoch 80/300\n",
      "Train -      Loss: 0.0093, Pearson r: 0.9985, Rho spearman: 0.9982\n",
      "Validation - Loss: 4.6993, Pearson r: 0.5528, Rho spearman: 0.5552\n",
      "Test -       Loss: 1.9120, Pearson r: 0.5433, Rho spearman: 0.5643\n",
      "\n",
      "Epoch 81/300\n",
      "Train -      Loss: 0.0084, Pearson r: 0.9987, Rho spearman: 0.9984\n",
      "Validation - Loss: 4.8472, Pearson r: 0.5342, Rho spearman: 0.5405\n",
      "Test -       Loss: 1.9269, Pearson r: 0.5388, Rho spearman: 0.5605\n",
      "\n",
      "Epoch 82/300\n",
      "Train -      Loss: 0.0106, Pearson r: 0.9983, Rho spearman: 0.9980\n",
      "Validation - Loss: 4.7147, Pearson r: 0.5509, Rho spearman: 0.5411\n",
      "Test -       Loss: 1.9143, Pearson r: 0.5464, Rho spearman: 0.5701\n",
      "\n",
      "Epoch 83/300\n",
      "Train -      Loss: 0.0107, Pearson r: 0.9983, Rho spearman: 0.9980\n",
      "Validation - Loss: 4.6987, Pearson r: 0.5507, Rho spearman: 0.5470\n",
      "Test -       Loss: 1.8863, Pearson r: 0.5457, Rho spearman: 0.5662\n",
      "\n",
      "Epoch 84/300\n",
      "Train -      Loss: 0.0103, Pearson r: 0.9984, Rho spearman: 0.9980\n",
      "Validation - Loss: 4.7141, Pearson r: 0.5489, Rho spearman: 0.5503\n",
      "Test -       Loss: 1.9082, Pearson r: 0.5421, Rho spearman: 0.5647\n",
      "\n",
      "Epoch 85/300\n",
      "Train -      Loss: 0.0108, Pearson r: 0.9983, Rho spearman: 0.9979\n",
      "Validation - Loss: 4.7741, Pearson r: 0.5424, Rho spearman: 0.5473\n",
      "Test -       Loss: 1.8945, Pearson r: 0.5440, Rho spearman: 0.5662\n",
      "\n",
      "Epoch 86/300\n",
      "Train -      Loss: 0.0098, Pearson r: 0.9985, Rho spearman: 0.9981\n",
      "Validation - Loss: 4.7881, Pearson r: 0.5376, Rho spearman: 0.5407\n",
      "Test -       Loss: 1.8980, Pearson r: 0.5434, Rho spearman: 0.5674\n",
      "\n",
      "Epoch 87/300\n",
      "Train -      Loss: 0.0095, Pearson r: 0.9985, Rho spearman: 0.9983\n",
      "Validation - Loss: 4.8232, Pearson r: 0.5343, Rho spearman: 0.5286\n",
      "Test -       Loss: 1.8864, Pearson r: 0.5417, Rho spearman: 0.5648\n",
      "\n",
      "Epoch 88/300\n",
      "Train -      Loss: 0.0099, Pearson r: 0.9984, Rho spearman: 0.9981\n",
      "Validation - Loss: 4.6191, Pearson r: 0.5619, Rho spearman: 0.5542\n",
      "Test -       Loss: 1.8990, Pearson r: 0.5428, Rho spearman: 0.5615\n",
      "\n",
      "Epoch 89/300\n",
      "Train -      Loss: 0.0099, Pearson r: 0.9984, Rho spearman: 0.9980\n",
      "Validation - Loss: 4.7075, Pearson r: 0.5489, Rho spearman: 0.5559\n",
      "Test -       Loss: 1.8933, Pearson r: 0.5393, Rho spearman: 0.5651\n",
      "\n",
      "Epoch 90/300\n",
      "Train -      Loss: 0.0098, Pearson r: 0.9985, Rho spearman: 0.9980\n",
      "Validation - Loss: 4.6138, Pearson r: 0.5618, Rho spearman: 0.5562\n",
      "Test -       Loss: 1.9040, Pearson r: 0.5402, Rho spearman: 0.5650\n",
      "\n",
      "Epoch 91/300\n",
      "Train -      Loss: 0.0092, Pearson r: 0.9986, Rho spearman: 0.9982\n",
      "Validation - Loss: 4.6331, Pearson r: 0.5578, Rho spearman: 0.5459\n",
      "Test -       Loss: 1.8902, Pearson r: 0.5407, Rho spearman: 0.5617\n",
      "\n",
      "Epoch 92/300\n",
      "Train -      Loss: 0.0087, Pearson r: 0.9986, Rho spearman: 0.9983\n",
      "Validation - Loss: 4.7364, Pearson r: 0.5464, Rho spearman: 0.5477\n",
      "Test -       Loss: 1.9162, Pearson r: 0.5409, Rho spearman: 0.5638\n",
      "\n",
      "Epoch 93/300\n",
      "Train -      Loss: 0.0090, Pearson r: 0.9986, Rho spearman: 0.9982\n",
      "Validation - Loss: 4.6518, Pearson r: 0.5562, Rho spearman: 0.5540\n",
      "Test -       Loss: 1.8781, Pearson r: 0.5465, Rho spearman: 0.5679\n",
      "\n",
      "Epoch 94/300\n",
      "Train -      Loss: 0.0092, Pearson r: 0.9986, Rho spearman: 0.9982\n",
      "Validation - Loss: 4.6370, Pearson r: 0.5585, Rho spearman: 0.5476\n",
      "Test -       Loss: 1.8964, Pearson r: 0.5438, Rho spearman: 0.5660\n",
      "\n",
      "\u001b[91mEpoch 95/300\n",
      "Train -      Loss: 0.0100, Pearson r: 0.9984, Rho spearman: 0.9981\n",
      "Validation - Loss: 4.5509, Pearson r: 0.5721, Rho spearman: 0.5641\n",
      "Test -       Loss: 1.8976, Pearson r: 0.5446, Rho spearman: 0.5669\u001b[0m\n",
      "\n",
      "Epoch 96/300\n",
      "Train -      Loss: 0.0092, Pearson r: 0.9986, Rho spearman: 0.9983\n",
      "Validation - Loss: 4.6102, Pearson r: 0.5625, Rho spearman: 0.5666\n",
      "Test -       Loss: 1.8919, Pearson r: 0.5422, Rho spearman: 0.5648\n",
      "\n",
      "Epoch 97/300\n",
      "Train -      Loss: 0.0083, Pearson r: 0.9987, Rho spearman: 0.9985\n",
      "Validation - Loss: 4.5735, Pearson r: 0.5681, Rho spearman: 0.5680\n",
      "Test -       Loss: 1.9223, Pearson r: 0.5381, Rho spearman: 0.5622\n",
      "\n",
      "\u001b[91mEpoch 98/300\n",
      "Train -      Loss: 0.0087, Pearson r: 0.9986, Rho spearman: 0.9983\n",
      "Validation - Loss: 4.5213, Pearson r: 0.5739, Rho spearman: 0.5678\n",
      "Test -       Loss: 1.8934, Pearson r: 0.5412, Rho spearman: 0.5641\u001b[0m\n",
      "\n",
      "Epoch 99/300\n",
      "Train -      Loss: 0.0081, Pearson r: 0.9987, Rho spearman: 0.9984\n",
      "Validation - Loss: 4.5566, Pearson r: 0.5684, Rho spearman: 0.5608\n",
      "Test -       Loss: 1.8769, Pearson r: 0.5420, Rho spearman: 0.5679\n",
      "\n",
      "Epoch 100/300\n",
      "Train -      Loss: 0.0088, Pearson r: 0.9986, Rho spearman: 0.9983\n",
      "Validation - Loss: 4.6161, Pearson r: 0.5630, Rho spearman: 0.5595\n",
      "Test -       Loss: 1.8866, Pearson r: 0.5440, Rho spearman: 0.5663\n",
      "\n",
      "Epoch 101/300\n",
      "Train -      Loss: 0.0098, Pearson r: 0.9985, Rho spearman: 0.9982\n",
      "Validation - Loss: 4.6645, Pearson r: 0.5567, Rho spearman: 0.5521\n",
      "Test -       Loss: 1.9192, Pearson r: 0.5424, Rho spearman: 0.5655\n",
      "\n",
      "Epoch 102/300\n",
      "Train -      Loss: 0.0076, Pearson r: 0.9988, Rho spearman: 0.9985\n",
      "Validation - Loss: 4.6900, Pearson r: 0.5510, Rho spearman: 0.5500\n",
      "Test -       Loss: 1.9108, Pearson r: 0.5358, Rho spearman: 0.5610\n",
      "\n",
      "Epoch 103/300\n",
      "Train -      Loss: 0.0067, Pearson r: 0.9989, Rho spearman: 0.9987\n",
      "Validation - Loss: 4.6294, Pearson r: 0.5596, Rho spearman: 0.5611\n",
      "Test -       Loss: 1.9119, Pearson r: 0.5384, Rho spearman: 0.5629\n",
      "\n",
      "Epoch 104/300\n",
      "Train -      Loss: 0.0073, Pearson r: 0.9989, Rho spearman: 0.9986\n",
      "Validation - Loss: 4.6662, Pearson r: 0.5538, Rho spearman: 0.5609\n",
      "Test -       Loss: 1.9123, Pearson r: 0.5354, Rho spearman: 0.5589\n",
      "\n",
      "Epoch 105/300\n",
      "Train -      Loss: 0.0085, Pearson r: 0.9987, Rho spearman: 0.9984\n",
      "Validation - Loss: 4.6731, Pearson r: 0.5534, Rho spearman: 0.5497\n",
      "Test -       Loss: 1.9060, Pearson r: 0.5418, Rho spearman: 0.5670\n",
      "\n",
      "Epoch 106/300\n",
      "Train -      Loss: 0.0086, Pearson r: 0.9987, Rho spearman: 0.9984\n",
      "Validation - Loss: 4.6212, Pearson r: 0.5607, Rho spearman: 0.5583\n",
      "Test -       Loss: 1.9122, Pearson r: 0.5414, Rho spearman: 0.5666\n",
      "\n",
      "Epoch 107/300\n",
      "Train -      Loss: 0.0081, Pearson r: 0.9987, Rho spearman: 0.9985\n",
      "Validation - Loss: 4.5431, Pearson r: 0.5709, Rho spearman: 0.5654\n",
      "Test -       Loss: 1.9179, Pearson r: 0.5407, Rho spearman: 0.5624\n",
      "\n",
      "Epoch 108/300\n",
      "Train -      Loss: 0.0085, Pearson r: 0.9987, Rho spearman: 0.9984\n",
      "Validation - Loss: 4.6209, Pearson r: 0.5605, Rho spearman: 0.5557\n",
      "Test -       Loss: 1.9274, Pearson r: 0.5368, Rho spearman: 0.5621\n",
      "\n",
      "Epoch 109/300\n",
      "Train -      Loss: 0.0081, Pearson r: 0.9987, Rho spearman: 0.9985\n",
      "Validation - Loss: 4.7336, Pearson r: 0.5450, Rho spearman: 0.5348\n",
      "Test -       Loss: 1.9219, Pearson r: 0.5392, Rho spearman: 0.5645\n",
      "\n",
      "Epoch 110/300\n",
      "Train -      Loss: 0.0077, Pearson r: 0.9988, Rho spearman: 0.9985\n",
      "Validation - Loss: 4.6642, Pearson r: 0.5541, Rho spearman: 0.5451\n",
      "Test -       Loss: 1.8876, Pearson r: 0.5424, Rho spearman: 0.5669\n",
      "\n",
      "Epoch 111/300\n",
      "Train -      Loss: 0.0078, Pearson r: 0.9988, Rho spearman: 0.9985\n",
      "Validation - Loss: 4.6809, Pearson r: 0.5512, Rho spearman: 0.5544\n",
      "Test -       Loss: 1.8982, Pearson r: 0.5392, Rho spearman: 0.5623\n",
      "\n",
      "Epoch 112/300\n",
      "Train -      Loss: 0.0079, Pearson r: 0.9988, Rho spearman: 0.9986\n",
      "Validation - Loss: 4.7424, Pearson r: 0.5432, Rho spearman: 0.5413\n",
      "Test -       Loss: 1.9088, Pearson r: 0.5410, Rho spearman: 0.5669\n",
      "\n",
      "Epoch 113/300\n",
      "Train -      Loss: 0.0081, Pearson r: 0.9987, Rho spearman: 0.9985\n",
      "Validation - Loss: 4.7210, Pearson r: 0.5472, Rho spearman: 0.5440\n",
      "Test -       Loss: 1.8971, Pearson r: 0.5447, Rho spearman: 0.5689\n",
      "\n",
      "Epoch 114/300\n",
      "Train -      Loss: 0.0066, Pearson r: 0.9990, Rho spearman: 0.9987\n",
      "Validation - Loss: 4.6477, Pearson r: 0.5572, Rho spearman: 0.5586\n",
      "Test -       Loss: 1.9070, Pearson r: 0.5424, Rho spearman: 0.5672\n",
      "\n",
      "Epoch 115/300\n",
      "Train -      Loss: 0.0074, Pearson r: 0.9988, Rho spearman: 0.9985\n",
      "Validation - Loss: 4.6555, Pearson r: 0.5562, Rho spearman: 0.5608\n",
      "Test -       Loss: 1.9107, Pearson r: 0.5412, Rho spearman: 0.5652\n",
      "\n",
      "Epoch 116/300\n",
      "Train -      Loss: 0.0076, Pearson r: 0.9988, Rho spearman: 0.9986\n",
      "Validation - Loss: 4.6068, Pearson r: 0.5607, Rho spearman: 0.5559\n",
      "Test -       Loss: 1.9045, Pearson r: 0.5371, Rho spearman: 0.5605\n",
      "\n",
      "Epoch 117/300\n",
      "Train -      Loss: 0.0080, Pearson r: 0.9987, Rho spearman: 0.9985\n",
      "Validation - Loss: 4.6003, Pearson r: 0.5634, Rho spearman: 0.5616\n",
      "Test -       Loss: 1.9183, Pearson r: 0.5366, Rho spearman: 0.5601\n",
      "\n",
      "Epoch 118/300\n",
      "Train -      Loss: 0.0086, Pearson r: 0.9987, Rho spearman: 0.9984\n",
      "Validation - Loss: 4.7059, Pearson r: 0.5477, Rho spearman: 0.5562\n",
      "Test -       Loss: 1.9036, Pearson r: 0.5369, Rho spearman: 0.5609\n",
      "\n",
      "Epoch 119/300\n",
      "Train -      Loss: 0.0073, Pearson r: 0.9989, Rho spearman: 0.9986\n",
      "Validation - Loss: 4.6217, Pearson r: 0.5604, Rho spearman: 0.5622\n",
      "Test -       Loss: 1.9131, Pearson r: 0.5375, Rho spearman: 0.5638\n",
      "\n",
      "Epoch 120/300\n",
      "Train -      Loss: 0.0070, Pearson r: 0.9989, Rho spearman: 0.9987\n",
      "Validation - Loss: 4.7212, Pearson r: 0.5454, Rho spearman: 0.5490\n",
      "Test -       Loss: 1.8911, Pearson r: 0.5394, Rho spearman: 0.5609\n",
      "\n",
      "Epoch 121/300\n",
      "Train -      Loss: 0.0071, Pearson r: 0.9989, Rho spearman: 0.9986\n",
      "Validation - Loss: 4.6206, Pearson r: 0.5596, Rho spearman: 0.5575\n",
      "Test -       Loss: 1.8936, Pearson r: 0.5419, Rho spearman: 0.5657\n",
      "\n",
      "Epoch 122/300\n",
      "Train -      Loss: 0.0068, Pearson r: 0.9989, Rho spearman: 0.9987\n",
      "Validation - Loss: 4.6616, Pearson r: 0.5533, Rho spearman: 0.5567\n",
      "Test -       Loss: 1.8684, Pearson r: 0.5446, Rho spearman: 0.5645\n",
      "\n",
      "Epoch 123/300\n",
      "Train -      Loss: 0.0091, Pearson r: 0.9986, Rho spearman: 0.9981\n",
      "Validation - Loss: 4.7540, Pearson r: 0.5426, Rho spearman: 0.5450\n",
      "Test -       Loss: 1.9135, Pearson r: 0.5390, Rho spearman: 0.5620\n",
      "\n",
      "Epoch 124/300\n",
      "Train -      Loss: 0.0076, Pearson r: 0.9988, Rho spearman: 0.9986\n",
      "Validation - Loss: 4.6763, Pearson r: 0.5519, Rho spearman: 0.5537\n",
      "Test -       Loss: 1.8771, Pearson r: 0.5458, Rho spearman: 0.5655\n",
      "\n",
      "Epoch 125/300\n",
      "Train -      Loss: 0.0064, Pearson r: 0.9990, Rho spearman: 0.9988\n",
      "Validation - Loss: 4.7765, Pearson r: 0.5394, Rho spearman: 0.5411\n",
      "Test -       Loss: 1.8999, Pearson r: 0.5431, Rho spearman: 0.5664\n",
      "\n",
      "Epoch 126/300\n",
      "Train -      Loss: 0.0058, Pearson r: 0.9991, Rho spearman: 0.9989\n",
      "Validation - Loss: 4.7130, Pearson r: 0.5483, Rho spearman: 0.5477\n",
      "Test -       Loss: 1.9161, Pearson r: 0.5411, Rho spearman: 0.5641\n",
      "\n",
      "Epoch 127/300\n",
      "Train -      Loss: 0.0064, Pearson r: 0.9990, Rho spearman: 0.9988\n",
      "Validation - Loss: 4.7400, Pearson r: 0.5427, Rho spearman: 0.5396\n",
      "Test -       Loss: 1.8729, Pearson r: 0.5461, Rho spearman: 0.5667\n",
      "\n",
      "Epoch 128/300\n",
      "Train -      Loss: 0.0069, Pearson r: 0.9989, Rho spearman: 0.9987\n",
      "Validation - Loss: 4.7010, Pearson r: 0.5487, Rho spearman: 0.5472\n",
      "Test -       Loss: 1.9060, Pearson r: 0.5425, Rho spearman: 0.5625\n",
      "\n",
      "Epoch 129/300\n",
      "Train -      Loss: 0.0073, Pearson r: 0.9988, Rho spearman: 0.9986\n",
      "Validation - Loss: 4.7725, Pearson r: 0.5410, Rho spearman: 0.5466\n",
      "Test -       Loss: 1.9132, Pearson r: 0.5417, Rho spearman: 0.5643\n",
      "\n",
      "Epoch 130/300\n",
      "Train -      Loss: 0.0075, Pearson r: 0.9988, Rho spearman: 0.9986\n",
      "Validation - Loss: 4.7259, Pearson r: 0.5458, Rho spearman: 0.5389\n",
      "Test -       Loss: 1.8976, Pearson r: 0.5430, Rho spearman: 0.5630\n",
      "\n",
      "Epoch 131/300\n",
      "Train -      Loss: 0.0062, Pearson r: 0.9990, Rho spearman: 0.9989\n",
      "Validation - Loss: 4.6839, Pearson r: 0.5521, Rho spearman: 0.5448\n",
      "Test -       Loss: 1.9110, Pearson r: 0.5454, Rho spearman: 0.5666\n",
      "\n",
      "Epoch 132/300\n",
      "Train -      Loss: 0.0058, Pearson r: 0.9991, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.7798, Pearson r: 0.5382, Rho spearman: 0.5291\n",
      "Test -       Loss: 1.9033, Pearson r: 0.5452, Rho spearman: 0.5650\n",
      "\n",
      "Epoch 133/300\n",
      "Train -      Loss: 0.0058, Pearson r: 0.9991, Rho spearman: 0.9989\n",
      "Validation - Loss: 4.7420, Pearson r: 0.5433, Rho spearman: 0.5374\n",
      "Test -       Loss: 1.8885, Pearson r: 0.5450, Rho spearman: 0.5674\n",
      "\n",
      "Epoch 134/300\n",
      "Train -      Loss: 0.0059, Pearson r: 0.9991, Rho spearman: 0.9989\n",
      "Validation - Loss: 4.7865, Pearson r: 0.5362, Rho spearman: 0.5235\n",
      "Test -       Loss: 1.8893, Pearson r: 0.5467, Rho spearman: 0.5677\n",
      "\n",
      "Epoch 135/300\n",
      "Train -      Loss: 0.0063, Pearson r: 0.9990, Rho spearman: 0.9988\n",
      "Validation - Loss: 4.7326, Pearson r: 0.5444, Rho spearman: 0.5369\n",
      "Test -       Loss: 1.8798, Pearson r: 0.5456, Rho spearman: 0.5680\n",
      "\n",
      "Epoch 136/300\n",
      "Train -      Loss: 0.0072, Pearson r: 0.9989, Rho spearman: 0.9986\n",
      "Validation - Loss: 4.7010, Pearson r: 0.5492, Rho spearman: 0.5460\n",
      "Test -       Loss: 1.8868, Pearson r: 0.5466, Rho spearman: 0.5699\n",
      "\n",
      "Epoch 137/300\n",
      "Train -      Loss: 0.0065, Pearson r: 0.9990, Rho spearman: 0.9988\n",
      "Validation - Loss: 4.7811, Pearson r: 0.5383, Rho spearman: 0.5416\n",
      "Test -       Loss: 1.9079, Pearson r: 0.5462, Rho spearman: 0.5703\n",
      "\n",
      "Epoch 138/300\n",
      "Train -      Loss: 0.0072, Pearson r: 0.9989, Rho spearman: 0.9986\n",
      "Validation - Loss: 4.6528, Pearson r: 0.5544, Rho spearman: 0.5396\n",
      "Test -       Loss: 1.8802, Pearson r: 0.5400, Rho spearman: 0.5605\n",
      "\n",
      "Epoch 139/300\n",
      "Train -      Loss: 0.0054, Pearson r: 0.9992, Rho spearman: 0.9989\n",
      "Validation - Loss: 4.6969, Pearson r: 0.5492, Rho spearman: 0.5366\n",
      "Test -       Loss: 1.8895, Pearson r: 0.5434, Rho spearman: 0.5645\n",
      "\n",
      "Epoch 140/300\n",
      "Train -      Loss: 0.0043, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.7296, Pearson r: 0.5452, Rho spearman: 0.5348\n",
      "Test -       Loss: 1.8964, Pearson r: 0.5475, Rho spearman: 0.5641\n",
      "\n",
      "Epoch 141/300\n",
      "Train -      Loss: 0.0052, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6902, Pearson r: 0.5532, Rho spearman: 0.5473\n",
      "Test -       Loss: 1.9041, Pearson r: 0.5491, Rho spearman: 0.5707\n",
      "\n",
      "Epoch 142/300\n",
      "Train -      Loss: 0.0073, Pearson r: 0.9989, Rho spearman: 0.9987\n",
      "Validation - Loss: 4.6976, Pearson r: 0.5501, Rho spearman: 0.5407\n",
      "Test -       Loss: 1.9041, Pearson r: 0.5453, Rho spearman: 0.5650\n",
      "\n",
      "Epoch 143/300\n",
      "Train -      Loss: 0.0064, Pearson r: 0.9990, Rho spearman: 0.9988\n",
      "Validation - Loss: 4.7133, Pearson r: 0.5477, Rho spearman: 0.5474\n",
      "Test -       Loss: 1.8963, Pearson r: 0.5421, Rho spearman: 0.5624\n",
      "\n",
      "Epoch 144/300\n",
      "Train -      Loss: 0.0059, Pearson r: 0.9991, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.7090, Pearson r: 0.5485, Rho spearman: 0.5450\n",
      "Test -       Loss: 1.8923, Pearson r: 0.5468, Rho spearman: 0.5645\n",
      "\n",
      "Epoch 145/300\n",
      "Train -      Loss: 0.0062, Pearson r: 0.9990, Rho spearman: 0.9989\n",
      "Validation - Loss: 4.6734, Pearson r: 0.5531, Rho spearman: 0.5612\n",
      "Test -       Loss: 1.8997, Pearson r: 0.5405, Rho spearman: 0.5607\n",
      "\n",
      "Epoch 146/300\n",
      "Train -      Loss: 0.0052, Pearson r: 0.9992, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.7035, Pearson r: 0.5511, Rho spearman: 0.5553\n",
      "Test -       Loss: 1.8965, Pearson r: 0.5491, Rho spearman: 0.5672\n",
      "\n",
      "Epoch 147/300\n",
      "Train -      Loss: 0.0057, Pearson r: 0.9991, Rho spearman: 0.9989\n",
      "Validation - Loss: 4.6741, Pearson r: 0.5519, Rho spearman: 0.5455\n",
      "Test -       Loss: 1.8868, Pearson r: 0.5429, Rho spearman: 0.5597\n",
      "\n",
      "Epoch 148/300\n",
      "Train -      Loss: 0.0065, Pearson r: 0.9990, Rho spearman: 0.9988\n",
      "Validation - Loss: 4.7770, Pearson r: 0.5390, Rho spearman: 0.5387\n",
      "Test -       Loss: 1.8961, Pearson r: 0.5463, Rho spearman: 0.5664\n",
      "\n",
      "Epoch 149/300\n",
      "Train -      Loss: 0.0060, Pearson r: 0.9991, Rho spearman: 0.9988\n",
      "Validation - Loss: 4.6536, Pearson r: 0.5557, Rho spearman: 0.5450\n",
      "Test -       Loss: 1.8879, Pearson r: 0.5455, Rho spearman: 0.5635\n",
      "\n",
      "Epoch 150/300\n",
      "Train -      Loss: 0.0052, Pearson r: 0.9992, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.7331, Pearson r: 0.5454, Rho spearman: 0.5409\n",
      "Test -       Loss: 1.8943, Pearson r: 0.5486, Rho spearman: 0.5690\n",
      "\n",
      "Epoch 151/300\n",
      "Train -      Loss: 0.0049, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6851, Pearson r: 0.5511, Rho spearman: 0.5498\n",
      "Test -       Loss: 1.8932, Pearson r: 0.5456, Rho spearman: 0.5657\n",
      "\n",
      "Epoch 152/300\n",
      "Train -      Loss: 0.0058, Pearson r: 0.9991, Rho spearman: 0.9989\n",
      "Validation - Loss: 4.7583, Pearson r: 0.5416, Rho spearman: 0.5339\n",
      "Test -       Loss: 1.9051, Pearson r: 0.5447, Rho spearman: 0.5630\n",
      "\n",
      "Epoch 153/300\n",
      "Train -      Loss: 0.0069, Pearson r: 0.9989, Rho spearman: 0.9989\n",
      "Validation - Loss: 4.7197, Pearson r: 0.5474, Rho spearman: 0.5473\n",
      "Test -       Loss: 1.8909, Pearson r: 0.5493, Rho spearman: 0.5682\n",
      "\n",
      "Epoch 154/300\n",
      "Train -      Loss: 0.0064, Pearson r: 0.9990, Rho spearman: 0.9988\n",
      "Validation - Loss: 4.6633, Pearson r: 0.5549, Rho spearman: 0.5465\n",
      "Test -       Loss: 1.9026, Pearson r: 0.5406, Rho spearman: 0.5603\n",
      "\n",
      "Epoch 155/300\n",
      "Train -      Loss: 0.0050, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6592, Pearson r: 0.5555, Rho spearman: 0.5543\n",
      "Test -       Loss: 1.9166, Pearson r: 0.5401, Rho spearman: 0.5591\n",
      "\n",
      "Epoch 156/300\n",
      "Train -      Loss: 0.0059, Pearson r: 0.9991, Rho spearman: 0.9988\n",
      "Validation - Loss: 4.6460, Pearson r: 0.5559, Rho spearman: 0.5487\n",
      "Test -       Loss: 1.8840, Pearson r: 0.5421, Rho spearman: 0.5614\n",
      "\n",
      "Epoch 157/300\n",
      "Train -      Loss: 0.0052, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6627, Pearson r: 0.5541, Rho spearman: 0.5517\n",
      "Test -       Loss: 1.8901, Pearson r: 0.5428, Rho spearman: 0.5631\n",
      "\n",
      "Epoch 158/300\n",
      "Train -      Loss: 0.0050, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6172, Pearson r: 0.5604, Rho spearman: 0.5563\n",
      "Test -       Loss: 1.8924, Pearson r: 0.5437, Rho spearman: 0.5647\n",
      "\n",
      "Epoch 159/300\n",
      "Train -      Loss: 0.0055, Pearson r: 0.9991, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.5831, Pearson r: 0.5656, Rho spearman: 0.5574\n",
      "Test -       Loss: 1.9083, Pearson r: 0.5428, Rho spearman: 0.5605\n",
      "\n",
      "Epoch 160/300\n",
      "Train -      Loss: 0.0060, Pearson r: 0.9991, Rho spearman: 0.9988\n",
      "Validation - Loss: 4.6309, Pearson r: 0.5583, Rho spearman: 0.5447\n",
      "Test -       Loss: 1.8780, Pearson r: 0.5461, Rho spearman: 0.5647\n",
      "\n",
      "Epoch 161/300\n",
      "Train -      Loss: 0.0058, Pearson r: 0.9991, Rho spearman: 0.9989\n",
      "Validation - Loss: 4.6512, Pearson r: 0.5576, Rho spearman: 0.5495\n",
      "Test -       Loss: 1.9222, Pearson r: 0.5374, Rho spearman: 0.5539\n",
      "\n",
      "Epoch 162/300\n",
      "Train -      Loss: 0.0056, Pearson r: 0.9991, Rho spearman: 0.9989\n",
      "Validation - Loss: 4.6232, Pearson r: 0.5612, Rho spearman: 0.5543\n",
      "Test -       Loss: 1.9046, Pearson r: 0.5441, Rho spearman: 0.5621\n",
      "\n",
      "Epoch 163/300\n",
      "Train -      Loss: 0.0054, Pearson r: 0.9992, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.6929, Pearson r: 0.5511, Rho spearman: 0.5475\n",
      "Test -       Loss: 1.9170, Pearson r: 0.5414, Rho spearman: 0.5604\n",
      "\n",
      "Epoch 164/300\n",
      "Train -      Loss: 0.0054, Pearson r: 0.9992, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.6573, Pearson r: 0.5559, Rho spearman: 0.5496\n",
      "Test -       Loss: 1.9170, Pearson r: 0.5394, Rho spearman: 0.5559\n",
      "\n",
      "Epoch 165/300\n",
      "Train -      Loss: 0.0051, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.7308, Pearson r: 0.5460, Rho spearman: 0.5357\n",
      "Test -       Loss: 1.8985, Pearson r: 0.5448, Rho spearman: 0.5604\n",
      "\n",
      "Epoch 166/300\n",
      "Train -      Loss: 0.0049, Pearson r: 0.9992, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.6478, Pearson r: 0.5569, Rho spearman: 0.5424\n",
      "Test -       Loss: 1.9155, Pearson r: 0.5395, Rho spearman: 0.5566\n",
      "\n",
      "Epoch 167/300\n",
      "Train -      Loss: 0.0047, Pearson r: 0.9993, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6413, Pearson r: 0.5578, Rho spearman: 0.5508\n",
      "Test -       Loss: 1.8937, Pearson r: 0.5443, Rho spearman: 0.5653\n",
      "\n",
      "Epoch 168/300\n",
      "Train -      Loss: 0.0049, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6657, Pearson r: 0.5533, Rho spearman: 0.5411\n",
      "Test -       Loss: 1.8891, Pearson r: 0.5421, Rho spearman: 0.5605\n",
      "\n",
      "Epoch 169/300\n",
      "Train -      Loss: 0.0055, Pearson r: 0.9991, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.7262, Pearson r: 0.5477, Rho spearman: 0.5427\n",
      "Test -       Loss: 1.9209, Pearson r: 0.5442, Rho spearman: 0.5642\n",
      "\n",
      "Epoch 170/300\n",
      "Train -      Loss: 0.0055, Pearson r: 0.9991, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.6902, Pearson r: 0.5524, Rho spearman: 0.5577\n",
      "Test -       Loss: 1.9077, Pearson r: 0.5424, Rho spearman: 0.5615\n",
      "\n",
      "Epoch 171/300\n",
      "Train -      Loss: 0.0054, Pearson r: 0.9991, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.6585, Pearson r: 0.5547, Rho spearman: 0.5578\n",
      "Test -       Loss: 1.8703, Pearson r: 0.5460, Rho spearman: 0.5648\n",
      "\n",
      "Epoch 172/300\n",
      "Train -      Loss: 0.0052, Pearson r: 0.9992, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.6392, Pearson r: 0.5571, Rho spearman: 0.5586\n",
      "Test -       Loss: 1.8902, Pearson r: 0.5427, Rho spearman: 0.5619\n",
      "\n",
      "Epoch 173/300\n",
      "Train -      Loss: 0.0050, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6780, Pearson r: 0.5523, Rho spearman: 0.5464\n",
      "Test -       Loss: 1.8852, Pearson r: 0.5445, Rho spearman: 0.5632\n",
      "\n",
      "Epoch 174/300\n",
      "Train -      Loss: 0.0048, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6244, Pearson r: 0.5596, Rho spearman: 0.5517\n",
      "Test -       Loss: 1.8905, Pearson r: 0.5433, Rho spearman: 0.5624\n",
      "\n",
      "Epoch 175/300\n",
      "Train -      Loss: 0.0051, Pearson r: 0.9992, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.6832, Pearson r: 0.5513, Rho spearman: 0.5407\n",
      "Test -       Loss: 1.8851, Pearson r: 0.5462, Rho spearman: 0.5671\n",
      "\n",
      "Epoch 176/300\n",
      "Train -      Loss: 0.0053, Pearson r: 0.9992, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.7199, Pearson r: 0.5473, Rho spearman: 0.5411\n",
      "Test -       Loss: 1.8879, Pearson r: 0.5450, Rho spearman: 0.5654\n",
      "\n",
      "Epoch 177/300\n",
      "Train -      Loss: 0.0049, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.7022, Pearson r: 0.5491, Rho spearman: 0.5501\n",
      "Test -       Loss: 1.8827, Pearson r: 0.5448, Rho spearman: 0.5647\n",
      "\n",
      "Epoch 178/300\n",
      "Train -      Loss: 0.0050, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.7453, Pearson r: 0.5436, Rho spearman: 0.5470\n",
      "Test -       Loss: 1.8731, Pearson r: 0.5479, Rho spearman: 0.5682\n",
      "\n",
      "Epoch 179/300\n",
      "Train -      Loss: 0.0051, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.7080, Pearson r: 0.5496, Rho spearman: 0.5532\n",
      "Test -       Loss: 1.8845, Pearson r: 0.5460, Rho spearman: 0.5648\n",
      "\n",
      "Epoch 180/300\n",
      "Train -      Loss: 0.0053, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6374, Pearson r: 0.5571, Rho spearman: 0.5567\n",
      "Test -       Loss: 1.8638, Pearson r: 0.5480, Rho spearman: 0.5663\n",
      "\n",
      "Epoch 181/300\n",
      "Train -      Loss: 0.0046, Pearson r: 0.9993, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6945, Pearson r: 0.5506, Rho spearman: 0.5528\n",
      "Test -       Loss: 1.8722, Pearson r: 0.5524, Rho spearman: 0.5727\n",
      "\n",
      "Epoch 182/300\n",
      "Train -      Loss: 0.0045, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6664, Pearson r: 0.5535, Rho spearman: 0.5502\n",
      "Test -       Loss: 1.8704, Pearson r: 0.5481, Rho spearman: 0.5687\n",
      "\n",
      "Epoch 183/300\n",
      "Train -      Loss: 0.0048, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.7228, Pearson r: 0.5466, Rho spearman: 0.5368\n",
      "Test -       Loss: 1.8742, Pearson r: 0.5511, Rho spearman: 0.5723\n",
      "\n",
      "Epoch 184/300\n",
      "Train -      Loss: 0.0054, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6924, Pearson r: 0.5506, Rho spearman: 0.5501\n",
      "Test -       Loss: 1.8748, Pearson r: 0.5476, Rho spearman: 0.5665\n",
      "\n",
      "Epoch 185/300\n",
      "Train -      Loss: 0.0047, Pearson r: 0.9993, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6603, Pearson r: 0.5548, Rho spearman: 0.5515\n",
      "Test -       Loss: 1.8865, Pearson r: 0.5483, Rho spearman: 0.5664\n",
      "\n",
      "Epoch 186/300\n",
      "Train -      Loss: 0.0045, Pearson r: 0.9993, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.8032, Pearson r: 0.5349, Rho spearman: 0.5303\n",
      "Test -       Loss: 1.8753, Pearson r: 0.5468, Rho spearman: 0.5660\n",
      "\n",
      "Epoch 187/300\n",
      "Train -      Loss: 0.0044, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.7112, Pearson r: 0.5468, Rho spearman: 0.5453\n",
      "Test -       Loss: 1.8769, Pearson r: 0.5462, Rho spearman: 0.5671\n",
      "\n",
      "Epoch 188/300\n",
      "Train -      Loss: 0.0044, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.7466, Pearson r: 0.5434, Rho spearman: 0.5469\n",
      "Test -       Loss: 1.8736, Pearson r: 0.5490, Rho spearman: 0.5676\n",
      "\n",
      "Epoch 189/300\n",
      "Train -      Loss: 0.0051, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6647, Pearson r: 0.5557, Rho spearman: 0.5546\n",
      "Test -       Loss: 1.8910, Pearson r: 0.5505, Rho spearman: 0.5679\n",
      "\n",
      "Epoch 190/300\n",
      "Train -      Loss: 0.0048, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.7339, Pearson r: 0.5451, Rho spearman: 0.5315\n",
      "Test -       Loss: 1.8684, Pearson r: 0.5543, Rho spearman: 0.5709\n",
      "\n",
      "Epoch 191/300\n",
      "Train -      Loss: 0.0043, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.7092, Pearson r: 0.5480, Rho spearman: 0.5470\n",
      "Test -       Loss: 1.8740, Pearson r: 0.5483, Rho spearman: 0.5678\n",
      "\n",
      "Epoch 192/300\n",
      "Train -      Loss: 0.0045, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6122, Pearson r: 0.5605, Rho spearman: 0.5564\n",
      "Test -       Loss: 1.8536, Pearson r: 0.5518, Rho spearman: 0.5716\n",
      "\n",
      "Epoch 193/300\n",
      "Train -      Loss: 0.0050, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6535, Pearson r: 0.5559, Rho spearman: 0.5410\n",
      "Test -       Loss: 1.8951, Pearson r: 0.5441, Rho spearman: 0.5672\n",
      "\n",
      "Epoch 194/300\n",
      "Train -      Loss: 0.0037, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6144, Pearson r: 0.5606, Rho spearman: 0.5595\n",
      "Test -       Loss: 1.8746, Pearson r: 0.5496, Rho spearman: 0.5712\n",
      "\n",
      "Epoch 195/300\n",
      "Train -      Loss: 0.0033, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.7558, Pearson r: 0.5433, Rho spearman: 0.5416\n",
      "Test -       Loss: 1.8912, Pearson r: 0.5493, Rho spearman: 0.5713\n",
      "\n",
      "Epoch 196/300\n",
      "Train -      Loss: 0.0036, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.7003, Pearson r: 0.5493, Rho spearman: 0.5383\n",
      "Test -       Loss: 1.8924, Pearson r: 0.5444, Rho spearman: 0.5664\n",
      "\n",
      "Epoch 197/300\n",
      "Train -      Loss: 0.0053, Pearson r: 0.9992, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.6936, Pearson r: 0.5510, Rho spearman: 0.5511\n",
      "Test -       Loss: 1.8977, Pearson r: 0.5425, Rho spearman: 0.5627\n",
      "\n",
      "Epoch 198/300\n",
      "Train -      Loss: 0.0049, Pearson r: 0.9992, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.6441, Pearson r: 0.5577, Rho spearman: 0.5527\n",
      "Test -       Loss: 1.8867, Pearson r: 0.5461, Rho spearman: 0.5701\n",
      "\n",
      "Epoch 199/300\n",
      "Train -      Loss: 0.0040, Pearson r: 0.9994, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6703, Pearson r: 0.5548, Rho spearman: 0.5453\n",
      "Test -       Loss: 1.8927, Pearson r: 0.5481, Rho spearman: 0.5691\n",
      "\n",
      "Epoch 200/300\n",
      "Train -      Loss: 0.0034, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6607, Pearson r: 0.5554, Rho spearman: 0.5521\n",
      "Test -       Loss: 1.8853, Pearson r: 0.5464, Rho spearman: 0.5684\n",
      "\n",
      "Epoch 201/300\n",
      "Train -      Loss: 0.0037, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5997, Pearson r: 0.5632, Rho spearman: 0.5606\n",
      "Test -       Loss: 1.8928, Pearson r: 0.5434, Rho spearman: 0.5641\n",
      "\n",
      "Epoch 202/300\n",
      "Train -      Loss: 0.0045, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.5769, Pearson r: 0.5655, Rho spearman: 0.5629\n",
      "Test -       Loss: 1.8815, Pearson r: 0.5449, Rho spearman: 0.5660\n",
      "\n",
      "Epoch 203/300\n",
      "Train -      Loss: 0.0053, Pearson r: 0.9992, Rho spearman: 0.9990\n",
      "Validation - Loss: 4.6713, Pearson r: 0.5546, Rho spearman: 0.5552\n",
      "Test -       Loss: 1.9082, Pearson r: 0.5420, Rho spearman: 0.5649\n",
      "\n",
      "Epoch 204/300\n",
      "Train -      Loss: 0.0048, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6542, Pearson r: 0.5544, Rho spearman: 0.5537\n",
      "Test -       Loss: 1.8687, Pearson r: 0.5452, Rho spearman: 0.5661\n",
      "\n",
      "Epoch 205/300\n",
      "Train -      Loss: 0.0039, Pearson r: 0.9994, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6904, Pearson r: 0.5500, Rho spearman: 0.5431\n",
      "Test -       Loss: 1.8826, Pearson r: 0.5444, Rho spearman: 0.5631\n",
      "\n",
      "Epoch 206/300\n",
      "Train -      Loss: 0.0041, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6840, Pearson r: 0.5531, Rho spearman: 0.5508\n",
      "Test -       Loss: 1.9042, Pearson r: 0.5440, Rho spearman: 0.5639\n",
      "\n",
      "Epoch 207/300\n",
      "Train -      Loss: 0.0042, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6340, Pearson r: 0.5584, Rho spearman: 0.5430\n",
      "Test -       Loss: 1.8976, Pearson r: 0.5404, Rho spearman: 0.5598\n",
      "\n",
      "Epoch 208/300\n",
      "Train -      Loss: 0.0040, Pearson r: 0.9994, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6368, Pearson r: 0.5581, Rho spearman: 0.5533\n",
      "Test -       Loss: 1.8861, Pearson r: 0.5444, Rho spearman: 0.5622\n",
      "\n",
      "Epoch 209/300\n",
      "Train -      Loss: 0.0045, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6575, Pearson r: 0.5545, Rho spearman: 0.5486\n",
      "Test -       Loss: 1.8860, Pearson r: 0.5419, Rho spearman: 0.5588\n",
      "\n",
      "Epoch 210/300\n",
      "Train -      Loss: 0.0050, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.7049, Pearson r: 0.5490, Rho spearman: 0.5477\n",
      "Test -       Loss: 1.8885, Pearson r: 0.5422, Rho spearman: 0.5652\n",
      "\n",
      "Epoch 211/300\n",
      "Train -      Loss: 0.0045, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.7371, Pearson r: 0.5467, Rho spearman: 0.5446\n",
      "Test -       Loss: 1.9157, Pearson r: 0.5421, Rho spearman: 0.5665\n",
      "\n",
      "Epoch 212/300\n",
      "Train -      Loss: 0.0036, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6513, Pearson r: 0.5553, Rho spearman: 0.5537\n",
      "Test -       Loss: 1.8922, Pearson r: 0.5389, Rho spearman: 0.5641\n",
      "\n",
      "Epoch 213/300\n",
      "Train -      Loss: 0.0036, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6137, Pearson r: 0.5615, Rho spearman: 0.5669\n",
      "Test -       Loss: 1.8825, Pearson r: 0.5440, Rho spearman: 0.5657\n",
      "\n",
      "Epoch 214/300\n",
      "Train -      Loss: 0.0040, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6839, Pearson r: 0.5516, Rho spearman: 0.5555\n",
      "Test -       Loss: 1.8963, Pearson r: 0.5422, Rho spearman: 0.5666\n",
      "\n",
      "Epoch 215/300\n",
      "Train -      Loss: 0.0048, Pearson r: 0.9992, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6762, Pearson r: 0.5530, Rho spearman: 0.5410\n",
      "Test -       Loss: 1.8972, Pearson r: 0.5497, Rho spearman: 0.5721\n",
      "\n",
      "Epoch 216/300\n",
      "Train -      Loss: 0.0040, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6582, Pearson r: 0.5554, Rho spearman: 0.5485\n",
      "Test -       Loss: 1.8792, Pearson r: 0.5441, Rho spearman: 0.5652\n",
      "\n",
      "Epoch 217/300\n",
      "Train -      Loss: 0.0036, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6682, Pearson r: 0.5543, Rho spearman: 0.5599\n",
      "Test -       Loss: 1.8832, Pearson r: 0.5451, Rho spearman: 0.5654\n",
      "\n",
      "Epoch 218/300\n",
      "Train -      Loss: 0.0034, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.7373, Pearson r: 0.5447, Rho spearman: 0.5419\n",
      "Test -       Loss: 1.8860, Pearson r: 0.5469, Rho spearman: 0.5674\n",
      "\n",
      "Epoch 219/300\n",
      "Train -      Loss: 0.0041, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6741, Pearson r: 0.5524, Rho spearman: 0.5475\n",
      "Test -       Loss: 1.8991, Pearson r: 0.5384, Rho spearman: 0.5594\n",
      "\n",
      "Epoch 220/300\n",
      "Train -      Loss: 0.0047, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.7110, Pearson r: 0.5489, Rho spearman: 0.5382\n",
      "Test -       Loss: 1.9235, Pearson r: 0.5389, Rho spearman: 0.5594\n",
      "\n",
      "Epoch 221/300\n",
      "Train -      Loss: 0.0036, Pearson r: 0.9994, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6561, Pearson r: 0.5565, Rho spearman: 0.5542\n",
      "Test -       Loss: 1.9059, Pearson r: 0.5398, Rho spearman: 0.5588\n",
      "\n",
      "Epoch 222/300\n",
      "Train -      Loss: 0.0034, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6649, Pearson r: 0.5543, Rho spearman: 0.5433\n",
      "Test -       Loss: 1.9033, Pearson r: 0.5373, Rho spearman: 0.5596\n",
      "\n",
      "Epoch 223/300\n",
      "Train -      Loss: 0.0034, Pearson r: 0.9995, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6811, Pearson r: 0.5523, Rho spearman: 0.5430\n",
      "Test -       Loss: 1.8977, Pearson r: 0.5422, Rho spearman: 0.5614\n",
      "\n",
      "Epoch 224/300\n",
      "Train -      Loss: 0.0043, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6782, Pearson r: 0.5528, Rho spearman: 0.5571\n",
      "Test -       Loss: 1.9085, Pearson r: 0.5407, Rho spearman: 0.5607\n",
      "\n",
      "Epoch 225/300\n",
      "Train -      Loss: 0.0045, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6842, Pearson r: 0.5509, Rho spearman: 0.5553\n",
      "Test -       Loss: 1.8877, Pearson r: 0.5413, Rho spearman: 0.5608\n",
      "\n",
      "Epoch 226/300\n",
      "Train -      Loss: 0.0044, Pearson r: 0.9993, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6368, Pearson r: 0.5588, Rho spearman: 0.5700\n",
      "Test -       Loss: 1.8602, Pearson r: 0.5521, Rho spearman: 0.5653\n",
      "\n",
      "Epoch 227/300\n",
      "Train -      Loss: 0.0036, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6977, Pearson r: 0.5512, Rho spearman: 0.5591\n",
      "Test -       Loss: 1.8758, Pearson r: 0.5509, Rho spearman: 0.5652\n",
      "\n",
      "Epoch 228/300\n",
      "Train -      Loss: 0.0023, Pearson r: 0.9996, Rho spearman: 0.9996\n",
      "Validation - Loss: 4.6266, Pearson r: 0.5593, Rho spearman: 0.5613\n",
      "Test -       Loss: 1.8781, Pearson r: 0.5456, Rho spearman: 0.5659\n",
      "\n",
      "Epoch 229/300\n",
      "Train -      Loss: 0.0025, Pearson r: 0.9996, Rho spearman: 0.9995\n",
      "Validation - Loss: 4.6337, Pearson r: 0.5588, Rho spearman: 0.5588\n",
      "Test -       Loss: 1.8793, Pearson r: 0.5459, Rho spearman: 0.5660\n",
      "\n",
      "Epoch 230/300\n",
      "Train -      Loss: 0.0036, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6449, Pearson r: 0.5573, Rho spearman: 0.5576\n",
      "Test -       Loss: 1.8801, Pearson r: 0.5473, Rho spearman: 0.5673\n",
      "\n",
      "Epoch 231/300\n",
      "Train -      Loss: 0.0050, Pearson r: 0.9992, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6060, Pearson r: 0.5632, Rho spearman: 0.5783\n",
      "Test -       Loss: 1.8969, Pearson r: 0.5437, Rho spearman: 0.5636\n",
      "\n",
      "Epoch 232/300\n",
      "Train -      Loss: 0.0045, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6163, Pearson r: 0.5604, Rho spearman: 0.5617\n",
      "Test -       Loss: 1.8793, Pearson r: 0.5445, Rho spearman: 0.5662\n",
      "\n",
      "Epoch 233/300\n",
      "Train -      Loss: 0.0031, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6507, Pearson r: 0.5579, Rho spearman: 0.5716\n",
      "Test -       Loss: 1.8954, Pearson r: 0.5468, Rho spearman: 0.5664\n",
      "\n",
      "Epoch 234/300\n",
      "Train -      Loss: 0.0029, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5578, Pearson r: 0.5698, Rho spearman: 0.5664\n",
      "Test -       Loss: 1.8896, Pearson r: 0.5445, Rho spearman: 0.5648\n",
      "\n",
      "Epoch 235/300\n",
      "Train -      Loss: 0.0038, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5456, Pearson r: 0.5709, Rho spearman: 0.5733\n",
      "Test -       Loss: 1.8883, Pearson r: 0.5439, Rho spearman: 0.5628\n",
      "\n",
      "Epoch 236/300\n",
      "Train -      Loss: 0.0046, Pearson r: 0.9993, Rho spearman: 0.9991\n",
      "Validation - Loss: 4.6205, Pearson r: 0.5604, Rho spearman: 0.5584\n",
      "Test -       Loss: 1.9014, Pearson r: 0.5408, Rho spearman: 0.5611\n",
      "\n",
      "Epoch 237/300\n",
      "Train -      Loss: 0.0038, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5439, Pearson r: 0.5717, Rho spearman: 0.5809\n",
      "Test -       Loss: 1.8954, Pearson r: 0.5443, Rho spearman: 0.5643\n",
      "\n",
      "Epoch 238/300\n",
      "Train -      Loss: 0.0033, Pearson r: 0.9995, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5587, Pearson r: 0.5686, Rho spearman: 0.5727\n",
      "Test -       Loss: 1.8850, Pearson r: 0.5441, Rho spearman: 0.5654\n",
      "\n",
      "Epoch 239/300\n",
      "Train -      Loss: 0.0034, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5305, Pearson r: 0.5733, Rho spearman: 0.5948\n",
      "Test -       Loss: 1.8988, Pearson r: 0.5396, Rho spearman: 0.5618\n",
      "\n",
      "Epoch 240/300\n",
      "Train -      Loss: 0.0041, Pearson r: 0.9994, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.5488, Pearson r: 0.5721, Rho spearman: 0.5862\n",
      "Test -       Loss: 1.9026, Pearson r: 0.5450, Rho spearman: 0.5648\n",
      "\n",
      "Epoch 241/300\n",
      "Train -      Loss: 0.0042, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.5363, Pearson r: 0.5724, Rho spearman: 0.5835\n",
      "Test -       Loss: 1.8852, Pearson r: 0.5436, Rho spearman: 0.5608\n",
      "\n",
      "Epoch 242/300\n",
      "Train -      Loss: 0.0039, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5434, Pearson r: 0.5720, Rho spearman: 0.5836\n",
      "Test -       Loss: 1.8997, Pearson r: 0.5415, Rho spearman: 0.5622\n",
      "\n",
      "Epoch 243/300\n",
      "Train -      Loss: 0.0038, Pearson r: 0.9994, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5255, Pearson r: 0.5739, Rho spearman: 0.5780\n",
      "Test -       Loss: 1.8954, Pearson r: 0.5439, Rho spearman: 0.5634\n",
      "\n",
      "Epoch 244/300\n",
      "Train -      Loss: 0.0041, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5947, Pearson r: 0.5649, Rho spearman: 0.5774\n",
      "Test -       Loss: 1.9073, Pearson r: 0.5400, Rho spearman: 0.5620\n",
      "\n",
      "Epoch 245/300\n",
      "Train -      Loss: 0.0032, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5464, Pearson r: 0.5722, Rho spearman: 0.5807\n",
      "Test -       Loss: 1.9082, Pearson r: 0.5401, Rho spearman: 0.5615\n",
      "\n",
      "Epoch 246/300\n",
      "Train -      Loss: 0.0031, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6171, Pearson r: 0.5619, Rho spearman: 0.5751\n",
      "Test -       Loss: 1.9119, Pearson r: 0.5417, Rho spearman: 0.5651\n",
      "\n",
      "Epoch 247/300\n",
      "Train -      Loss: 0.0036, Pearson r: 0.9994, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5376, Pearson r: 0.5713, Rho spearman: 0.5740\n",
      "Test -       Loss: 1.8916, Pearson r: 0.5425, Rho spearman: 0.5646\n",
      "\n",
      "Epoch 248/300\n",
      "Train -      Loss: 0.0039, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5899, Pearson r: 0.5659, Rho spearman: 0.5719\n",
      "Test -       Loss: 1.9038, Pearson r: 0.5412, Rho spearman: 0.5617\n",
      "\n",
      "Epoch 249/300\n",
      "Train -      Loss: 0.0037, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5752, Pearson r: 0.5673, Rho spearman: 0.5716\n",
      "Test -       Loss: 1.8887, Pearson r: 0.5440, Rho spearman: 0.5638\n",
      "\n",
      "Epoch 250/300\n",
      "Train -      Loss: 0.0036, Pearson r: 0.9994, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5911, Pearson r: 0.5672, Rho spearman: 0.5795\n",
      "Test -       Loss: 1.9086, Pearson r: 0.5445, Rho spearman: 0.5647\n",
      "\n",
      "Epoch 251/300\n",
      "Train -      Loss: 0.0038, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6163, Pearson r: 0.5623, Rho spearman: 0.5729\n",
      "Test -       Loss: 1.8943, Pearson r: 0.5466, Rho spearman: 0.5684\n",
      "\n",
      "Epoch 252/300\n",
      "Train -      Loss: 0.0039, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5682, Pearson r: 0.5685, Rho spearman: 0.5805\n",
      "Test -       Loss: 1.8927, Pearson r: 0.5442, Rho spearman: 0.5637\n",
      "\n",
      "\u001b[91mEpoch 253/300\n",
      "Train -      Loss: 0.0037, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5290, Pearson r: 0.5744, Rho spearman: 0.5779\n",
      "Test -       Loss: 1.8992, Pearson r: 0.5464, Rho spearman: 0.5677\u001b[0m\n",
      "\n",
      "Epoch 254/300\n",
      "Train -      Loss: 0.0032, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5724, Pearson r: 0.5684, Rho spearman: 0.5773\n",
      "Test -       Loss: 1.8931, Pearson r: 0.5469, Rho spearman: 0.5676\n",
      "\n",
      "Epoch 255/300\n",
      "Train -      Loss: 0.0030, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5307, Pearson r: 0.5722, Rho spearman: 0.5802\n",
      "Test -       Loss: 1.8929, Pearson r: 0.5431, Rho spearman: 0.5640\n",
      "\n",
      "Epoch 256/300\n",
      "Train -      Loss: 0.0035, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5421, Pearson r: 0.5710, Rho spearman: 0.5799\n",
      "Test -       Loss: 1.8873, Pearson r: 0.5407, Rho spearman: 0.5609\n",
      "\n",
      "Epoch 257/300\n",
      "Train -      Loss: 0.0041, Pearson r: 0.9994, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.5448, Pearson r: 0.5708, Rho spearman: 0.5735\n",
      "Test -       Loss: 1.8865, Pearson r: 0.5447, Rho spearman: 0.5645\n",
      "\n",
      "Epoch 258/300\n",
      "Train -      Loss: 0.0038, Pearson r: 0.9994, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.5238, Pearson r: 0.5737, Rho spearman: 0.5823\n",
      "Test -       Loss: 1.8991, Pearson r: 0.5476, Rho spearman: 0.5667\n",
      "\n",
      "Epoch 259/300\n",
      "Train -      Loss: 0.0039, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5264, Pearson r: 0.5724, Rho spearman: 0.5824\n",
      "Test -       Loss: 1.9001, Pearson r: 0.5392, Rho spearman: 0.5594\n",
      "\n",
      "Epoch 260/300\n",
      "Train -      Loss: 0.0033, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5643, Pearson r: 0.5688, Rho spearman: 0.5749\n",
      "Test -       Loss: 1.8959, Pearson r: 0.5417, Rho spearman: 0.5604\n",
      "\n",
      "\u001b[91mEpoch 261/300\n",
      "Train -      Loss: 0.0028, Pearson r: 0.9996, Rho spearman: 0.9995\n",
      "Validation - Loss: 4.4886, Pearson r: 0.5789, Rho spearman: 0.5846\n",
      "Test -       Loss: 1.8922, Pearson r: 0.5442, Rho spearman: 0.5641\u001b[0m\n",
      "\n",
      "Epoch 262/300\n",
      "Train -      Loss: 0.0030, Pearson r: 0.9995, Rho spearman: 0.9995\n",
      "Validation - Loss: 4.5288, Pearson r: 0.5729, Rho spearman: 0.5783\n",
      "Test -       Loss: 1.8888, Pearson r: 0.5460, Rho spearman: 0.5647\n",
      "\n",
      "Epoch 263/300\n",
      "Train -      Loss: 0.0033, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6035, Pearson r: 0.5619, Rho spearman: 0.5632\n",
      "Test -       Loss: 1.8860, Pearson r: 0.5448, Rho spearman: 0.5664\n",
      "\n",
      "Epoch 264/300\n",
      "Train -      Loss: 0.0039, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5941, Pearson r: 0.5645, Rho spearman: 0.5635\n",
      "Test -       Loss: 1.8766, Pearson r: 0.5485, Rho spearman: 0.5693\n",
      "\n",
      "Epoch 265/300\n",
      "Train -      Loss: 0.0037, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.6076, Pearson r: 0.5618, Rho spearman: 0.5632\n",
      "Test -       Loss: 1.8754, Pearson r: 0.5478, Rho spearman: 0.5693\n",
      "\n",
      "Epoch 266/300\n",
      "Train -      Loss: 0.0031, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5658, Pearson r: 0.5692, Rho spearman: 0.5677\n",
      "Test -       Loss: 1.8994, Pearson r: 0.5437, Rho spearman: 0.5649\n",
      "\n",
      "Epoch 267/300\n",
      "Train -      Loss: 0.0032, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5765, Pearson r: 0.5658, Rho spearman: 0.5627\n",
      "Test -       Loss: 1.8682, Pearson r: 0.5463, Rho spearman: 0.5672\n",
      "\n",
      "Epoch 268/300\n",
      "Train -      Loss: 0.0035, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5830, Pearson r: 0.5673, Rho spearman: 0.5744\n",
      "Test -       Loss: 1.8964, Pearson r: 0.5460, Rho spearman: 0.5649\n",
      "\n",
      "Epoch 269/300\n",
      "Train -      Loss: 0.0035, Pearson r: 0.9994, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5378, Pearson r: 0.5718, Rho spearman: 0.5748\n",
      "Test -       Loss: 1.8846, Pearson r: 0.5446, Rho spearman: 0.5626\n",
      "\n",
      "Epoch 270/300\n",
      "Train -      Loss: 0.0026, Pearson r: 0.9996, Rho spearman: 0.9995\n",
      "Validation - Loss: 4.6098, Pearson r: 0.5622, Rho spearman: 0.5768\n",
      "Test -       Loss: 1.8746, Pearson r: 0.5491, Rho spearman: 0.5686\n",
      "\n",
      "Epoch 271/300\n",
      "Train -      Loss: 0.0023, Pearson r: 0.9996, Rho spearman: 0.9996\n",
      "Validation - Loss: 4.5534, Pearson r: 0.5714, Rho spearman: 0.5862\n",
      "Test -       Loss: 1.8906, Pearson r: 0.5465, Rho spearman: 0.5651\n",
      "\n",
      "Epoch 272/300\n",
      "Train -      Loss: 0.0028, Pearson r: 0.9996, Rho spearman: 0.9995\n",
      "Validation - Loss: 4.6198, Pearson r: 0.5617, Rho spearman: 0.5797\n",
      "Test -       Loss: 1.8842, Pearson r: 0.5451, Rho spearman: 0.5651\n",
      "\n",
      "Epoch 273/300\n",
      "Train -      Loss: 0.0035, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6211, Pearson r: 0.5610, Rho spearman: 0.5778\n",
      "Test -       Loss: 1.8844, Pearson r: 0.5476, Rho spearman: 0.5656\n",
      "\n",
      "Epoch 274/300\n",
      "Train -      Loss: 0.0034, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6312, Pearson r: 0.5598, Rho spearman: 0.5792\n",
      "Test -       Loss: 1.8763, Pearson r: 0.5502, Rho spearman: 0.5681\n",
      "\n",
      "Epoch 275/300\n",
      "Train -      Loss: 0.0034, Pearson r: 0.9995, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5717, Pearson r: 0.5678, Rho spearman: 0.5769\n",
      "Test -       Loss: 1.8822, Pearson r: 0.5451, Rho spearman: 0.5657\n",
      "\n",
      "Epoch 276/300\n",
      "Train -      Loss: 0.0033, Pearson r: 0.9995, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5775, Pearson r: 0.5673, Rho spearman: 0.5743\n",
      "Test -       Loss: 1.8799, Pearson r: 0.5479, Rho spearman: 0.5667\n",
      "\n",
      "Epoch 277/300\n",
      "Train -      Loss: 0.0030, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5779, Pearson r: 0.5667, Rho spearman: 0.5706\n",
      "Test -       Loss: 1.8777, Pearson r: 0.5484, Rho spearman: 0.5675\n",
      "\n",
      "Epoch 278/300\n",
      "Train -      Loss: 0.0029, Pearson r: 0.9995, Rho spearman: 0.9995\n",
      "Validation - Loss: 4.5662, Pearson r: 0.5674, Rho spearman: 0.5671\n",
      "Test -       Loss: 1.8679, Pearson r: 0.5484, Rho spearman: 0.5690\n",
      "\n",
      "Epoch 279/300\n",
      "Train -      Loss: 0.0033, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6179, Pearson r: 0.5619, Rho spearman: 0.5696\n",
      "Test -       Loss: 1.8744, Pearson r: 0.5504, Rho spearman: 0.5691\n",
      "\n",
      "Epoch 280/300\n",
      "Train -      Loss: 0.0043, Pearson r: 0.9993, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.5722, Pearson r: 0.5678, Rho spearman: 0.5728\n",
      "Test -       Loss: 1.8762, Pearson r: 0.5484, Rho spearman: 0.5660\n",
      "\n",
      "Epoch 281/300\n",
      "Train -      Loss: 0.0033, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6458, Pearson r: 0.5588, Rho spearman: 0.5705\n",
      "Test -       Loss: 1.8921, Pearson r: 0.5448, Rho spearman: 0.5630\n",
      "\n",
      "Epoch 282/300\n",
      "Train -      Loss: 0.0032, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5432, Pearson r: 0.5723, Rho spearman: 0.5776\n",
      "Test -       Loss: 1.8736, Pearson r: 0.5460, Rho spearman: 0.5639\n",
      "\n",
      "Epoch 283/300\n",
      "Train -      Loss: 0.0029, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6002, Pearson r: 0.5654, Rho spearman: 0.5695\n",
      "Test -       Loss: 1.8934, Pearson r: 0.5469, Rho spearman: 0.5678\n",
      "\n",
      "\u001b[91mEpoch 284/300\n",
      "Train -      Loss: 0.0031, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.4859, Pearson r: 0.5790, Rho spearman: 0.5824\n",
      "Test -       Loss: 1.8748, Pearson r: 0.5492, Rho spearman: 0.5682\u001b[0m\n",
      "\n",
      "\u001b[91mEpoch 285/300\n",
      "Train -      Loss: 0.0024, Pearson r: 0.9996, Rho spearman: 0.9996\n",
      "Validation - Loss: 4.4795, Pearson r: 0.5812, Rho spearman: 0.5779\n",
      "Test -       Loss: 1.8749, Pearson r: 0.5510, Rho spearman: 0.5698\u001b[0m\n",
      "\n",
      "\u001b[91mEpoch 286/300\n",
      "Train -      Loss: 0.0025, Pearson r: 0.9996, Rho spearman: 0.9995\n",
      "Validation - Loss: 4.4506, Pearson r: 0.5837, Rho spearman: 0.5786\n",
      "Test -       Loss: 1.8929, Pearson r: 0.5423, Rho spearman: 0.5643\u001b[0m\n",
      "\n",
      "Epoch 287/300\n",
      "Train -      Loss: 0.0030, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5228, Pearson r: 0.5736, Rho spearman: 0.5706\n",
      "Test -       Loss: 1.8848, Pearson r: 0.5466, Rho spearman: 0.5678\n",
      "\n",
      "Epoch 288/300\n",
      "Train -      Loss: 0.0035, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5921, Pearson r: 0.5651, Rho spearman: 0.5563\n",
      "Test -       Loss: 1.9076, Pearson r: 0.5414, Rho spearman: 0.5622\n",
      "\n",
      "Epoch 289/300\n",
      "Train -      Loss: 0.0031, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5373, Pearson r: 0.5722, Rho spearman: 0.5719\n",
      "Test -       Loss: 1.8947, Pearson r: 0.5462, Rho spearman: 0.5667\n",
      "\n",
      "Epoch 290/300\n",
      "Train -      Loss: 0.0036, Pearson r: 0.9994, Rho spearman: 0.9993\n",
      "Validation - Loss: 4.5502, Pearson r: 0.5710, Rho spearman: 0.5617\n",
      "Test -       Loss: 1.8883, Pearson r: 0.5456, Rho spearman: 0.5646\n",
      "\n",
      "Epoch 291/300\n",
      "Train -      Loss: 0.0038, Pearson r: 0.9994, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5907, Pearson r: 0.5649, Rho spearman: 0.5558\n",
      "Test -       Loss: 1.8704, Pearson r: 0.5454, Rho spearman: 0.5668\n",
      "\n",
      "Epoch 292/300\n",
      "Train -      Loss: 0.0028, Pearson r: 0.9996, Rho spearman: 0.9995\n",
      "Validation - Loss: 4.5978, Pearson r: 0.5630, Rho spearman: 0.5638\n",
      "Test -       Loss: 1.8584, Pearson r: 0.5508, Rho spearman: 0.5697\n",
      "\n",
      "Epoch 293/300\n",
      "Train -      Loss: 0.0023, Pearson r: 0.9996, Rho spearman: 0.9996\n",
      "Validation - Loss: 4.6311, Pearson r: 0.5602, Rho spearman: 0.5662\n",
      "Test -       Loss: 1.8832, Pearson r: 0.5463, Rho spearman: 0.5670\n",
      "\n",
      "Epoch 294/300\n",
      "Train -      Loss: 0.0026, Pearson r: 0.9996, Rho spearman: 0.9995\n",
      "Validation - Loss: 4.5875, Pearson r: 0.5650, Rho spearman: 0.5752\n",
      "Test -       Loss: 1.8778, Pearson r: 0.5454, Rho spearman: 0.5665\n",
      "\n",
      "Epoch 295/300\n",
      "Train -      Loss: 0.0030, Pearson r: 0.9995, Rho spearman: 0.9995\n",
      "Validation - Loss: 4.6521, Pearson r: 0.5572, Rho spearman: 0.5648\n",
      "Test -       Loss: 1.8882, Pearson r: 0.5459, Rho spearman: 0.5695\n",
      "\n",
      "Epoch 296/300\n",
      "Train -      Loss: 0.0031, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.7110, Pearson r: 0.5492, Rho spearman: 0.5609\n",
      "Test -       Loss: 1.8919, Pearson r: 0.5449, Rho spearman: 0.5678\n",
      "\n",
      "Epoch 297/300\n",
      "Train -      Loss: 0.0031, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.5723, Pearson r: 0.5676, Rho spearman: 0.5579\n",
      "Test -       Loss: 1.8749, Pearson r: 0.5457, Rho spearman: 0.5687\n",
      "\n",
      "Epoch 298/300\n",
      "Train -      Loss: 0.0036, Pearson r: 0.9994, Rho spearman: 0.9992\n",
      "Validation - Loss: 4.6472, Pearson r: 0.5581, Rho spearman: 0.5630\n",
      "Test -       Loss: 1.8789, Pearson r: 0.5471, Rho spearman: 0.5681\n",
      "\n",
      "Epoch 299/300\n",
      "Train -      Loss: 0.0033, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6553, Pearson r: 0.5572, Rho spearman: 0.5629\n",
      "Test -       Loss: 1.8943, Pearson r: 0.5456, Rho spearman: 0.5674\n",
      "\n",
      "Epoch 300/300\n",
      "Train -      Loss: 0.0030, Pearson r: 0.9995, Rho spearman: 0.9994\n",
      "Validation - Loss: 4.6153, Pearson r: 0.5623, Rho spearman: 0.5710\n",
      "Test -       Loss: 1.8764, Pearson r: 0.5479, Rho spearman: 0.5694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PROVA base base\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lr = 1e-4\n",
    "input_dim = 1280\n",
    "\n",
    "transf_parameters={'input_dim':1280, 'num_heads':8,\n",
    "                    'dropout_rate':0.,}\n",
    "\n",
    "patience = 300\n",
    "DDG_model = TransformerRegression\n",
    "Final_model = Cross_Attention_DDG(DDG_model, cross_att = True, dual_cross_att=True, **transf_parameters)\n",
    "\n",
    "path_save_fig = 'DDGemb \\n ----------------------------------'\n",
    "print(path_save_fig)\n",
    "p_tr,p_val, p_te, l_tr,l_val, l_te, pearson_max_val, best_model = training_and_validation_loop_ddg(Final_model, dataloader_train, dataloader_test,\n",
    "                                                                                   dataloader_validation,\n",
    "                                                                                   path_save_fig, epochs=300, lr =lr,patience = patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de417afa-6803-45bb-9612-54581f7c6358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('base_ddg.conv1d.weight',\n",
       "              tensor([[[ 3.7088e-02,  1.9108e-02,  1.6996e-02,  ...,  3.9826e-02,\n",
       "                         3.5682e-03,  5.5280e-02],\n",
       "                       [ 6.3159e-02, -1.8141e-02,  1.1711e-02,  ...,  4.7014e-02,\n",
       "                         8.5385e-03, -1.0180e-02],\n",
       "                       [ 1.3502e-02,  5.9901e-02,  1.7355e-02,  ..., -1.4850e-02,\n",
       "                         3.8437e-02, -9.4598e-02],\n",
       "                       ...,\n",
       "                       [-2.0559e-02, -9.6466e-04,  4.2317e-03,  ..., -2.2692e-02,\n",
       "                         2.2367e-02,  6.7275e-02],\n",
       "                       [ 7.1387e-02,  5.3270e-02,  3.5843e-02,  ...,  1.4182e-02,\n",
       "                         3.0170e-02, -3.5793e-03],\n",
       "                       [ 1.6414e-02,  3.4843e-02,  6.6215e-02,  ..., -5.1677e-02,\n",
       "                         2.6038e-02,  2.0384e-02]],\n",
       "              \n",
       "                      [[-1.0834e-01, -4.9843e-02,  2.6710e-02,  ..., -3.2249e-02,\n",
       "                         3.1481e-02,  3.0103e-02],\n",
       "                       [-1.8271e-02,  1.7393e-02, -4.7726e-03,  ..., -4.2942e-02,\n",
       "                         4.1515e-02, -4.8907e-02],\n",
       "                       [-4.4523e-02,  1.1564e-03,  2.9734e-02,  ..., -2.1464e-02,\n",
       "                        -4.9195e-03,  1.1923e-02],\n",
       "                       ...,\n",
       "                       [-4.1414e-02, -7.4431e-02, -1.8975e-02,  ...,  3.5759e-02,\n",
       "                        -2.4908e-02,  2.9494e-02],\n",
       "                       [-2.6263e-02,  1.0544e-02, -1.2784e-02,  ...,  4.4589e-03,\n",
       "                        -5.5482e-02, -1.9163e-02],\n",
       "                       [-3.9010e-03, -6.1938e-02,  7.1016e-03,  ..., -5.9056e-02,\n",
       "                         6.0531e-03,  2.4004e-02]],\n",
       "              \n",
       "                      [[-4.5317e-02, -4.7770e-02, -1.4185e-02,  ..., -4.0577e-02,\n",
       "                        -7.7162e-02, -9.9454e-02],\n",
       "                       [ 7.0289e-02,  7.4847e-02,  6.1188e-02,  ..., -2.5220e-02,\n",
       "                         5.7936e-02, -1.1678e-03],\n",
       "                       [ 7.9922e-04,  4.1115e-02, -3.5790e-04,  ...,  3.8254e-02,\n",
       "                        -4.5266e-02, -5.9755e-02],\n",
       "                       ...,\n",
       "                       [ 3.5509e-02, -3.9289e-02, -8.4645e-02,  ..., -4.9415e-02,\n",
       "                        -7.5985e-02, -2.0676e-02],\n",
       "                       [-4.0048e-02, -7.5525e-02, -1.0902e-01,  ..., -1.8007e-02,\n",
       "                         2.8211e-02, -2.0830e-02],\n",
       "                       [-1.4978e-01,  4.6405e-03,  2.6121e-02,  ..., -3.3994e-02,\n",
       "                         3.0811e-02,  7.0080e-03]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 5.7037e-02, -1.4872e-03, -3.6268e-02,  ..., -2.6017e-02,\n",
       "                         7.3910e-02, -2.1142e-02],\n",
       "                       [ 1.6433e-02,  5.6152e-03,  2.0407e-02,  ..., -9.7020e-03,\n",
       "                        -5.3343e-02, -8.6073e-03],\n",
       "                       [-2.3623e-03,  3.7507e-02, -2.2526e-02,  ..., -2.8508e-02,\n",
       "                        -8.2865e-02,  4.3110e-02],\n",
       "                       ...,\n",
       "                       [ 3.0658e-02,  8.3230e-02,  9.7572e-02,  ...,  3.6456e-02,\n",
       "                         5.0464e-05,  8.5595e-03],\n",
       "                       [-3.0213e-02,  8.2459e-02,  8.5462e-02,  ...,  4.6710e-02,\n",
       "                        -3.3549e-03, -7.8001e-03],\n",
       "                       [-1.7101e-02, -5.5755e-02,  1.4687e-02,  ...,  1.5824e-02,\n",
       "                         5.7799e-02, -1.1608e-02]],\n",
       "              \n",
       "                      [[ 1.3775e-02, -2.7626e-02,  4.5092e-03,  ...,  5.4386e-02,\n",
       "                        -4.8494e-02, -6.3038e-02],\n",
       "                       [ 2.1996e-02,  3.6649e-02,  5.9129e-02,  ...,  1.6261e-02,\n",
       "                         4.9427e-02,  3.6863e-03],\n",
       "                       [-1.9733e-03,  3.3180e-02,  8.1131e-02,  ...,  7.9344e-02,\n",
       "                         3.0228e-02, -7.2263e-02],\n",
       "                       ...,\n",
       "                       [-2.3851e-02, -8.4896e-02, -9.8729e-02,  ..., -4.0077e-02,\n",
       "                        -1.0364e-01, -4.5185e-02],\n",
       "                       [-2.9344e-02, -1.0279e-01, -3.1098e-02,  ...,  1.4078e-02,\n",
       "                        -1.2366e-02, -9.4144e-02],\n",
       "                       [-5.7577e-02, -8.8424e-03,  4.8781e-03,  ..., -8.5504e-02,\n",
       "                        -9.8527e-03,  5.4474e-02]],\n",
       "              \n",
       "                      [[-4.2438e-02,  5.3501e-03,  4.2532e-02,  ..., -5.3387e-03,\n",
       "                         4.1796e-03, -4.9110e-02],\n",
       "                       [ 1.3515e-01,  1.7356e-02,  4.8770e-03,  ...,  2.2264e-02,\n",
       "                         2.8635e-02, -2.0401e-02],\n",
       "                       [ 5.7087e-02, -3.4498e-02,  2.1659e-02,  ..., -7.9350e-02,\n",
       "                        -5.6722e-02, -1.3479e-02],\n",
       "                       ...,\n",
       "                       [-3.5497e-02, -9.5709e-02, -7.2671e-02,  ..., -5.7419e-03,\n",
       "                         1.7151e-02,  1.8914e-02],\n",
       "                       [-3.1334e-02, -2.4910e-02, -8.4833e-03,  ...,  7.5960e-02,\n",
       "                         4.2652e-02, -2.5363e-02],\n",
       "                       [-6.6476e-02,  2.1041e-03,  2.8155e-02,  ..., -8.7083e-02,\n",
       "                        -6.3881e-02,  3.5888e-02]]], device='cuda:0')),\n",
       "             ('base_ddg.conv1d.bias',\n",
       "              tensor([ 0.0260,  0.0162, -0.0009, -0.0077,  0.0405,  0.0291,  0.0030,  0.0305,\n",
       "                       0.0058,  0.0049,  0.0022, -0.0219, -0.0227,  0.0175, -0.0174,  0.0099,\n",
       "                      -0.0030, -0.0120,  0.0130,  0.0257, -0.0083,  0.0041, -0.0072, -0.0140,\n",
       "                      -0.0175, -0.0064,  0.0031, -0.0161,  0.0459,  0.0029,  0.0108,  0.0099,\n",
       "                      -0.0268, -0.0071,  0.0199, -0.0166, -0.0010, -0.0037, -0.0064,  0.0134,\n",
       "                       0.0058,  0.0136, -0.0203, -0.0097,  0.0025,  0.0005, -0.0054,  0.0322,\n",
       "                       0.0030,  0.0018, -0.0192,  0.0722, -0.0224,  0.0342, -0.0266,  0.0154,\n",
       "                      -0.0021,  0.0165, -0.0319,  0.0167, -0.0439,  0.0004, -0.0702, -0.0020,\n",
       "                      -0.0464,  0.0034, -0.0894, -0.0123, -0.0457, -0.0268, -0.0770, -0.0770,\n",
       "                      -0.0624, -0.0446, -0.0573, -0.0309, -0.0824, -0.0312, -0.0877, -0.0320,\n",
       "                      -0.0963, -0.0639, -0.0930, -0.1200, -0.0321, -0.0193, -0.0579, -0.0338,\n",
       "                      -0.0137, -0.1099, -0.0252, -0.1091,  0.0027, -0.0895, -0.0006, -0.1045,\n",
       "                      -0.0005, -0.1198, -0.0017, -0.0696,  0.0014, -0.0537, -0.0175, -0.1304,\n",
       "                      -0.0123, -0.0734,  0.0068, -0.0508,  0.0031, -0.1808, -0.0082, -0.0826,\n",
       "                      -0.0182, -0.1109,  0.0023, -0.0252, -0.0128, -0.0361, -0.0018, -0.1862,\n",
       "                       0.0120, -0.0543, -0.0138, -0.1112, -0.0120, -0.0458,  0.0121, -0.0411],\n",
       "                     device='cuda:0')),\n",
       "             ('base_ddg.conv1d_wild.weight',\n",
       "              tensor([[[-0.0139, -0.0091, -0.0184,  ...,  0.0191,  0.0330,  0.0020],\n",
       "                       [ 0.0274, -0.0092, -0.0288,  ...,  0.0145,  0.0514,  0.0410],\n",
       "                       [-0.0375, -0.1031, -0.0400,  ..., -0.0728, -0.0108,  0.0402],\n",
       "                       ...,\n",
       "                       [ 0.0272, -0.0117, -0.0327,  ...,  0.0025,  0.0382,  0.0509],\n",
       "                       [-0.0421,  0.0715,  0.0737,  ..., -0.0030,  0.0620,  0.0555],\n",
       "                       [-0.0012,  0.0467,  0.0219,  ..., -0.0247, -0.0395, -0.0466]],\n",
       "              \n",
       "                      [[-0.0492, -0.0425, -0.0165,  ..., -0.0613, -0.0713, -0.0325],\n",
       "                       [ 0.0519,  0.0495, -0.0467,  ..., -0.0487, -0.0344, -0.0167],\n",
       "                       [ 0.0195,  0.0112, -0.0091,  ...,  0.0409, -0.0386, -0.0180],\n",
       "                       ...,\n",
       "                       [-0.0116,  0.0540,  0.0470,  ..., -0.0394, -0.0262, -0.0396],\n",
       "                       [ 0.0589, -0.0246, -0.0418,  ...,  0.0396, -0.0290,  0.0011],\n",
       "                       [ 0.0428, -0.0030,  0.0578,  ...,  0.0400,  0.0006,  0.0668]],\n",
       "              \n",
       "                      [[-0.0755, -0.1054, -0.1102,  ..., -0.0239, -0.0179, -0.0326],\n",
       "                       [-0.0295,  0.0231,  0.0063,  ...,  0.0316, -0.0418, -0.0195],\n",
       "                       [ 0.0189,  0.0296,  0.0903,  ..., -0.0132,  0.0193,  0.0399],\n",
       "                       ...,\n",
       "                       [-0.0057,  0.0191,  0.0012,  ...,  0.0129,  0.0416,  0.0271],\n",
       "                       [ 0.0235,  0.0227,  0.0302,  ...,  0.0503, -0.0306,  0.0066],\n",
       "                       [ 0.0325,  0.0045,  0.0410,  ..., -0.0007, -0.0268,  0.0591]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0226,  0.0375,  0.0231,  ...,  0.0075,  0.0037, -0.0133],\n",
       "                       [-0.0347, -0.0419, -0.0067,  ..., -0.0105, -0.0667,  0.0023],\n",
       "                       [-0.0205, -0.0470, -0.0416,  ..., -0.0065,  0.0155,  0.0062],\n",
       "                       ...,\n",
       "                       [ 0.0083,  0.0068,  0.0165,  ..., -0.0173, -0.0127,  0.0254],\n",
       "                       [ 0.0384,  0.0456,  0.0597,  ...,  0.0403,  0.0187, -0.0443],\n",
       "                       [-0.0249, -0.0112,  0.0401,  ..., -0.0245,  0.0327, -0.0283]],\n",
       "              \n",
       "                      [[-0.0379, -0.0583, -0.0287,  ...,  0.0262,  0.0348, -0.0044],\n",
       "                       [-0.0083, -0.0150,  0.0425,  ...,  0.0710, -0.0093, -0.0218],\n",
       "                       [ 0.0772, -0.0068,  0.0823,  ..., -0.0174, -0.0032, -0.0107],\n",
       "                       ...,\n",
       "                       [ 0.0244,  0.0402,  0.0237,  ...,  0.0430,  0.0838,  0.1142],\n",
       "                       [-0.0292, -0.0832,  0.0090,  ...,  0.0446,  0.0312,  0.0495],\n",
       "                       [-0.0159,  0.0161, -0.0151,  ...,  0.0494,  0.0109, -0.0442]],\n",
       "              \n",
       "                      [[-0.0142, -0.0104,  0.0558,  ..., -0.0022,  0.0263, -0.0266],\n",
       "                       [ 0.0403, -0.0461,  0.0028,  ..., -0.0553, -0.0317,  0.0092],\n",
       "                       [-0.0027, -0.0015, -0.0240,  ..., -0.0160,  0.0296,  0.0263],\n",
       "                       ...,\n",
       "                       [-0.0219, -0.0656, -0.0579,  ..., -0.0786, -0.0658, -0.0506],\n",
       "                       [-0.0067,  0.0408, -0.0410,  ...,  0.0059,  0.0350, -0.0952],\n",
       "                       [-0.0216,  0.0361,  0.0039,  ...,  0.0041,  0.0141,  0.0647]]],\n",
       "                     device='cuda:0')),\n",
       "             ('base_ddg.conv1d_wild.bias',\n",
       "              tensor([ 2.5508e-02, -1.0781e-02,  5.8643e-03,  1.0111e-02, -7.6802e-03,\n",
       "                      -2.3739e-02, -1.0491e-02, -9.9349e-03,  9.7352e-03,  3.5623e-02,\n",
       "                       7.3630e-03, -1.1392e-02,  3.6146e-03,  7.3464e-03, -1.0452e-02,\n",
       "                       1.7956e-03, -1.9241e-02, -1.2707e-02, -6.7507e-03,  7.1291e-03,\n",
       "                      -3.9931e-02, -1.0278e-02,  4.1396e-03,  5.0304e-03,  2.1437e-02,\n",
       "                      -1.7773e-02,  7.6845e-04, -2.8200e-03,  1.1575e-03,  2.8689e-04,\n",
       "                       1.1648e-02, -4.0629e-03, -1.5959e-03,  1.6000e-02,  1.8120e-03,\n",
       "                      -7.0643e-03,  4.9636e-03, -1.4870e-02, -2.8701e-03, -1.0571e-02,\n",
       "                      -5.8292e-03,  8.0305e-03,  2.1162e-03, -8.2010e-03, -1.0981e-02,\n",
       "                      -6.7673e-04,  2.7345e-02, -6.5739e-05,  2.3049e-02,  1.2160e-02,\n",
       "                       3.0668e-02, -1.9355e-02, -6.4101e-03,  1.0471e-02, -4.9889e-03,\n",
       "                       3.5906e-03, -2.4222e-03, -7.9245e-03,  3.7107e-03,  6.4441e-03,\n",
       "                       1.4141e-04,  1.1956e-02,  3.2482e-03, -1.6993e-04, -6.7212e-03,\n",
       "                      -5.4669e-03, -1.3390e-02, -6.4587e-03, -1.6722e-03,  3.2608e-03,\n",
       "                      -7.3456e-03, -1.6049e-02, -7.8509e-03,  8.1520e-03, -1.6616e-02,\n",
       "                      -1.7782e-02,  1.3989e-02, -1.1253e-02,  2.9095e-03, -8.3631e-03,\n",
       "                       4.2752e-03, -7.2152e-04, -1.0179e-02, -1.6233e-02,  1.2036e-02,\n",
       "                       1.0004e-03, -6.0401e-03,  9.4129e-03, -5.2927e-02, -4.4971e-03,\n",
       "                       9.5890e-03, -3.2565e-03, -2.5426e-02, -1.2216e-02, -4.4962e-02,\n",
       "                       9.3954e-04, -1.6589e-02, -1.6936e-02,  2.0833e-02,  1.4009e-02,\n",
       "                      -2.8518e-03,  3.0203e-02, -2.7983e-03,  8.3480e-03,  5.4188e-03,\n",
       "                      -9.8586e-03,  1.4626e-02, -1.1039e-02, -6.9157e-03, -1.3285e-02,\n",
       "                       2.5079e-03, -1.8048e-02, -1.1843e-02, -1.4523e-02, -2.1297e-03,\n",
       "                       1.1716e-03, -1.1264e-02, -2.6731e-02, -9.4424e-03,  2.7676e-02,\n",
       "                       7.3077e-03, -4.9846e-03,  7.9629e-03,  8.2582e-03, -2.8857e-03,\n",
       "                      -5.1256e-03, -8.7238e-03,  4.0209e-02], device='cuda:0')),\n",
       "             ('base_ddg.norm1.weight',\n",
       "              tensor([1.1492, 1.0843, 1.2523, 1.2067, 1.0959, 1.0747, 1.0589, 1.0584, 1.0845,\n",
       "                      1.0337, 1.0393, 1.0649, 1.0395, 1.0415, 1.0246, 1.0706, 1.0303, 1.0321,\n",
       "                      1.0049, 1.0122, 1.0209, 1.0148, 1.0177, 0.9935, 1.0410, 1.0258, 1.0136,\n",
       "                      1.0250, 0.9425, 0.9863, 0.9621, 0.9920, 1.0024, 0.9808, 0.9811, 0.9377,\n",
       "                      1.0035, 0.8926, 1.0106, 0.9343, 0.9839, 0.9929, 0.9613, 0.9878, 0.8656,\n",
       "                      0.9611, 0.9582, 0.9926, 0.9884, 1.0429, 0.9218, 1.0147, 0.9260, 0.8887,\n",
       "                      0.7370, 0.9986, 0.9682, 0.7471, 0.8891, 0.8273, 0.9785, 1.0027, 0.9252,\n",
       "                      0.9242, 0.9257, 0.9776, 0.9212, 0.9909, 0.8833, 0.6583, 0.8460, 1.0273,\n",
       "                      0.8214, 0.9936, 0.6801, 0.7712, 0.9033, 0.9470, 0.9381, 0.9125, 0.9191,\n",
       "                      0.9708, 0.8973, 0.9033, 0.8700, 0.9422, 0.9164, 1.0107, 0.9267, 0.8085,\n",
       "                      1.0205, 0.9500, 0.9405, 0.9141, 1.0135, 0.8798, 0.9057, 0.9623, 0.8580,\n",
       "                      0.9758, 0.9625, 0.9597, 1.0254, 0.9201, 0.9645, 0.9689, 0.9262, 0.9278,\n",
       "                      1.0489, 0.9444, 1.0259, 0.8669, 1.0681, 1.0320, 0.8053, 0.9372, 1.0240,\n",
       "                      0.9411, 1.0439, 0.9027, 0.8797, 0.9511, 1.0552, 0.9293, 1.0209, 1.0206,\n",
       "                      0.9035, 1.0104], device='cuda:0')),\n",
       "             ('base_ddg.norm1.bias',\n",
       "              tensor([ 0.0278,  0.0129, -0.0073, -0.0027,  0.0380,  0.0421,  0.0041,  0.0026,\n",
       "                       0.0149,  0.0364,  0.0155, -0.0248, -0.0049,  0.0070, -0.0115,  0.0342,\n",
       "                       0.0440, -0.0031, -0.0037,  0.0063, -0.0214,  0.0050, -0.0495, -0.0069,\n",
       "                      -0.0208,  0.0099,  0.0068, -0.0565,  0.0176, -0.0136,  0.0206,  0.0136,\n",
       "                       0.0034, -0.0020,  0.0065, -0.0121, -0.0065,  0.0212, -0.0164,  0.0173,\n",
       "                      -0.0123, -0.0004, -0.0275, -0.0406, -0.0476, -0.0095, -0.0094, -0.0069,\n",
       "                      -0.0105, -0.0175, -0.0271,  0.0007, -0.0292, -0.0162, -0.0397, -0.0303,\n",
       "                       0.0019, -0.0088, -0.0247, -0.0298, -0.0209, -0.0070, -0.0267, -0.0017,\n",
       "                      -0.0367, -0.0023, -0.0873, -0.0079, -0.0236, -0.0572, -0.0190, -0.0425,\n",
       "                      -0.0229, -0.0348, -0.0279, -0.0593, -0.0595, -0.0457, -0.0125, -0.0422,\n",
       "                      -0.0209, -0.0627, -0.0038, -0.0756,  0.0025, -0.0189, -0.0039, -0.0088,\n",
       "                       0.0038, -0.1239,  0.0028, -0.0620,  0.0209, -0.0924,  0.0248, -0.0704,\n",
       "                      -0.0065, -0.1147,  0.0095, -0.0753,  0.0364, -0.0293,  0.0245, -0.1670,\n",
       "                       0.0279, -0.0800,  0.0003, -0.1325, -0.0084, -0.0950, -0.0007, -0.0586,\n",
       "                      -0.0036, -0.0180,  0.0091, -0.0388, -0.0021, -0.0903,  0.0062, -0.1076,\n",
       "                       0.0080, -0.0748,  0.0041, -0.0902, -0.0014, -0.0600, -0.0165, -0.0085],\n",
       "                     device='cuda:0')),\n",
       "             ('base_ddg.norm2.weight',\n",
       "              tensor([0.8923, 0.8547, 0.8457, 0.8256, 0.7046, 0.7798, 0.7375, 0.7983, 0.8032,\n",
       "                      0.7286, 0.7571, 0.8056, 0.6650, 0.6691, 0.6351, 0.5861, 0.7375, 0.9386,\n",
       "                      0.8907, 0.7532, 0.8806, 0.9197, 0.6929, 0.7851, 0.6252, 0.8454, 0.6839,\n",
       "                      0.7888, 0.7454, 0.7405, 0.8982, 0.7977, 0.7899, 0.7149, 0.7245, 0.7095,\n",
       "                      0.7646, 0.8129, 0.7116, 0.8864, 0.8127, 0.8114, 0.6943, 0.7361, 0.7386,\n",
       "                      0.7540, 0.8832, 0.6919, 0.6389, 0.9350, 1.1608, 0.8055, 0.6328, 0.8181,\n",
       "                      0.6500, 0.9829, 0.6943, 0.8179, 0.8140, 0.6337, 0.6420, 0.7620, 0.7128,\n",
       "                      0.8442, 0.7738, 0.8361, 0.9577, 0.5761, 0.9611, 0.6553, 0.4855, 0.6185,\n",
       "                      0.7667, 0.7297, 0.9334, 0.5792, 0.9229, 0.7696, 1.0071, 0.5622, 0.5498,\n",
       "                      0.7684, 0.6378, 0.6265, 0.7288, 0.6574, 0.6928, 0.7832, 1.0665, 0.7345,\n",
       "                      1.0076, 0.8120, 1.1941, 0.6298, 0.9258, 0.6682, 0.7839, 0.7244, 0.9811,\n",
       "                      1.0847, 1.0569, 0.6280, 0.9460, 1.0338, 0.5763, 0.5694, 1.1004, 0.5609,\n",
       "                      0.6960, 0.7752, 0.9406, 0.8363, 0.7976, 0.6831, 0.6991, 0.7778, 0.6994,\n",
       "                      0.7425, 0.7362, 1.0722, 0.6479, 0.7649, 0.7666, 0.7345, 0.8027, 0.8604,\n",
       "                      1.1104, 0.9872], device='cuda:0')),\n",
       "             ('base_ddg.norm2.bias',\n",
       "              tensor([ 5.0466e-02,  1.7170e-02,  6.2484e-02,  4.8966e-02,  5.7082e-02,\n",
       "                       8.5337e-02,  4.3708e-02,  5.1456e-02,  4.1011e-02, -1.1693e-02,\n",
       "                       5.5327e-02,  4.7463e-02,  6.7137e-02,  8.3219e-02,  5.8641e-02,\n",
       "                       6.4826e-02,  5.3501e-02,  4.3009e-02,  1.5926e-02,  5.8749e-02,\n",
       "                       9.3022e-02,  4.5731e-02,  6.4100e-02,  5.1288e-02,  2.4876e-02,\n",
       "                      -5.2229e-05,  2.8494e-02,  6.9772e-02,  5.8314e-02,  1.1808e-01,\n",
       "                       4.8042e-02,  7.8038e-02,  4.0784e-02,  7.9113e-02,  3.7008e-02,\n",
       "                       7.4637e-02,  3.1410e-02,  2.1424e-02,  3.2797e-02,  4.8142e-02,\n",
       "                       5.2116e-02,  2.5928e-02,  4.5108e-02,  9.7008e-02,  4.2705e-02,\n",
       "                       2.6722e-02,  3.0912e-02,  2.8855e-02,  5.2897e-02,  5.7437e-02,\n",
       "                       5.9571e-02,  3.3140e-02,  4.6698e-02,  3.2724e-02,  3.8652e-02,\n",
       "                       3.2193e-02,  2.4594e-02,  5.1795e-02,  4.4079e-02,  3.8412e-02,\n",
       "                       5.0705e-02,  3.6430e-02,  5.3754e-02,  3.3727e-02,  2.2831e-02,\n",
       "                       2.9248e-02,  3.3767e-03,  4.3156e-02, -6.7551e-03,  3.3206e-02,\n",
       "                       4.1995e-02,  5.9770e-02,  4.6296e-02,  4.2296e-02, -1.2950e-02,\n",
       "                       3.9094e-02,  3.6083e-02,  3.4657e-02,  4.2009e-02, -1.4653e-03,\n",
       "                       3.6665e-02,  1.7208e-02,  4.7174e-02,  2.7310e-02,  7.5119e-02,\n",
       "                       2.5626e-02,  2.5771e-02,  1.7560e-02, -7.6110e-02,  2.9365e-02,\n",
       "                       2.8878e-02,  1.6862e-02, -4.7635e-02,  4.9749e-02,  3.4566e-02,\n",
       "                       1.0326e-02,  1.0277e-01,  4.8956e-02,  2.7486e-02,  7.6000e-02,\n",
       "                      -2.3119e-02,  3.5407e-02, -1.7390e-02,  5.3550e-02,  3.9449e-02,\n",
       "                       1.8175e-02,  3.1864e-02,  1.5374e-02,  3.1557e-02,  1.9975e-02,\n",
       "                       3.6454e-02,  3.9769e-02,  6.0122e-02,  3.0291e-02,  3.8550e-02,\n",
       "                       5.0810e-02,  4.6736e-02,  4.8739e-02,  6.5781e-02,  7.0722e-02,\n",
       "                       5.8458e-02,  4.4139e-02,  1.4703e-02, -6.3297e-02,  3.0956e-02,\n",
       "                       2.6434e-02,  4.7656e-02,  2.5160e-02], device='cuda:0')),\n",
       "             ('base_ddg.positional_encoding.pe',\n",
       "              tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "                         0.0000e+00,  1.0000e+00],\n",
       "                       [ 8.4147e-01,  5.4030e-01,  7.6172e-01,  ...,  1.0000e+00,\n",
       "                         1.1548e-04,  1.0000e+00],\n",
       "                       [ 9.0930e-01, -4.1615e-01,  9.8705e-01,  ...,  1.0000e+00,\n",
       "                         2.3096e-04,  1.0000e+00],\n",
       "                       ...,\n",
       "                       [ 6.0880e-01, -7.9332e-01, -1.8621e-01,  ...,  8.8092e-01,\n",
       "                         4.1407e-01,  9.1024e-01],\n",
       "                       [-3.3862e-01, -9.4092e-01, -8.6904e-01,  ...,  8.8085e-01,\n",
       "                         4.1418e-01,  9.1020e-01],\n",
       "                       [-9.7472e-01, -2.2345e-01, -9.3991e-01,  ...,  8.8079e-01,\n",
       "                         4.1428e-01,  9.1015e-01]]], device='cuda:0')),\n",
       "             ('base_ddg.multihead_attention.in_proj_weight',\n",
       "              tensor([[-0.0035,  0.0327, -0.0793,  ..., -0.0549, -0.0250,  0.0412],\n",
       "                      [-0.0443,  0.0742,  0.0707,  ...,  0.1058, -0.0446,  0.1709],\n",
       "                      [-0.0791, -0.0377, -0.0803,  ...,  0.0513, -0.0885,  0.0528],\n",
       "                      ...,\n",
       "                      [-0.0474, -0.0226, -0.0733,  ..., -0.0393, -0.0927,  0.0757],\n",
       "                      [-0.0670, -0.0949, -0.1525,  ...,  0.0020, -0.0129, -0.0244],\n",
       "                      [ 0.0631, -0.0220, -0.0563,  ..., -0.0546, -0.0126,  0.0624]],\n",
       "                     device='cuda:0')),\n",
       "             ('base_ddg.multihead_attention.in_proj_bias',\n",
       "              tensor([-1.7555e-02,  1.0436e-01,  6.3444e-02,  2.9189e-02,  2.6097e-02,\n",
       "                       3.4231e-02,  5.4131e-02,  4.6281e-02, -1.1091e-02, -8.0164e-03,\n",
       "                      -4.4199e-02, -1.0333e-01, -5.4211e-02, -6.6455e-03,  1.3034e-02,\n",
       "                       1.8812e-02, -2.1982e-02, -3.2186e-02, -4.4204e-02,  4.5152e-02,\n",
       "                       7.5140e-02, -6.5517e-02, -8.1796e-03,  7.8421e-02,  1.2065e-02,\n",
       "                      -3.7885e-02, -3.8348e-02,  4.6354e-02, -5.3826e-02, -4.4386e-02,\n",
       "                      -1.4655e-02, -4.8046e-02,  1.7004e-02, -2.8095e-02,  1.8270e-02,\n",
       "                       1.8603e-02,  2.5686e-02, -4.8124e-02,  1.8351e-02,  9.3545e-03,\n",
       "                       3.0998e-03, -1.1192e-02,  1.4616e-02, -2.3756e-02,  8.7132e-03,\n",
       "                      -1.0479e-03, -1.4957e-03,  4.7675e-02,  2.2139e-02,  4.8034e-02,\n",
       "                      -2.3073e-02, -3.2256e-02, -2.9095e-02, -1.0575e-02,  4.9697e-03,\n",
       "                      -3.8408e-03, -1.1159e-02,  2.4050e-02,  6.4628e-04, -2.2109e-02,\n",
       "                      -2.2165e-02, -2.2003e-02,  5.5424e-02,  1.2345e-02, -1.6772e-03,\n",
       "                       2.0426e-02, -2.8659e-03,  1.4396e-02,  2.8205e-02, -1.9695e-02,\n",
       "                       3.3554e-02,  2.9444e-02, -2.0283e-02,  2.6788e-02,  3.9357e-02,\n",
       "                      -1.1642e-02, -2.1382e-02,  2.3139e-02,  1.2328e-02, -1.8653e-02,\n",
       "                      -3.6153e-02,  4.8695e-02,  2.5389e-02,  5.8106e-03, -5.2015e-02,\n",
       "                       4.2540e-02, -7.0187e-03,  3.2915e-02, -1.5564e-02,  7.8112e-02,\n",
       "                       1.4597e-02, -1.9262e-02,  1.1371e-02, -3.1103e-02,  1.8202e-02,\n",
       "                       3.1534e-02, -5.0361e-03, -9.3932e-02, -2.3190e-02, -7.6267e-03,\n",
       "                      -2.9503e-02, -6.7969e-02,  5.1986e-02, -3.9363e-02, -9.0645e-03,\n",
       "                       8.4398e-02,  2.7160e-02,  5.5936e-04, -2.7362e-02,  1.6433e-02,\n",
       "                      -3.9275e-02, -3.0821e-02,  5.2348e-02,  8.9963e-02, -6.7451e-02,\n",
       "                       2.3095e-02, -5.0902e-02,  1.9578e-02,  3.2103e-03,  4.3145e-02,\n",
       "                       1.6668e-02, -2.4798e-02,  6.1909e-02,  1.2171e-02,  5.9046e-02,\n",
       "                       9.5416e-02,  5.9113e-02,  2.1324e-02, -4.3411e-03, -9.4622e-03,\n",
       "                      -7.3289e-03,  2.6369e-03,  1.4922e-03, -3.7448e-03,  4.7203e-03,\n",
       "                       5.4409e-03,  1.6678e-03, -8.2644e-03, -8.3526e-03, -1.3735e-02,\n",
       "                      -9.8604e-03,  2.8364e-04,  7.0884e-03,  1.9729e-04, -3.2313e-04,\n",
       "                       1.1850e-02,  8.1898e-03, -2.7009e-05, -1.1440e-02,  1.1449e-02,\n",
       "                       1.0890e-02, -6.7604e-03, -3.9962e-03,  2.4617e-03,  1.5369e-04,\n",
       "                      -5.4128e-03, -2.1353e-03,  1.3889e-02, -5.6786e-04,  1.7320e-02,\n",
       "                      -1.4725e-04, -1.5706e-03, -1.4860e-04, -3.7732e-04,  2.3899e-04,\n",
       "                       1.6191e-03, -8.8045e-05,  1.1084e-03,  1.5967e-03, -1.1501e-03,\n",
       "                       2.3844e-03, -5.4419e-04, -7.7608e-04, -1.0315e-03,  1.1932e-03,\n",
       "                      -9.2482e-04, -7.8606e-03, -3.3732e-03, -4.7992e-03,  6.2571e-03,\n",
       "                      -3.3629e-03,  9.9099e-04,  2.8020e-03, -1.2791e-03, -1.8599e-03,\n",
       "                      -1.2119e-02,  3.3029e-03, -2.9461e-03, -1.2532e-04, -2.0989e-03,\n",
       "                      -9.1785e-04, -2.1193e-03,  1.2459e-03,  1.1718e-03,  7.1425e-04,\n",
       "                       3.1237e-05,  4.6639e-04, -2.8475e-03,  1.5425e-04,  7.1614e-04,\n",
       "                       4.2762e-04,  3.0123e-04,  5.5980e-04, -1.9568e-03,  1.5242e-04,\n",
       "                       1.4824e-03, -1.8411e-03, -9.3800e-04,  2.5618e-04, -8.6303e-03,\n",
       "                       5.2952e-04, -4.2975e-03,  9.1118e-03, -7.0854e-03, -3.8000e-03,\n",
       "                      -1.2811e-02,  1.2059e-03, -6.0653e-03,  1.7462e-03,  1.3689e-02,\n",
       "                      -5.9328e-03, -1.0569e-03, -3.3697e-03, -2.1166e-02,  1.4398e-03,\n",
       "                      -2.3380e-04,  1.5352e-03,  1.5500e-03, -5.8005e-04, -2.3370e-04,\n",
       "                      -6.1831e-03,  1.1416e-03, -5.4662e-04,  8.3241e-04,  4.4834e-04,\n",
       "                       2.3317e-04, -1.2319e-03, -1.9945e-04, -1.8431e-03, -1.2364e-03,\n",
       "                       7.0935e-03,  1.3846e-02, -4.5328e-03,  2.3703e-03, -2.3070e-03,\n",
       "                      -6.6211e-03,  2.6585e-03,  3.5438e-03,  1.2827e-02, -2.3683e-03,\n",
       "                       1.2989e-02,  7.8687e-05,  1.6257e-03,  2.0795e-02,  7.7110e-03,\n",
       "                      -8.5587e-04,  1.5858e-02,  8.6113e-03,  1.8247e-03,  2.3191e-03,\n",
       "                       2.2254e-03, -1.5425e-02,  3.0321e-03, -1.1192e-02,  6.7831e-03,\n",
       "                       2.2642e-02, -6.3654e-03, -3.6468e-03,  3.1767e-03,  2.4356e-02,\n",
       "                       1.5021e-02, -7.7592e-04,  4.3112e-02,  8.9368e-03,  1.3534e-02,\n",
       "                      -1.8539e-02,  1.7238e-02, -3.3781e-02, -5.4249e-03,  2.4256e-02,\n",
       "                       3.2874e-02,  3.2839e-02, -4.6086e-03,  1.2725e-02,  1.2289e-02,\n",
       "                       1.0161e-02, -1.9286e-02, -1.9752e-02, -2.3533e-03, -4.0260e-02,\n",
       "                      -6.2528e-03,  1.8410e-02,  1.5502e-03,  7.4002e-04,  1.6039e-02,\n",
       "                       1.5939e-02,  1.7413e-02,  1.6934e-02, -1.8110e-02,  1.5668e-02,\n",
       "                      -3.1743e-03, -3.1925e-03,  4.2510e-03, -1.4756e-03, -3.2466e-02,\n",
       "                       1.4364e-04, -1.8031e-02, -2.0224e-02, -2.2507e-02, -1.0303e-02,\n",
       "                       3.2279e-02, -2.8687e-03,  1.0904e-02, -1.2984e-03,  1.0839e-02,\n",
       "                       2.0880e-03, -1.3682e-02,  2.7711e-02, -3.7434e-02, -1.4028e-02,\n",
       "                      -1.5367e-02, -1.3128e-02, -8.7456e-03, -1.0112e-02, -4.4934e-03,\n",
       "                       1.6531e-03,  1.3893e-02, -6.0785e-03, -5.7853e-04,  6.8261e-04,\n",
       "                      -6.0095e-03,  7.0533e-03,  2.6785e-03,  1.7564e-02, -3.8463e-03,\n",
       "                       3.1542e-04,  6.6907e-03,  1.2202e-02, -8.4351e-03,  4.7456e-02,\n",
       "                       2.4656e-02, -2.6254e-03,  2.1491e-02,  1.4865e-02, -1.6352e-03,\n",
       "                      -1.7365e-02,  2.1677e-02, -1.3499e-02, -5.7776e-03,  1.3783e-02,\n",
       "                      -1.0856e-02, -6.3122e-03, -6.8231e-03, -1.2843e-02, -4.5267e-02,\n",
       "                       3.0288e-04,  1.3770e-02, -6.9100e-03,  2.3324e-02, -9.7434e-03,\n",
       "                      -1.0853e-02, -1.0813e-02, -1.9377e-03,  1.7578e-02,  8.9336e-03,\n",
       "                      -1.1768e-02, -1.5228e-02,  1.7285e-02,  7.6342e-04, -7.1924e-05,\n",
       "                       1.5858e-02,  5.9444e-04, -1.0337e-02,  4.6639e-03,  4.0182e-03,\n",
       "                       2.1482e-02,  4.6369e-03, -1.1122e-02, -1.2068e-03,  8.3168e-03,\n",
       "                       1.7806e-02,  2.6651e-03, -2.8493e-03,  2.3819e-02], device='cuda:0')),\n",
       "             ('base_ddg.multihead_attention.out_proj.weight',\n",
       "              tensor([[ 0.0238, -0.0228,  0.0495,  ..., -0.0680,  0.0381, -0.0198],\n",
       "                      [-0.0324, -0.0372, -0.0896,  ...,  0.0209,  0.0578,  0.0368],\n",
       "                      [-0.0434,  0.0377, -0.0967,  ...,  0.0020,  0.0461, -0.0083],\n",
       "                      ...,\n",
       "                      [ 0.0108,  0.0365,  0.0547,  ...,  0.0242, -0.0261,  0.0344],\n",
       "                      [-0.0022,  0.0495, -0.0702,  ...,  0.0302,  0.0742, -0.0112],\n",
       "                      [-0.0136, -0.0781,  0.0614,  ..., -0.0213, -0.0960, -0.0139]],\n",
       "                     device='cuda:0')),\n",
       "             ('base_ddg.multihead_attention.out_proj.bias',\n",
       "              tensor([ 0.0630,  0.0235,  0.0045,  0.0002,  0.0550,  0.1337, -0.0052,  0.0567,\n",
       "                       0.0555,  0.0432,  0.0131, -0.0554, -0.0146,  0.0005, -0.0522,  0.0458,\n",
       "                       0.0845, -0.0079,  0.0567,  0.0140, -0.0362, -0.0007, -0.0774, -0.0191,\n",
       "                      -0.0317,  0.0019, -0.0100, -0.1036,  0.0579, -0.0357,  0.0200,  0.0369,\n",
       "                      -0.0224,  0.0007,  0.0185, -0.0298, -0.0153,  0.0608, -0.0349,  0.0640,\n",
       "                      -0.0186, -0.0042, -0.0500, -0.0954, -0.0272, -0.0280, -0.0051, -0.0130,\n",
       "                       0.0139, -0.0279,  0.0030,  0.0291,  0.0049, -0.0216,  0.0245, -0.0233,\n",
       "                       0.0220,  0.0113,  0.0862,  0.0777,  0.0584,  0.0027,  0.0047,  0.0625,\n",
       "                       0.0259,  0.0141, -0.0016,  0.0243,  0.0189,  0.0341, -0.0194, -0.0011,\n",
       "                       0.0634,  0.0080,  0.0188,  0.0559,  0.0452,  0.0426,  0.0018,  0.0256,\n",
       "                      -0.0195, -0.0147,  0.0463,  0.0140,  0.0087,  0.0082, -0.0029,  0.0510,\n",
       "                       0.0306,  0.0174, -0.0133,  0.0042,  0.0108,  0.0054, -0.0061, -0.0033,\n",
       "                       0.0249,  0.0078,  0.0629, -0.0216,  0.0311,  0.0051, -0.0112, -0.0365,\n",
       "                       0.0115,  0.0034,  0.0762,  0.0307,  0.0521, -0.0238, -0.0120,  0.0037,\n",
       "                      -0.0532, -0.0563,  0.0812,  0.0048, -0.0159,  0.0164, -0.0136, -0.0403,\n",
       "                       0.0778, -0.0020, -0.0269, -0.0186, -0.0187, -0.0193,  0.0244, -0.0056],\n",
       "                     device='cuda:0')),\n",
       "             ('base_ddg.inverse_attention.in_proj_weight',\n",
       "              tensor([[-2.9315e-02,  4.2178e-02, -1.1111e-02,  ..., -1.6320e-01,\n",
       "                        5.0181e-02, -1.5335e-01],\n",
       "                      [ 7.3048e-02,  8.4091e-02,  7.4932e-02,  ..., -5.9435e-02,\n",
       "                       -8.6623e-02, -9.1404e-02],\n",
       "                      [-6.3214e-03, -7.2135e-02,  8.7648e-03,  ...,  2.4295e-02,\n",
       "                        6.5321e-02, -1.1069e-01],\n",
       "                      ...,\n",
       "                      [-3.5809e-02, -1.7029e-02,  9.8521e-02,  ...,  2.6946e-02,\n",
       "                       -4.2819e-02, -7.2575e-02],\n",
       "                      [ 6.4082e-02, -4.9780e-03, -9.9309e-03,  ..., -7.0082e-02,\n",
       "                       -2.5741e-02, -5.0041e-05],\n",
       "                      [ 1.3535e-02, -5.0412e-02,  3.1092e-02,  ..., -1.4005e-01,\n",
       "                        1.0324e-02, -1.2345e-01]], device='cuda:0')),\n",
       "             ('base_ddg.inverse_attention.in_proj_bias',\n",
       "              tensor([-1.2383e-01, -4.8009e-02, -8.1314e-02,  6.5995e-02,  6.2061e-02,\n",
       "                       8.7265e-02, -9.5497e-02, -8.4765e-02, -7.2945e-02,  9.0638e-02,\n",
       "                       7.2982e-02,  3.5422e-02,  1.7749e-02, -8.1362e-02, -5.4017e-02,\n",
       "                      -2.7794e-02,  3.4563e-02, -4.2414e-03,  4.0521e-03,  3.0933e-02,\n",
       "                      -4.8101e-02, -7.4146e-03, -7.2319e-02,  8.6935e-03, -3.5165e-03,\n",
       "                       3.9226e-02, -2.6355e-02, -3.3197e-02, -8.2091e-03, -7.1089e-03,\n",
       "                       2.4939e-02,  2.9375e-02,  2.1330e-02,  5.0316e-03,  1.1967e-02,\n",
       "                      -2.3193e-03,  8.9688e-04,  4.9995e-03, -1.1424e-02,  8.1861e-03,\n",
       "                      -1.6875e-02,  6.5658e-03, -2.7092e-02, -2.4326e-02,  1.8619e-02,\n",
       "                       7.0752e-03,  4.8432e-03,  3.0153e-02, -2.9619e-02,  1.0062e-01,\n",
       "                      -2.6226e-02,  9.2663e-02,  5.1078e-02,  7.9269e-02,  1.2987e-01,\n",
       "                      -7.8633e-02, -7.9234e-02, -3.7188e-02,  9.7387e-02, -8.7964e-02,\n",
       "                       5.8120e-02,  3.9179e-02, -1.2833e-01,  6.2275e-02,  2.5647e-02,\n",
       "                      -2.6877e-04,  2.2257e-02,  3.1679e-02, -2.6077e-02, -4.0915e-02,\n",
       "                       3.5419e-02,  5.9744e-02,  9.9418e-03,  2.0363e-03,  1.3873e-02,\n",
       "                       1.8355e-03,  4.4030e-03,  4.8648e-02, -2.2162e-02, -9.3135e-03,\n",
       "                       1.1610e-02,  3.1657e-02, -1.2353e-02,  2.8530e-03,  3.4818e-02,\n",
       "                      -7.6543e-04,  3.7092e-02,  2.9650e-02, -1.6476e-02,  2.2478e-02,\n",
       "                       6.7786e-02,  1.0958e-02, -1.4868e-03, -3.8490e-02,  4.8239e-02,\n",
       "                       5.5544e-02,  1.2797e-01, -9.7695e-02,  6.4587e-03, -1.2274e-01,\n",
       "                      -5.1023e-02,  6.9257e-02,  5.9149e-02,  1.0486e-01,  7.5942e-02,\n",
       "                       1.3288e-01, -1.3601e-01, -9.5134e-02, -1.2940e-01, -5.1281e-02,\n",
       "                       1.2340e-01, -1.4606e-02, -2.1184e-02, -1.7669e-02,  4.1392e-02,\n",
       "                      -4.3742e-02, -4.8787e-02, -2.8493e-02,  1.6376e-02,  4.9177e-02,\n",
       "                       2.9583e-02,  1.0620e-02, -2.0953e-02, -1.6540e-02,  4.9255e-02,\n",
       "                       2.5871e-03,  7.7945e-02, -6.1874e-03, -1.0391e-04, -1.7144e-05,\n",
       "                      -1.5711e-04,  4.9487e-05, -1.8601e-05, -1.2236e-05, -1.7765e-04,\n",
       "                      -1.5924e-04,  3.6386e-05,  1.0528e-05, -6.7126e-05,  6.0284e-05,\n",
       "                       1.5275e-04,  2.4535e-05, -2.5642e-04,  7.5221e-05, -1.2138e-04,\n",
       "                      -4.8495e-05, -9.3736e-05, -1.1071e-04,  5.6519e-05, -1.1216e-04,\n",
       "                       1.1579e-04,  1.4385e-04,  1.3263e-04, -2.6196e-04,  1.0063e-04,\n",
       "                      -6.7215e-05, -3.4807e-05,  2.5687e-04, -1.1845e-04, -3.0019e-04,\n",
       "                      -2.5036e-05, -4.6395e-05, -6.7971e-05,  1.1663e-05, -4.0640e-05,\n",
       "                      -2.1666e-04,  2.0499e-04,  4.7931e-05,  8.6961e-05, -9.4429e-05,\n",
       "                       4.5087e-05, -5.6462e-05,  1.5361e-04,  6.5122e-05,  2.0676e-05,\n",
       "                       8.0788e-05, -1.6786e-04,  1.9552e-04,  1.7979e-04,  2.5417e-04,\n",
       "                       4.3919e-05,  3.6585e-05,  3.4519e-04, -2.8738e-04, -3.1290e-04,\n",
       "                      -1.8620e-04,  1.7284e-04, -6.0150e-05,  5.0651e-05, -2.2001e-04,\n",
       "                      -2.1459e-04,  9.6780e-05, -3.1438e-04,  4.0454e-05, -1.0898e-04,\n",
       "                      -1.7339e-04, -2.5165e-05, -4.9294e-06,  6.0987e-05,  1.1248e-04,\n",
       "                      -1.2502e-04,  2.0865e-04,  2.3059e-05, -7.2170e-06, -5.1581e-05,\n",
       "                      -1.3959e-04,  8.8131e-05,  2.0178e-04,  1.2390e-04, -1.3860e-05,\n",
       "                       7.5254e-05,  4.5613e-05,  7.1759e-05,  4.6230e-05, -2.6153e-05,\n",
       "                      -8.1554e-05, -4.9814e-05,  1.2023e-04,  3.4229e-05,  5.1605e-05,\n",
       "                       1.2506e-05, -7.1161e-05,  1.3551e-04,  5.9632e-05, -5.8568e-05,\n",
       "                       1.2193e-04,  3.1432e-04, -2.5947e-04,  1.6596e-04,  2.4017e-04,\n",
       "                      -1.4522e-04,  1.6219e-04,  6.5370e-05, -8.4494e-05, -2.3432e-04,\n",
       "                       1.3597e-04, -8.4628e-05,  7.0993e-05,  2.3523e-04, -4.2128e-05,\n",
       "                      -1.7937e-04,  5.6147e-05,  1.0680e-05,  4.6318e-05, -8.2916e-05,\n",
       "                       9.2523e-05, -1.2125e-05, -4.9458e-05,  4.3372e-05, -2.8508e-05,\n",
       "                      -1.6480e-04, -1.7886e-05, -2.8327e-05,  1.3830e-04, -5.8106e-05,\n",
       "                      -4.5578e-05, -8.0920e-02,  2.0140e-02,  1.1431e-02, -1.4284e-02,\n",
       "                       6.3318e-02, -3.6522e-02, -6.7402e-02, -6.2555e-02,  8.1830e-02,\n",
       "                      -2.7016e-02,  6.0508e-02,  3.7347e-02,  4.5932e-02, -1.6291e-02,\n",
       "                       3.2867e-02,  4.6496e-02,  9.7213e-03,  5.8407e-03,  8.2955e-02,\n",
       "                       8.2383e-02,  3.9971e-02, -6.8886e-02, -4.4683e-02,  7.5002e-02,\n",
       "                      -4.0619e-02, -4.9386e-02, -3.3860e-02,  1.1690e-03,  7.5074e-04,\n",
       "                      -1.7656e-02,  9.0890e-03, -4.0650e-02, -1.7505e-02, -1.3088e-02,\n",
       "                      -3.3937e-02,  4.1310e-02,  1.7138e-02,  7.8286e-02, -4.2237e-04,\n",
       "                       2.0547e-02,  3.0402e-02,  4.2466e-03,  1.0113e-02, -7.4450e-02,\n",
       "                       1.5550e-02,  7.8063e-03,  1.1186e-02,  5.7060e-02,  3.9133e-02,\n",
       "                       1.7954e-02, -3.1705e-02,  1.2565e-01,  6.1624e-02, -7.9967e-02,\n",
       "                       1.0000e-03, -9.8605e-06,  1.0748e-01, -5.9972e-03, -1.0814e-02,\n",
       "                       7.2431e-02,  4.1929e-02, -9.5051e-02,  1.7467e-02,  1.9134e-03,\n",
       "                       5.5960e-02,  1.2619e-02,  7.6835e-02,  6.5098e-03, -4.9316e-02,\n",
       "                       8.3457e-02,  1.8438e-02,  2.2253e-02, -8.4421e-02,  3.0209e-02,\n",
       "                       6.0841e-02,  4.1852e-02, -5.1260e-02, -4.1166e-03,  3.7739e-02,\n",
       "                      -6.8648e-03, -2.6610e-02,  1.0435e-02, -2.7163e-02, -3.0477e-02,\n",
       "                      -5.4001e-02,  3.0145e-02, -6.5294e-02, -3.3144e-02,  9.4191e-03,\n",
       "                      -2.7321e-02, -8.0410e-02, -1.4544e-02,  1.0075e-01, -5.6855e-02,\n",
       "                      -1.5891e-02, -1.2215e-02,  1.1812e-01,  7.3974e-02, -6.7096e-02,\n",
       "                      -5.6008e-02,  5.6921e-02, -3.5538e-02,  1.6784e-02, -2.7140e-02,\n",
       "                      -6.3408e-02, -1.0461e-01, -7.6236e-03, -3.2536e-02,  3.9125e-02,\n",
       "                      -1.3720e-02,  1.5044e-02,  2.6719e-02, -4.7193e-02, -7.7198e-02,\n",
       "                      -3.1181e-03, -2.6065e-02, -3.1581e-02, -1.0269e-02,  7.8921e-02,\n",
       "                      -6.8807e-02,  4.5386e-02, -3.3940e-02,  7.3353e-02, -1.9123e-02,\n",
       "                       2.0441e-02, -3.7730e-03,  2.8048e-02, -6.5013e-02], device='cuda:0')),\n",
       "             ('base_ddg.inverse_attention.out_proj.weight',\n",
       "              tensor([[-0.0346,  0.0550,  0.0388,  ..., -0.0149, -0.0377, -0.0405],\n",
       "                      [ 0.0559, -0.0687,  0.0419,  ..., -0.0071, -0.0055,  0.0675],\n",
       "                      [ 0.0061, -0.0246, -0.0136,  ..., -0.0916, -0.0627,  0.0318],\n",
       "                      ...,\n",
       "                      [-0.0610, -0.0369, -0.0668,  ...,  0.0064,  0.0417, -0.0152],\n",
       "                      [ 0.0452,  0.0251, -0.0163,  ..., -0.0358,  0.0723,  0.0381],\n",
       "                      [ 0.0173, -0.0197,  0.0310,  ..., -0.0540, -0.0609, -0.0488]],\n",
       "                     device='cuda:0')),\n",
       "             ('base_ddg.inverse_attention.out_proj.bias',\n",
       "              tensor([ 0.0050, -0.0248, -0.0153,  0.0072, -0.0032,  0.0238, -0.0155,  0.0172,\n",
       "                      -0.0073, -0.0507, -0.0056, -0.0007, -0.0040,  0.0052, -0.0110, -0.0112,\n",
       "                      -0.0082,  0.0031, -0.0213,  0.0012,  0.0395,  0.0279,  0.0056,  0.0014,\n",
       "                      -0.0218, -0.0376, -0.0256,  0.0100,  0.0035,  0.0417,  0.0120,  0.0106,\n",
       "                       0.0019,  0.0234, -0.0300,  0.0172, -0.0026, -0.0115, -0.0117,  0.0282,\n",
       "                       0.0145, -0.0243, -0.0235,  0.0203, -0.0181, -0.0181, -0.0026, -0.0145,\n",
       "                      -0.0065,  0.0103,  0.0337, -0.0217,  0.0033, -0.0246,  0.0083, -0.0041,\n",
       "                      -0.0221,  0.0215,  0.0235, -0.0239, -0.0021, -0.0151,  0.0019,  0.0092,\n",
       "                      -0.0263, -0.0111, -0.0347, -0.0079, -0.0211,  0.0084, -0.0184, -0.0050,\n",
       "                       0.0310,  0.0109, -0.0239, -0.0085,  0.0067, -0.0121,  0.0085, -0.0100,\n",
       "                       0.0073,  0.0054, -0.0022, -0.0187,  0.0163, -0.0149, -0.0200,  0.0016,\n",
       "                      -0.3389,  0.0066,  0.1153, -0.0109, -0.0402, -0.0029,  0.0570, -0.0362,\n",
       "                       0.0407,  0.0227,  0.0132,  0.1684, -0.0270, -0.0329, -0.0188,  0.0386,\n",
       "                      -0.0038, -0.0051,  0.0226, -0.0294, -0.0168, -0.0098,  0.0270, -0.0100,\n",
       "                       0.0076, -0.0107, -0.0160,  0.0038,  0.0004, -0.0071,  0.0140,  0.2215,\n",
       "                       0.0194,  0.0032, -0.0077, -0.0449, -0.0111, -0.0180,  0.0239,  0.0049],\n",
       "                     device='cuda:0')),\n",
       "             ('base_ddg.norm3.weight',\n",
       "              tensor([ 9.9621e-01,  9.7906e-01,  8.6511e-01,  9.0724e-01,  9.8770e-01,\n",
       "                       9.7427e-01,  9.8415e-01,  9.6709e-01,  9.6066e-01,  9.8136e-01,\n",
       "                       9.6901e-01,  9.2857e-01,  9.8402e-01,  9.3745e-01,  9.8915e-01,\n",
       "                       8.4534e-01,  9.8163e-01,  9.8771e-01,  9.6320e-01,  9.6524e-01,\n",
       "                       9.8703e-01,  9.9497e-01,  9.9537e-01,  9.7115e-01,  9.9197e-01,\n",
       "                       1.0045e+00,  9.8530e-01,  9.7873e-01,  8.5806e-01,  9.8756e-01,\n",
       "                       9.5107e-01,  9.7706e-01,  9.8732e-01,  9.3615e-01,  9.7264e-01,\n",
       "                       7.5355e-01,  9.8539e-01,  5.1428e-01,  9.9525e-01,  9.1737e-01,\n",
       "                       9.8034e-01,  9.7547e-01,  9.8001e-01,  8.4339e-01,  7.5405e-01,\n",
       "                       8.9935e-01,  9.6766e-01,  9.8350e-01,  9.9627e-01,  1.0190e+00,\n",
       "                       9.6684e-01,  1.0094e+00,  9.5533e-01,  9.1113e-01,  7.3793e-01,\n",
       "                       9.9951e-01,  9.8328e-01,  7.8841e-01,  5.6078e-01,  8.8663e-01,\n",
       "                       9.9124e-01,  1.0091e+00,  9.3622e-01,  9.0678e-01,  9.4971e-01,\n",
       "                       9.7747e-01,  9.5192e-01,  1.0072e+00,  9.3083e-01,  7.4812e-01,\n",
       "                       7.7683e-01,  1.0242e+00,  9.0352e-01,  9.8079e-01,  7.3044e-01,\n",
       "                       8.1805e-01,  9.4821e-01,  9.4757e-01,  9.6922e-01,  9.5934e-01,\n",
       "                       9.3794e-01,  9.7077e-01,  9.4515e-01,  9.1996e-01,  9.2540e-01,\n",
       "                       9.4901e-01,  9.1736e-01,  9.0599e-01,  9.5339e-01,  9.1221e-01,\n",
       "                       9.8768e-01,  9.8014e-01,  9.5544e-01,  9.6756e-01,  1.0056e+00,\n",
       "                       9.7592e-01,  9.3174e-01,  1.0070e+00,  9.6423e-01,  1.0000e+00,\n",
       "                       9.7171e-01,  9.2501e-01,  1.0048e+00,  9.7883e-01,  7.4753e-01,\n",
       "                       9.7903e-01,  9.3979e-01,  9.6962e-01,  5.8281e-01,  9.8235e-01,\n",
       "                       9.9682e-01,  9.4648e-01,  1.0444e+00,  1.0160e+00,  7.8203e-01,\n",
       "                       9.2992e-01,  9.9573e-01,  9.6528e-01,  1.0189e+00,  9.4988e-01,\n",
       "                       9.3652e-01,  9.6885e-01,  1.0254e+00,  9.7852e-01,  1.0027e+00,\n",
       "                       9.8744e-01,  9.1109e-01,  9.7929e-01,  3.9745e-01,  2.5514e-03,\n",
       "                       2.0426e-01,  4.5955e-01,  6.3941e-01,  5.6655e-01,  3.4698e-01,\n",
       "                       5.7386e-01,  4.4603e-01,  4.1178e-01,  1.8117e-01,  9.0813e-01,\n",
       "                       8.1809e-02,  4.5541e-01,  6.6422e-01,  4.9597e-01,  6.4520e-02,\n",
       "                       7.2199e-02,  1.3369e-01,  8.0348e-01,  9.3107e-01,  5.8003e-02,\n",
       "                       3.6443e-01,  1.5343e-04,  4.8231e-01,  1.2029e-01,  4.7054e-01,\n",
       "                       4.9468e-01,  1.8477e-01,  2.5197e-01,  5.1385e-01,  4.5643e-01,\n",
       "                       7.5297e-01,  3.6198e-01,  2.7079e-01,  5.9282e-01,  6.1051e-01,\n",
       "                       1.0060e-01,  6.3514e-01,  9.5097e-01,  3.0751e-01,  3.1530e-01,\n",
       "                       2.6535e-01,  2.3558e-01,  3.3675e-01, -4.6146e-05,  4.1931e-01,\n",
       "                       2.6353e-01,  1.8759e-01,  3.0747e-01,  1.7538e-01,  2.9451e-01,\n",
       "                       4.9637e-01,  5.7971e-01,  4.8848e-01,  2.5866e-01,  6.9507e-01,\n",
       "                       2.8659e-01,  2.5797e-01,  2.0762e-01,  5.6333e-01,  7.0609e-02,\n",
       "                       2.8458e-01, -1.8493e-05, -5.5664e-06,  1.3114e-01,  2.4793e-01,\n",
       "                       1.9990e-01,  6.6647e-04,  2.9983e-01,  5.2909e-01,  2.8770e-01,\n",
       "                       9.3952e-01,  6.8932e-01,  3.7046e-01,  5.0438e-01,  8.5885e-01,\n",
       "                       1.5296e-01,  1.0636e-01,  2.1667e-01,  3.8719e-01,  5.1825e-01,\n",
       "                       3.1007e-01,  7.9487e-01,  7.4905e-01,  7.5853e-01,  1.5211e-01,\n",
       "                       5.6570e-01,  6.7538e-01,  3.6403e-01,  2.3956e-01,  8.1190e-01,\n",
       "                       2.2158e-01,  4.2806e-01,  8.9548e-01,  5.7304e-01,  5.6400e-02,\n",
       "                       4.2311e-01,  5.0501e-01,  5.0762e-01,  5.7098e-05,  3.5407e-01,\n",
       "                       1.2830e-01,  5.6925e-01,  4.0276e-01,  5.1023e-01,  2.0562e-01,\n",
       "                       1.8666e-01,  3.7054e-01,  2.4688e-01,  3.9987e-01,  1.1465e-01,\n",
       "                       2.8653e-01,  6.0060e-01,  4.5947e-02,  2.0394e-01,  1.5699e-01,\n",
       "                       2.0581e-01,  3.2308e-01,  5.8121e-01,  7.1198e-01,  2.7055e-01,\n",
       "                       3.1157e-01,  2.5475e-01,  6.0742e-01,  2.1882e-01,  2.6913e-01,\n",
       "                       6.0950e-01], device='cuda:0')),\n",
       "             ('base_ddg.norm3.bias',\n",
       "              tensor([-1.7629e-02, -8.9850e-03, -1.0407e-01, -4.5481e-02, -9.9307e-02,\n",
       "                      -4.2299e-01, -1.1891e-03, -1.2800e-01, -3.5601e-02, -1.0513e-02,\n",
       "                      -8.1292e-03,  3.1985e-02, -2.6692e-03, -1.5852e-03,  2.0911e-03,\n",
       "                      -2.1976e-02, -1.8047e-01, -1.2726e-03, -1.2915e-01, -4.8926e-03,\n",
       "                      -6.9616e-02, -2.2274e-03, -1.2225e-02, -2.4313e-03,  3.3289e-03,\n",
       "                      -2.9863e-03,  4.4731e-04,  4.6581e-02, -1.1559e-01, -1.6851e-02,\n",
       "                      -4.2945e-03, -8.9162e-04,  4.1416e-03,  5.5631e-03, -1.0075e-02,\n",
       "                      -7.3592e-03, -2.9100e-03, -1.3246e-01,  2.1049e-03, -1.4891e-01,\n",
       "                      -7.9262e-03, -4.5428e-03,  6.0345e-03,  3.8657e-02, -1.8423e-01,\n",
       "                      -2.8814e-03, -1.0125e-02,  6.8446e-04, -2.3720e-02, -1.0580e-02,\n",
       "                      -4.6146e-02, -2.3566e-02, -3.1261e-02, -2.5341e-02, -5.9517e-02,\n",
       "                      -2.9036e-02, -1.1042e-02, -3.6681e-02, -1.2355e-01, -2.9852e-01,\n",
       "                      -9.8310e-02, -1.6570e-02, -3.0171e-02, -9.8443e-02, -1.3428e-01,\n",
       "                      -1.9062e-02, -4.5053e-02, -3.3721e-02, -8.5433e-02, -8.8503e-02,\n",
       "                       2.5429e-03, -9.9608e-02, -1.6160e-01, -4.7730e-02, -7.3973e-02,\n",
       "                      -3.0714e-01, -1.1394e-01, -3.6534e-01,  2.9231e-03, -4.9083e-02,\n",
       "                       2.2807e-04, -5.3638e-02, -1.0541e-01, -7.9174e-02, -9.4450e-03,\n",
       "                      -3.5000e-02,  7.7368e-03, -6.2127e-02, -2.8806e-02, -3.8608e-01,\n",
       "                       5.3997e-04, -1.1681e-01,  6.1669e-04, -1.7046e-01,  6.4878e-03,\n",
       "                      -5.6846e-02, -3.7824e-02, -2.7862e-01, -2.1778e-02, -4.0321e-01,\n",
       "                      -2.4574e-02, -3.6787e-02,  5.3592e-03, -4.9313e-01,  7.8621e-02,\n",
       "                      -7.1162e-01, -8.7815e-02, -4.6475e-01, -1.7872e-01, -7.8990e-02,\n",
       "                       1.1287e-03, -4.6079e-02,  6.0424e-03, -3.5381e-02, -7.6998e-02,\n",
       "                      -4.5525e-02,  3.3270e-03, -1.1729e-01,  1.6569e-05, -1.5988e-01,\n",
       "                      -1.0931e-01, -1.2340e-01,  5.5046e-03, -1.1859e-01,  1.0530e-03,\n",
       "                      -1.9112e-01, -4.7861e-02, -1.8010e-02, -1.3796e-01, -1.0371e-03,\n",
       "                      -1.1359e-01, -1.0743e-01, -1.5036e-01, -1.1365e-01, -1.2248e-01,\n",
       "                       1.0350e-01, -1.1660e-01, -1.3216e-01, -9.9020e-02, -2.0450e-01,\n",
       "                      -3.0839e-02, -1.4084e-01, -1.5322e-01, -8.6762e-02, -1.7210e-02,\n",
       "                      -1.0291e-01, -3.7206e-02, -2.4725e-01, -5.9910e-02, -2.0891e-02,\n",
       "                      -6.7504e-02,  1.6506e-04, -1.1187e-01, -2.5878e-02, -1.2881e-01,\n",
       "                      -5.3018e-02, -4.4689e-02, -6.8772e-02, -1.3976e-01, -1.7896e-01,\n",
       "                      -1.1764e-01, -7.4246e-02, -9.3631e-02, -1.4399e-01, -1.3180e-01,\n",
       "                      -3.2371e-02, -1.8108e-01,  2.4420e-02, -9.5331e-02, -5.9890e-02,\n",
       "                      -6.4262e-02, -5.8598e-02, -9.2782e-02,  1.2822e-05, -8.4472e-02,\n",
       "                      -6.4481e-02, -5.9570e-02, -1.4857e-01, -3.0366e-01, -1.0320e-01,\n",
       "                      -1.1103e-01, -1.8110e-01, -1.1351e-01, -1.1364e-01, -1.2190e-01,\n",
       "                      -1.3230e-01, -1.4158e-01, -9.7510e-02, -1.3358e-01, -3.0455e-02,\n",
       "                      -1.2425e-01,  6.4995e-06, -3.9936e-07, -7.6729e-02, -1.1678e-01,\n",
       "                      -4.4214e-02, -8.3269e-04, -5.9210e-02, -1.0853e-01, -1.0213e-01,\n",
       "                      -3.1904e-02, -1.3111e-01, -3.5538e-02, -1.2910e-01, -2.2519e-01,\n",
       "                      -6.5010e-02, -7.8518e-02, -4.6306e-02, -6.6775e-02, -1.2739e-01,\n",
       "                      -8.8701e-02, -1.7672e-01, -1.9208e-01, -8.7600e-02, -5.6011e-02,\n",
       "                      -1.4453e-01,  3.0775e-01, -1.1308e-01, -8.7394e-01, -1.9077e-01,\n",
       "                       2.7489e-01, -8.6541e-02,  1.8176e-01, -1.2464e-01, -1.8148e-02,\n",
       "                      -1.5213e-01, -1.7350e-01, -6.5429e-01,  1.0515e-05, -6.5708e-02,\n",
       "                      -7.7620e-03, -1.0098e+00, -8.6157e-02, -7.8711e-02, -1.6814e-01,\n",
       "                      -4.5151e-02, -9.4073e-02, -9.7867e-02, -1.7784e-01, -5.9561e-02,\n",
       "                      -1.1932e-01, -1.2901e-01, -3.1729e-02, -1.2901e-01, -2.9711e-02,\n",
       "                      -9.0676e-02, -7.1499e-02, -3.5847e-01, -2.1259e-01, -8.0105e-02,\n",
       "                      -8.7473e-02, -5.5436e-02, -1.2887e-01, -8.4709e-02, -3.1391e-01,\n",
       "                      -8.4711e-01], device='cuda:0')),\n",
       "             ('base_ddg.router.weight',\n",
       "              tensor([[ 1.5757e-02, -3.2003e-02,  4.1040e-02, -3.8383e-02, -2.6953e-02,\n",
       "                       -5.8538e-02, -4.6991e-02,  2.5503e-02,  5.1678e-02,  6.0755e-02,\n",
       "                       -5.7396e-02, -5.0906e-02, -5.8063e-02, -1.2513e-02, -5.7051e-02,\n",
       "                        6.9366e-03, -2.9003e-02, -4.1159e-02, -3.1408e-02, -1.0235e-02,\n",
       "                       -1.5125e-02,  5.4708e-02,  8.3787e-03, -3.6070e-02, -1.0056e-02,\n",
       "                        5.1484e-02,  3.2596e-03, -4.7325e-02,  5.3607e-02, -1.3160e-02,\n",
       "                       -4.0908e-02,  1.8070e-02,  4.0980e-02,  5.3643e-04,  2.8884e-02,\n",
       "                       -4.9351e-02,  6.0032e-03, -1.7929e-02, -1.0020e-02, -7.8731e-03,\n",
       "                       -1.9060e-02, -6.0144e-02, -2.5896e-02, -2.5416e-02,  7.3790e-03,\n",
       "                        1.7506e-02, -2.5446e-02, -4.0366e-02, -5.9676e-02,  2.7159e-02,\n",
       "                        2.4071e-02, -6.1344e-02,  6.4020e-04, -2.7290e-02, -2.0257e-02,\n",
       "                        3.0985e-02,  4.2334e-04,  5.2059e-02, -2.6348e-03,  5.1768e-03,\n",
       "                        3.3748e-02, -1.2892e-02,  1.5997e-02, -3.3585e-02,  3.0512e-02,\n",
       "                        5.2253e-02,  2.3701e-02,  1.6045e-02, -1.7927e-02, -9.3426e-03,\n",
       "                        5.2305e-02, -3.0117e-02, -8.9041e-03, -5.7199e-02, -3.3134e-02,\n",
       "                       -4.3945e-02, -4.9593e-02,  1.2398e-02,  3.8587e-02,  9.6348e-03,\n",
       "                       -5.7065e-02,  1.8774e-02, -1.9058e-03, -2.8481e-02, -4.5692e-02,\n",
       "                       -6.5601e-03, -5.4087e-03,  2.1469e-02, -5.7238e-02,  2.7333e-02,\n",
       "                        1.4477e-02, -8.7174e-03, -4.6881e-02,  3.8678e-02, -5.8623e-02,\n",
       "                        1.2014e-02,  4.8833e-02,  1.9191e-02, -1.1845e-02, -3.1695e-02,\n",
       "                        3.3881e-02,  1.3673e-02,  5.4415e-02, -5.1919e-02,  1.5208e-02,\n",
       "                       -3.5226e-02, -2.2705e-02, -1.6376e-02, -2.0109e-02,  3.9208e-03,\n",
       "                       -3.1740e-02, -6.0372e-03, -1.0001e-02, -4.8086e-02, -7.9796e-05,\n",
       "                        2.9346e-02,  5.8186e-03, -7.1485e-04, -3.6325e-02, -5.9496e-02,\n",
       "                       -3.9749e-02, -2.4194e-02, -5.7474e-02,  3.0468e-02, -9.4369e-03,\n",
       "                       -1.3980e-02,  3.5861e-02, -4.1232e-02, -1.1200e-02, -2.1482e-02,\n",
       "                       -3.8741e-02, -6.6977e-03,  5.2684e-02,  4.3682e-02, -4.2530e-02,\n",
       "                        9.4532e-03, -1.1071e-02,  1.1813e-02, -3.3345e-02,  2.6706e-03,\n",
       "                        1.0378e-03,  1.3413e-03,  2.0013e-02, -5.3786e-04, -5.3247e-04,\n",
       "                        2.3350e-03, -5.3019e-02, -4.4220e-02,  4.0278e-02, -3.8383e-02,\n",
       "                       -2.7971e-02, -1.6274e-02, -3.6015e-02,  3.5028e-03,  8.2130e-03,\n",
       "                       -5.1779e-02,  4.5719e-02,  4.1378e-02,  8.3975e-03,  3.4339e-02,\n",
       "                        1.4900e-03,  3.2065e-02, -2.6399e-02, -9.7549e-03, -5.7837e-02,\n",
       "                       -3.0504e-02,  3.9279e-02, -3.0769e-02, -6.0072e-02,  5.0866e-02,\n",
       "                        6.0503e-02, -1.3553e-02, -5.9702e-02, -5.9154e-02,  4.6584e-02,\n",
       "                        4.3471e-02, -7.3663e-03, -3.1609e-02,  1.9313e-02,  2.5407e-02,\n",
       "                       -3.6018e-02,  2.9792e-02,  1.1594e-02, -7.6272e-05, -4.5140e-02,\n",
       "                       -1.1841e-03, -5.8531e-02, -1.3681e-02, -2.4529e-02, -1.3591e-02,\n",
       "                       -2.0360e-02,  3.1048e-02,  1.7376e-02, -3.1463e-02, -4.6603e-02,\n",
       "                       -5.2612e-02,  3.7218e-02, -2.8495e-02, -3.4347e-02,  6.5754e-03,\n",
       "                       -3.6419e-02,  5.6048e-02, -1.8866e-03, -4.8368e-02,  4.3696e-02,\n",
       "                       -4.5549e-02, -5.1257e-02,  1.2102e-03,  3.2033e-02, -6.1945e-02,\n",
       "                        3.1970e-02,  5.6070e-03, -3.5170e-02,  4.2547e-02, -5.1560e-02,\n",
       "                        5.2261e-02, -1.8230e-02, -5.3260e-02,  5.2048e-02,  2.4130e-02,\n",
       "                       -3.0617e-02, -2.8213e-02,  5.2293e-02, -2.3919e-02,  3.3962e-02,\n",
       "                        5.5751e-02,  7.0408e-03,  5.3013e-02,  5.2569e-02,  1.5608e-02,\n",
       "                       -8.4859e-03, -1.7588e-02,  4.1963e-03, -6.0798e-02, -1.3135e-02,\n",
       "                       -2.8249e-02, -2.4164e-02, -3.8905e-03,  5.6996e-02, -2.9323e-02,\n",
       "                       -5.9480e-02,  2.0351e-02,  3.9645e-02,  5.8755e-02,  2.5660e-03,\n",
       "                        4.6170e-02, -2.6897e-02,  2.8177e-02,  2.2739e-02, -3.6674e-02,\n",
       "                        4.0662e-02,  5.6388e-02,  1.2599e-02, -5.1619e-02, -6.3254e-03,\n",
       "                        3.2738e-02]], device='cuda:0')),\n",
       "             ('base_ddg.router.bias', tensor([0.0481], device='cuda:0')),\n",
       "             ('base_ddg.pw_ffnn.0.weight',\n",
       "              tensor([[ 0.0389, -0.0672, -0.0650,  ...,  0.0487,  0.0027,  0.0152],\n",
       "                      [ 0.1270,  0.1166,  0.0605,  ..., -0.0077, -0.0915, -0.0735],\n",
       "                      [-0.0844,  0.0338,  0.0877,  ..., -0.0539, -0.1077,  0.0464],\n",
       "                      ...,\n",
       "                      [-0.0173, -0.0911, -0.0684,  ..., -0.0566, -0.0746, -0.0832],\n",
       "                      [ 0.0893,  0.0513,  0.0216,  ...,  0.0224,  0.0562, -0.0163],\n",
       "                      [ 0.0355, -0.1486, -0.0687,  ...,  0.0569,  0.0756, -0.0551]],\n",
       "                     device='cuda:0')),\n",
       "             ('base_ddg.pw_ffnn.0.bias',\n",
       "              tensor([ 1.6736e-02, -6.2859e-02, -5.6808e-04, -7.9133e-02, -1.0853e-01,\n",
       "                      -3.9862e-02, -8.8592e-02,  3.4413e-02,  1.1713e-02, -3.7979e-02,\n",
       "                      -4.1144e-02, -2.7152e-03, -4.4600e-02, -2.3398e-02, -3.7306e-02,\n",
       "                       3.5154e-02,  6.0787e-02, -4.6077e-02, -9.2332e-02, -2.5037e-02,\n",
       "                      -2.5796e-02, -4.2840e-02, -2.9474e-03, -5.9838e-02, -7.5302e-02,\n",
       "                      -7.5899e-02, -8.7998e-02,  4.3734e-02, -7.8953e-02, -3.0887e-02,\n",
       "                      -9.3614e-02,  3.7893e-02, -9.9569e-02, -8.7922e-02, -7.4137e-02,\n",
       "                      -2.2087e-02, -3.3811e-02, -8.7764e-03, -5.4302e-02, -4.8476e-02,\n",
       "                       8.4977e-03, -3.3229e-02, -1.4657e-02,  1.3910e-02,  6.9681e-03,\n",
       "                      -6.5386e-02, -7.7570e-02,  2.9592e-02, -5.5772e-02, -3.2123e-02,\n",
       "                      -1.4630e-02, -1.7893e-02, -4.5620e-02,  2.0106e-02,  9.6375e-03,\n",
       "                      -1.2481e-02, -3.2548e-02, -4.3494e-02, -5.5323e-02,  3.3247e-02,\n",
       "                      -1.7653e-02,  1.8540e-02,  1.5455e-02, -2.6547e-02, -3.7956e-02,\n",
       "                       2.8107e-02,  5.6646e-03, -6.5704e-02, -2.8029e-02, -2.9001e-02,\n",
       "                       2.5620e-02, -1.0029e-01, -3.4364e-02, -1.0508e-01, -3.2550e-02,\n",
       "                      -8.0277e-02, -5.1032e-02, -3.9558e-02, -8.3786e-02, -9.5929e-02,\n",
       "                       2.7773e-02,  2.6210e-03, -4.5764e-02, -6.6712e-02,  4.2638e-02,\n",
       "                      -9.0340e-02, -6.2446e-02, -1.6750e-02, -7.7202e-03, -2.5819e-02,\n",
       "                      -6.0232e-02, -2.9537e-02,  3.5610e-02, -1.7586e-02, -4.6996e-02,\n",
       "                      -3.7278e-02, -6.8823e-02, -6.4730e-02,  2.8117e-02, -8.2724e-02,\n",
       "                      -1.5965e-02,  2.7414e-02,  3.3973e-02,  2.9838e-02,  1.2673e-02,\n",
       "                      -6.9130e-02,  9.1327e-03, -8.6850e-02, -6.9239e-02,  1.0175e-02,\n",
       "                      -4.3661e-02, -6.6923e-02, -2.5849e-02, -6.0420e-02, -6.2949e-02,\n",
       "                       2.0973e-03, -4.8177e-02, -2.9770e-02, -2.0557e-02,  1.5689e-02,\n",
       "                       4.6099e-03,  5.9117e-02, -3.8916e-02, -2.9903e-02, -3.3980e-02,\n",
       "                      -6.5750e-03, -6.8970e-02,  2.7410e-02, -3.8920e-02, -5.4700e-02,\n",
       "                      -2.0358e-02,  4.0736e-02, -5.1488e-02, -7.8765e-02, -1.7700e-02,\n",
       "                      -8.0006e-02, -3.6140e-02,  7.0507e-03, -1.2991e-02, -3.8512e-02,\n",
       "                       2.4047e-03, -3.5990e-02, -6.8983e-02, -1.4002e-01, -2.7998e-02,\n",
       "                       7.5593e-03, -2.6425e-02,  7.3973e-03,  1.4804e-02, -3.0672e-02,\n",
       "                      -2.3976e-02,  2.8714e-03, -3.5185e-03, -6.3910e-02, -5.1277e-02,\n",
       "                      -3.2016e-02, -7.7329e-02, -6.7100e-02, -6.0674e-02, -7.9815e-02,\n",
       "                      -4.7248e-02, -1.2455e-02, -2.0679e-02, -2.0252e-02, -9.6225e-02,\n",
       "                      -5.0312e-02, -6.6012e-02, -9.4760e-02, -2.2746e-02,  2.4545e-02,\n",
       "                       5.5838e-02, -7.2088e-02, -3.0092e-02, -2.2807e-02, -5.2861e-02,\n",
       "                      -3.2199e-02, -1.0597e-02, -9.1413e-02,  2.5604e-02, -1.0942e-03,\n",
       "                       1.1570e-02,  5.5463e-02, -1.1586e-02, -1.0719e-02, -7.7232e-02,\n",
       "                       5.1190e-04,  6.6475e-03, -8.2014e-02, -1.4517e-02, -5.4063e-02,\n",
       "                      -2.3026e-02, -3.6352e-02, -1.0583e-01, -4.4834e-02,  5.4081e-02,\n",
       "                      -2.9428e-02, -1.9615e-03, -3.7750e-03, -5.4245e-02, -1.0522e-02,\n",
       "                       3.2742e-03, -9.6483e-02, -3.5005e-02,  3.2819e-02, -2.4154e-02,\n",
       "                      -5.6857e-02, -2.6085e-02, -2.7397e-02, -6.3379e-02, -7.0471e-02,\n",
       "                      -1.0553e-01, -1.3062e-02, -3.1557e-02,  1.0857e-02, -5.4978e-02,\n",
       "                      -3.9592e-02, -7.5861e-02, -1.0888e-02, -3.3485e-02, -5.1191e-02,\n",
       "                      -6.4277e-02, -4.1072e-02,  4.4458e-02, -4.0353e-02,  1.4126e-02,\n",
       "                      -6.1476e-02,  3.1505e-02, -6.8805e-02, -4.8354e-02,  4.7080e-02,\n",
       "                      -7.9574e-02, -9.9219e-03, -2.3189e-02, -5.3701e-02, -8.2716e-02,\n",
       "                       2.4107e-02, -1.1554e-02, -7.8274e-03, -1.1603e-01, -8.4184e-02,\n",
       "                      -4.1689e-02,  1.6691e-02, -6.3671e-02, -6.0787e-02, -5.0057e-02,\n",
       "                      -7.7624e-02, -9.3319e-03, -5.0581e-02, -4.4522e-02, -1.1308e-01,\n",
       "                      -3.1166e-02, -8.7984e-03, -3.3614e-02, -5.5943e-02,  1.9160e-02,\n",
       "                      -3.6627e-02,  7.8484e-03, -4.8796e-02, -1.6662e-02, -9.8712e-02,\n",
       "                      -4.3508e-02, -6.2141e-02, -7.7797e-02, -2.5218e-02, -6.1327e-02,\n",
       "                       5.1357e-02, -7.5849e-02,  1.4974e-03,  3.7907e-02,  4.9904e-02,\n",
       "                       1.4950e-02, -3.1373e-02, -4.1624e-03, -2.2297e-02, -5.3205e-02,\n",
       "                      -7.0176e-02, -4.8987e-02,  2.0963e-02, -5.2366e-02, -2.7055e-02,\n",
       "                      -1.1789e-01, -7.7301e-02, -5.0022e-02, -7.1904e-02, -4.9604e-02,\n",
       "                      -1.8616e-02, -2.1063e-02,  1.6523e-02,  2.2316e-02, -1.1650e-02,\n",
       "                      -6.3113e-03, -2.1679e-02, -9.1829e-02, -6.2457e-02, -6.8506e-02,\n",
       "                      -1.1736e-02, -2.9659e-02, -1.4037e-03,  6.3272e-03, -2.5600e-02,\n",
       "                      -7.9321e-02, -9.8442e-03, -5.5381e-02, -3.9395e-02, -7.5863e-02,\n",
       "                      -2.4874e-02, -6.0750e-02, -2.2449e-02, -5.6002e-02,  2.8023e-03,\n",
       "                      -3.4110e-02, -6.8149e-02, -7.7453e-03, -3.9533e-02, -3.6916e-02,\n",
       "                      -6.2107e-02, -1.3711e-02, -7.7484e-02, -7.1174e-02, -3.7591e-03,\n",
       "                      -2.5504e-02,  9.3086e-03, -5.2817e-02, -5.8271e-02, -7.9890e-02,\n",
       "                      -9.6190e-02, -1.5885e-02, -7.1812e-02, -6.3279e-03, -1.8181e-02,\n",
       "                      -1.0460e-01, -2.5736e-02, -7.2668e-02, -1.3863e-02, -2.2422e-02,\n",
       "                      -1.2112e-01, -5.2807e-02, -3.7890e-03, -8.5205e-02, -7.6328e-02,\n",
       "                      -7.1576e-02,  6.4612e-04, -4.0869e-02, -4.9232e-02,  1.3135e-02,\n",
       "                      -6.0092e-02, -3.7792e-02, -2.9435e-02,  1.1078e-02,  3.6806e-02,\n",
       "                      -9.1634e-02,  1.3567e-02, -7.9791e-02, -2.6461e-02, -3.0336e-02,\n",
       "                      -3.5274e-02,  2.3834e-02,  7.3855e-03,  3.5458e-02, -7.5427e-03,\n",
       "                      -8.0773e-02, -3.5739e-02, -6.6387e-02, -7.4444e-02, -5.5774e-02,\n",
       "                      -2.5343e-02, -7.8300e-02, -8.4921e-02, -3.6070e-02, -4.0061e-02,\n",
       "                      -7.3820e-02, -6.1995e-02, -4.7728e-02,  3.4010e-03, -3.2204e-02,\n",
       "                      -9.1583e-02, -3.3539e-02, -7.7659e-05, -7.8552e-02, -1.0749e-01,\n",
       "                      -5.2797e-02, -4.8690e-02, -1.7825e-02, -9.2299e-02, -7.2770e-02,\n",
       "                      -8.7357e-02, -7.2979e-02, -5.0163e-03, -5.0206e-02,  3.5248e-04,\n",
       "                      -1.0014e-01, -2.1327e-02, -3.8694e-03, -1.2386e-01, -1.0312e-01,\n",
       "                      -4.1394e-02,  2.0001e-02,  7.4151e-03, -1.1657e-01,  2.8315e-02,\n",
       "                       1.4640e-02, -4.0561e-02, -5.3574e-02, -1.4854e-02, -7.3813e-02,\n",
       "                      -5.3809e-02, -6.0544e-02,  2.2250e-02, -5.1244e-02, -8.4716e-02,\n",
       "                       3.3979e-02, -5.1058e-02, -4.9365e-02,  7.5726e-03, -3.7625e-02,\n",
       "                      -7.8757e-02, -2.4015e-02, -4.0768e-02,  2.0637e-02, -4.4117e-02,\n",
       "                      -1.3559e-02, -1.3496e-03, -3.1382e-02, -9.3571e-02, -3.0159e-02,\n",
       "                      -9.8209e-02, -1.4692e-02, -1.2341e-02, -9.9024e-02, -4.2000e-02,\n",
       "                      -9.6266e-02, -1.6618e-02, -8.3845e-02,  2.7578e-03, -6.5137e-03,\n",
       "                      -3.1561e-02,  1.7744e-02, -1.0863e-01, -3.7430e-02, -6.4046e-02,\n",
       "                       1.1072e-02, -7.3926e-02, -7.8130e-02, -7.4840e-02,  2.7229e-02,\n",
       "                      -1.2302e-02,  3.7348e-02,  8.3356e-03, -6.7129e-02,  3.0840e-02,\n",
       "                      -4.2139e-02, -4.5525e-02, -3.3429e-02,  6.5453e-03, -1.6996e-02,\n",
       "                      -3.6901e-02, -5.4251e-02,  6.1729e-03, -6.0383e-02,  2.2698e-02,\n",
       "                      -9.6628e-02, -3.8484e-02, -1.1336e-02, -6.3147e-02, -5.2488e-02,\n",
       "                      -5.7440e-02,  1.2022e-02, -8.5095e-02,  7.6376e-03, -6.6384e-02,\n",
       "                       1.5107e-02,  1.3446e-02, -8.3307e-02,  2.6490e-02, -7.3903e-02,\n",
       "                      -4.9945e-02, -9.1474e-02,  4.8545e-02, -1.0187e-01,  1.3994e-02,\n",
       "                       1.8889e-02,  5.2637e-02, -2.9178e-02, -4.4465e-02, -2.2044e-02,\n",
       "                      -5.0357e-02, -1.8183e-02, -4.5190e-02, -1.5034e-02, -6.8886e-02,\n",
       "                       3.1544e-02, -5.4881e-02, -2.3262e-02, -5.9065e-02,  5.1520e-02,\n",
       "                      -9.1288e-02, -2.9557e-02, -4.8384e-02, -4.6235e-02,  1.1363e-02,\n",
       "                      -4.0965e-02, -8.5038e-02, -1.0493e-01, -2.0090e-02, -7.3295e-03,\n",
       "                      -6.9641e-02, -1.4278e-02, -3.3413e-02, -8.5066e-02, -4.1438e-02,\n",
       "                      -1.8684e-02,  2.5144e-03], device='cuda:0')),\n",
       "             ('base_ddg.pw_ffnn.2.weight',\n",
       "              tensor([[-0.0156, -0.0518, -0.0645,  ...,  0.0122,  0.0078, -0.0432],\n",
       "                      [ 0.0277,  0.0392,  0.0117,  ...,  0.0002, -0.0392,  0.0419],\n",
       "                      [ 0.0489,  0.0321, -0.0836,  ...,  0.0438,  0.0355, -0.0273],\n",
       "                      ...,\n",
       "                      [ 0.0280,  0.0496, -0.0289,  ..., -0.0077,  0.0246,  0.0053],\n",
       "                      [ 0.0588,  0.0187, -0.0299,  ..., -0.0264,  0.0475, -0.0338],\n",
       "                      [ 0.0185,  0.0319, -0.0966,  ..., -0.0130,  0.0024, -0.0425]],\n",
       "                     device='cuda:0')),\n",
       "             ('base_ddg.pw_ffnn.2.bias',\n",
       "              tensor([ 0.0226, -0.0022, -0.0196, -0.0725, -0.0222,  0.0745,  0.0108, -0.0061,\n",
       "                      -0.0201, -0.0415, -0.0514, -0.1098, -0.0440, -0.0238, -0.0009,  0.0368,\n",
       "                       0.0097, -0.0337, -0.0552, -0.0520, -0.0204,  0.0086, -0.0571,  0.0034,\n",
       "                      -0.0384, -0.0207, -0.0457, -0.0724, -0.0058, -0.0104, -0.0569, -0.0014,\n",
       "                       0.0157, -0.0123,  0.0436, -0.0389, -0.0341,  0.0224, -0.0527, -0.0566,\n",
       "                      -0.0380, -0.0166, -0.0361, -0.0853, -0.0626, -0.0272, -0.0246, -0.0141,\n",
       "                       0.0129,  0.0206, -0.0421, -0.0140,  0.0158, -0.0580, -0.0271, -0.0134,\n",
       "                       0.0065, -0.0084, -0.0264, -0.0210,  0.0039, -0.0442, -0.0008, -0.0222,\n",
       "                       0.0104, -0.0139, -0.0115, -0.0347, -0.0349,  0.0314, -0.0612, -0.0265,\n",
       "                       0.0272, -0.0370,  0.0272, -0.0257, -0.0362,  0.0147, -0.0109, -0.0288,\n",
       "                      -0.0412, -0.0319,  0.0469, -0.0355, -0.0461,  0.0012, -0.0666,  0.0639,\n",
       "                       0.0043,  0.0277, -0.0472, -0.0212,  0.0208, -0.0388, -0.0140, -0.0425,\n",
       "                       0.0123,  0.0101, -0.0459, -0.0164,  0.0247, -0.0242, -0.0123,  0.0176,\n",
       "                      -0.0467,  0.0945, -0.0748, -0.0492,  0.0211, -0.0463, -0.0484, -0.0045,\n",
       "                      -0.0276,  0.0105, -0.0415, -0.0296, -0.0422, -0.0021,  0.0028, -0.0340,\n",
       "                      -0.0256, -0.0162,  0.0212, -0.0830, -0.0171, -0.0506, -0.0546, -0.0136,\n",
       "                       0.1420,  0.1220,  0.1216,  0.1246,  0.1557,  0.0790,  0.1322, -0.0888,\n",
       "                       0.1368,  0.1069,  0.1427,  0.0972,  0.1622,  0.1594,  0.1294,  0.1137,\n",
       "                       0.0962,  0.2009,  0.1119,  0.1394, -0.0303,  0.0880,  0.1153,  0.1810,\n",
       "                       0.1655,  0.0858,  0.1455,  0.0798,  0.1035,  0.1153,  0.0805,  0.1599,\n",
       "                       0.0842,  0.0956,  0.1776,  0.1710,  0.1341,  0.1676,  0.1728,  0.0015,\n",
       "                       0.1663,  0.0825,  0.1415,  0.1413,  0.1665,  0.1515,  0.0979,  0.1130,\n",
       "                       0.1114,  0.1861,  0.1635,  0.1426,  0.1324,  0.1683,  0.1700,  0.1018,\n",
       "                       0.0975,  0.1253,  0.1616,  0.1364,  0.1164,  0.1478,  0.1829,  0.1154,\n",
       "                       0.1370,  0.1732,  0.1450,  0.1124, -0.0244,  0.1053,  0.1004,  0.1865,\n",
       "                      -0.0334,  0.0685,  0.0336,  0.1916,  0.1032,  0.1339,  0.2107,  0.1540,\n",
       "                       0.1032,  0.0806,  0.1825,  0.1785,  0.1480,  0.0664,  0.1158,  0.1112,\n",
       "                      -0.0782,  0.1914,  0.1656,  0.0662, -0.1440,  0.0897, -0.0789,  0.1014,\n",
       "                       0.0764,  0.1772,  0.0930,  0.1114, -0.0370,  0.0896, -0.0119,  0.1428,\n",
       "                       0.1489,  0.0731,  0.1449,  0.1188,  0.1668,  0.1321,  0.1495,  0.1337,\n",
       "                       0.1489,  0.1237,  0.1922,  0.1712,  0.1207,  0.1094,  0.1378,  0.1472,\n",
       "                       0.1000,  0.1616,  0.1305,  0.0726,  0.0914,  0.1658,  0.1580,  0.1225],\n",
       "                     device='cuda:0')),\n",
       "             ('base_ddg.Linear_ddg.weight',\n",
       "              tensor([[-1.7911e-02, -1.1736e-02,  7.4127e-03,  7.4715e-03,  2.7897e-02,\n",
       "                        3.0047e-02, -2.5351e-03, -1.8973e-02, -1.4472e-02, -2.7704e-02,\n",
       "                       -1.8384e-02, -9.5907e-03,  2.4626e-02,  1.7408e-02, -2.5693e-02,\n",
       "                        1.5090e-03,  2.8174e-02, -1.9899e-02, -2.2459e-02,  5.9494e-03,\n",
       "                       -3.2875e-02,  2.1753e-02, -4.6346e-02, -9.8494e-03, -6.3243e-04,\n",
       "                        3.8831e-02,  3.1363e-02, -2.8326e-02, -4.1070e-03,  2.1791e-02,\n",
       "                       -1.3926e-02,  2.0883e-02,  2.8159e-02, -6.7208e-03,  1.6132e-02,\n",
       "                       -7.6007e-03,  1.0575e-02, -2.3476e-03,  1.1788e-02,  9.5908e-03,\n",
       "                       -1.0112e-02,  4.6583e-03,  2.7304e-02, -5.0405e-03, -7.4416e-03,\n",
       "                        3.1373e-02,  1.1107e-02, -2.3703e-02, -1.8481e-02,  3.3495e-02,\n",
       "                       -1.7506e-02, -4.7582e-02,  1.1116e-02,  1.8076e-02, -4.6346e-03,\n",
       "                        3.6819e-02, -1.2175e-02,  8.9116e-03, -6.0163e-03,  7.2660e-03,\n",
       "                       -3.4185e-02,  1.8106e-02,  5.9013e-04, -3.4369e-02, -1.7349e-02,\n",
       "                        1.7957e-02,  1.2250e-02,  3.4616e-02,  1.1015e-02,  1.5333e-03,\n",
       "                       -2.9980e-03, -5.1842e-02,  1.2316e-02, -2.1647e-02,  3.4798e-03,\n",
       "                        4.3462e-03,  1.9537e-02,  3.8385e-02,  2.6025e-02,  1.2642e-02,\n",
       "                        1.1821e-02, -2.3624e-02,  2.0132e-02, -9.2351e-03,  9.9595e-03,\n",
       "                        1.0396e-02,  8.4716e-03, -3.1320e-02, -1.8615e-02,  1.0588e-02,\n",
       "                       -2.2265e-02, -3.2704e-02, -1.0530e-02,  2.8131e-02,  3.0475e-02,\n",
       "                       -2.2623e-02,  8.4954e-03, -4.5807e-02, -2.0848e-02,  4.4007e-02,\n",
       "                       -1.8709e-02,  1.0113e-02,  3.4766e-02, -2.6954e-02,  4.3437e-03,\n",
       "                        2.9472e-02, -1.6152e-02, -2.5075e-02,  1.2591e-02,  1.9612e-02,\n",
       "                       -1.8349e-02, -1.3978e-02, -4.0159e-02, -3.8193e-02,  6.7944e-03,\n",
       "                       -1.9283e-03, -4.1749e-03, -3.6830e-02,  1.8239e-02, -1.2848e-02,\n",
       "                        1.3786e-02, -1.4507e-02,  1.8858e-02, -2.8211e-02,  1.6722e-03,\n",
       "                       -2.6885e-02,  1.4577e-02, -2.1337e-02,  1.3589e-04,  3.5938e-04,\n",
       "                       -1.6448e-03,  6.1063e-04, -3.5513e-04,  8.4321e-04,  8.2086e-04,\n",
       "                        1.2034e-03, -4.2387e-04, -3.7825e-04,  6.0358e-04,  1.3400e-02,\n",
       "                        1.0749e-03,  7.4598e-05,  2.0728e-03,  6.7159e-04,  5.2063e-04,\n",
       "                        2.8738e-04, -6.9827e-04,  1.9465e-03,  1.6861e-02, -4.9979e-04,\n",
       "                        2.9250e-04, -1.2495e-04,  6.4561e-04, -2.8800e-05,  1.4938e-05,\n",
       "                        6.1017e-05, -8.8173e-04,  1.1949e-03,  3.9999e-04,  8.3725e-04,\n",
       "                        3.8417e-04, -5.9908e-04,  2.1036e-04,  2.9239e-04,  2.5527e-04,\n",
       "                        1.0564e-04, -3.6869e-04,  2.6290e-02,  7.8324e-04,  1.0063e-03,\n",
       "                        8.4504e-04,  6.7315e-04,  8.6843e-04,  4.2592e-06,  5.8217e-04,\n",
       "                        3.1296e-04, -1.9668e-04,  7.6884e-04, -2.0149e-03,  5.8411e-04,\n",
       "                        7.4818e-04,  5.3541e-04,  3.7405e-04, -2.8149e-04,  6.3819e-04,\n",
       "                        3.1156e-04,  8.4947e-04,  3.7144e-04,  1.4162e-04,  1.7395e-03,\n",
       "                        9.2304e-04,  6.2919e-06, -1.5151e-06,  2.1310e-03, -8.7280e-04,\n",
       "                        5.7676e-04,  5.4227e-04,  1.1705e-03,  9.0968e-04,  6.9937e-04,\n",
       "                        1.9479e-02,  1.9747e-03, -1.5539e-03,  2.2490e-04, -5.3239e-03,\n",
       "                       -3.2990e-04, -1.2964e-03,  7.3397e-04,  2.6716e-04, -4.6720e-04,\n",
       "                        5.1719e-04,  1.5548e-03,  1.1502e-03,  1.8204e-03, -2.5739e-04,\n",
       "                       -8.2380e-04,  1.6529e-03, -9.3929e-04,  5.0765e-03,  1.7683e-03,\n",
       "                       -2.6133e-03,  5.0660e-04, -1.2968e-02,  3.6352e-04, -5.6231e-05,\n",
       "                        3.4843e-04, -1.2034e-04,  7.2830e-03, -6.0991e-06, -7.4945e-04,\n",
       "                       -1.7334e-03,  3.0531e-03,  3.3908e-05,  1.2713e-04,  1.0009e-03,\n",
       "                        9.2431e-04,  2.4353e-04,  2.5585e-04, -1.2836e-03, -9.2905e-04,\n",
       "                       -6.2853e-04,  1.1011e-03, -2.6554e-04,  1.3167e-04, -1.5672e-04,\n",
       "                       -1.5639e-03,  7.6547e-04, -9.9892e-03,  1.0464e-03,  3.5527e-04,\n",
       "                        4.0604e-04,  1.5125e-03,  4.9743e-04, -1.0329e-03, -1.5877e-03,\n",
       "                        3.7077e-03,  4.1583e-02, -3.8447e-02, -6.6216e-03, -8.4016e-03,\n",
       "                       -2.9333e-02, -2.7191e-02,  4.4285e-02,  2.0076e-02,  1.7688e-02,\n",
       "                        3.3693e-02,  2.1384e-02,  1.1957e-02,  5.0818e-02, -1.1350e-02,\n",
       "                        5.3004e-02, -6.0443e-03, -2.7693e-02, -4.7687e-02,  2.0534e-02,\n",
       "                        3.4539e-02,  3.2696e-02, -4.4560e-02,  3.8700e-02,  3.5845e-02,\n",
       "                       -4.4304e-02, -5.5182e-02, -4.1687e-02,  3.2597e-02,  5.0747e-03,\n",
       "                       -2.8958e-02,  1.6653e-02, -2.9180e-02, -4.0025e-02,  1.6887e-02,\n",
       "                       -2.8209e-02, -1.8244e-03,  4.5961e-02,  2.5241e-03, -5.4479e-02,\n",
       "                       -1.0397e-02,  3.2915e-02,  3.9673e-02, -2.6555e-02,  8.2267e-03,\n",
       "                        3.4092e-03,  4.8138e-03, -2.4233e-02,  4.4291e-02,  5.7803e-02,\n",
       "                       -5.9767e-02,  2.3204e-02,  5.8811e-02, -2.3115e-02, -8.7989e-03,\n",
       "                        3.2844e-03, -4.3377e-02, -4.6714e-02, -1.9149e-03,  1.7825e-03,\n",
       "                       -1.0235e-02,  4.3944e-02, -6.3578e-02, -1.4926e-02,  2.1134e-02,\n",
       "                        1.6831e-02,  3.8285e-02, -2.0868e-02, -5.7213e-02, -1.3434e-02,\n",
       "                       -3.0878e-03,  3.4771e-03,  5.8310e-02, -1.0793e-02,  4.2075e-02,\n",
       "                       -3.5265e-03, -6.7484e-03, -1.8127e-02, -2.8215e-02, -3.2704e-02,\n",
       "                       -2.9133e-02, -1.5772e-02,  3.3671e-02, -1.9311e-02,  1.4063e-02,\n",
       "                       -1.4271e-02,  2.1508e-02, -1.2207e-02,  2.7996e-03,  2.3973e-02,\n",
       "                       -1.1253e-02,  5.0439e-02,  4.5421e-02,  2.4589e-02, -2.9415e-02,\n",
       "                       -3.7286e-02,  2.9984e-02, -1.3248e-02,  4.2481e-02,  1.9321e-02,\n",
       "                       -4.3948e-02,  2.1466e-02,  1.3527e-02, -5.3066e-02,  2.6661e-02,\n",
       "                       -3.9959e-03, -3.0302e-02,  1.3391e-02,  2.5499e-02, -3.6953e-03,\n",
       "                       -2.3011e-02, -5.4561e-02,  2.3477e-02,  7.4390e-02,  6.5777e-02,\n",
       "                       -4.5562e-03,  1.2434e-02, -5.2795e-02,  3.0663e-02,  7.4863e-02,\n",
       "                        1.5566e-02, -1.0732e-02,  2.2563e-02, -7.3296e-02,  3.0273e-02,\n",
       "                       -6.2115e-02,  3.3070e-02, -1.3921e-02, -3.2151e-02, -1.7664e-04,\n",
       "                       -2.2114e-05,  2.3593e-03, -6.7695e-04,  1.6169e-03, -3.5563e-04,\n",
       "                        4.8597e-04,  8.4616e-04, -3.1631e-04,  1.6882e-03, -1.6980e-04,\n",
       "                       -1.4096e-02, -1.6062e-04,  7.3059e-04, -6.8604e-04, -9.3148e-04,\n",
       "                        8.1448e-04,  1.0803e-03,  2.5473e-04, -2.1649e-03, -2.1627e-02,\n",
       "                        7.4206e-04,  4.4114e-04, -5.7071e-05,  5.6994e-04,  1.1404e-03,\n",
       "                        1.4452e-03,  1.0142e-03,  1.1585e-03,  7.7237e-04, -7.6178e-04,\n",
       "                        3.8256e-04, -1.2552e-03, -7.4913e-04,  1.4530e-03,  1.1679e-04,\n",
       "                        7.4889e-04,  1.4826e-04,  5.7865e-04, -3.0854e-02,  1.2709e-03,\n",
       "                       -8.3955e-04,  1.5121e-03, -1.4638e-03,  2.6181e-04, -1.1547e-05,\n",
       "                       -1.3653e-04, -1.2774e-04,  9.2071e-04,  2.1602e-03,  3.9978e-03,\n",
       "                       -1.0444e-04,  3.0211e-05,  1.5007e-04, -4.8244e-04, -8.7493e-04,\n",
       "                       -3.7574e-04,  8.1307e-05, -1.6029e-04,  1.4318e-03,  4.0864e-04,\n",
       "                       -5.8399e-04, -1.8785e-03, -1.1533e-05,  6.2989e-06, -8.3730e-04,\n",
       "                       -5.2254e-04, -3.1423e-04,  8.8833e-04, -1.2114e-04, -4.6183e-04,\n",
       "                        8.0313e-04, -2.4296e-02, -1.1440e-03,  9.5032e-04, -7.8690e-05,\n",
       "                        5.2130e-03,  1.3224e-03,  2.1739e-03, -1.2231e-03, -3.7528e-04,\n",
       "                       -4.3660e-04,  7.1767e-04, -1.3588e-03, -1.3011e-03, -8.9186e-04,\n",
       "                        1.1698e-03,  1.0652e-03,  3.6857e-03,  9.3803e-04, -5.1052e-03,\n",
       "                       -3.4541e-03,  5.6259e-05, -1.0678e-04,  1.3477e-02,  1.2833e-03,\n",
       "                        5.9349e-04, -4.0654e-04, -5.5936e-04, -8.9708e-03,  1.6306e-05,\n",
       "                        2.2329e-03,  1.7520e-03, -3.7674e-03,  7.0901e-04,  1.1956e-03,\n",
       "                        1.7653e-03, -3.7256e-04,  2.4475e-05,  2.4549e-04, -1.0562e-03,\n",
       "                        1.0597e-03,  1.5026e-03,  3.1032e-04,  1.3433e-03,  5.1663e-04,\n",
       "                        5.8182e-05,  1.4844e-03,  2.5361e-04,  7.4025e-03, -6.3746e-04,\n",
       "                       -7.0350e-04,  5.1148e-04, -4.7716e-04,  1.6134e-03,  1.0032e-03,\n",
       "                        1.3818e-03, -1.6747e-03]], device='cuda:0')),\n",
       "             ('base_ddg.Linear_ddg.bias', tensor([-0.0097], device='cuda:0'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46deecd2-d303-4290-ab70-12b8de5333b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##  MODELLO PER MULTIPLE\n",
    "#train_path =[f's2450_fold_{i}.pkl' for i in [0,1,2,3,4]]+[f's2450_fold_{i}_inv.pkl' for i in [0,1,2,3,4]] + ['ptmul_train.pkl']+['DA_s2450.pkl']#\n",
    "\n",
    "\n",
    "\n",
    "#PROVA A USARE SOLO DA_MULTIPLE_2450 DA FARE IL DATASET...\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "#JANUS300 EPOCHS\n",
    "\n",
    "# Epoch 1/300\n",
    "# Train -      Loss: 1.8881, Pearson r: 0.6380, Rho spearman: 0.6373\n",
    "# Validation - Loss: 2.3668, Pearson r: 0.5073, Rho spearman: 0.5271\n",
    "# Test -       Loss: 2.3662, Pearson r: 0.5075, Rho spearman: 0.5273\n",
    "\n",
    "# Epoch 2/300\n",
    "# Train -      Loss: 1.1301, Pearson r: 0.8035, Rho spearman: 0.7935\n",
    "# Validation - Loss: 1.9912, Pearson r: 0.5080, Rho spearman: 0.5103\n",
    "# Test -       Loss: 1.9932, Pearson r: 0.5073, Rho spearman: 0.5099\n",
    "\n",
    "# Epoch 3/300\n",
    "# Train -      Loss: 0.7244, Pearson r: 0.8791, Rho spearman: 0.8684\n",
    "# Validation - Loss: 1.9245, Pearson r: 0.5210, Rho spearman: 0.5209\n",
    "# Test -       Loss: 1.9255, Pearson r: 0.5208, Rho spearman: 0.5209\n",
    "\n",
    "\n",
    "#ARRIVA A 55\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "\n",
    "#s2450 +s2450inv+ ptmul_train : \n",
    "\n",
    "# Epoch 111/300\n",
    "# Train -      Loss: 0.0096, Pearson r: 0.9987, Rho spearman: 0.9984\n",
    "# Validation - Loss: 4.6144, Pearson r: 0.5621, Rho spearman: 0.5345\n",
    "# Test -       Loss: 1.9337, Pearson r: 0.5208, Rho spearman: 0.5338\n",
    "\n",
    "# Epoch 112/300\n",
    "# Train -      Loss: 0.0107, Pearson r: 0.9985, Rho spearman: 0.9983\n",
    "# Validation - Loss: 4.6020, Pearson r: 0.5630, Rho spearman: 0.5403\n",
    "# Test -       Loss: 1.9419, Pearson r: 0.5208, Rho spearman: 0.5367\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "train con hydra\n",
    "# Epoch 145/300\n",
    "# Train -      Loss: 0.0034, Pearson r: 0.9995, Rho spearman: 0.9995\n",
    "# Validation - Loss: 0.9176, Pearson r: 0.8205, Rho spearman: 0.8206\n",
    "# Test -       Loss: 2.0290, Pearson r: 0.5171, Rho spearman: 0.5325\n",
    "\n",
    "# Epoch 146/300\n",
    "# Train -      Loss: 0.0040, Pearson r: 0.9994, Rho spearman: 0.9994\n",
    "# Validation - Loss: 0.9274, Pearson r: 0.8196, Rho spearman: 0.8189\n",
    "# Test -       Loss: 2.0288, Pearson r: 0.5173, Rho spearman: 0.5318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fa01826-76f9-4c6f-9eb7-175e59270e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Final_model, 'JanusDDG_300epochs_ARXIVE.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38af44d-f819-46de-a972-540806475d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(best_model, 'DDGemb_Cross_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872aaa6-d7ed-4792-99a0-faf83b6c2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(Final_model, 'JanusDDG_300epochs_plus25_hydra_slim.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56dfd40-edf3-46fb-a967-1be1994cb631",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Final_model, 'JanusDDG_300epochs_plus25_hydra_slim_plus20_double.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac2fb1-c821-489e-850f-e51b6659dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(Final_model, 'JanusDDG_300_all_train.pth') #window 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64401710-5d96-4a70-9a7f-de99a1a36395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(Final_model, 'CHECKJANUS_multiple.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02337b38-b682-44fd-a284-d0a3344460a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3542a-d235-4b44-ad6f-5ca161d01859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Lista dei file dei modelli salvati\n",
    "model_paths = [f'JanusDDG_{epoch}_ensamble.pth' for epoch in range(100, 301,50)]\n",
    "\n",
    "# Carica gli state_dict dei modelli\n",
    "state_dicts = [torch.load(path).state_dict() for path in model_paths]\n",
    "\n",
    "# Crea un nuovo state_dict per il modello mediato\n",
    "avg_state_dict = {}\n",
    "\n",
    "# Itera su tutti i parametri del modello\n",
    "for key in state_dicts[0]:  # Prendi le chiavi dal primo modello\n",
    "    avg_state_dict[key] = sum(d[key] for d in state_dicts) / len(state_dicts)\n",
    "\n",
    "# Carica i pesi mediati in un nuovo modello\n",
    "final_model = torch.load(model_paths[0])  # Carica uno dei modelli per l'architettura\n",
    "final_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "# Salva il modello mediato\n",
    "torch.save(final_model, \"JanusDDG_avg_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6713aa-f17c-4810-8192-6d1f7dd46fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "con \n",
    "\n",
    "#SENZA RELU NELLA CONV1D\n",
    "# lr = 1e-4\n",
    "# input_dim = 1280\n",
    "\n",
    "# transf_parameters={'input_dim':1280, 'num_heads':8,\n",
    "#                     'dropout_rate':0.,}\n",
    "# arrivo a 0.54  (dopo un po' meno di 200 epoche)\n",
    "\n",
    "\n",
    "#     def __init__(self, input_dim=1280, num_heads=8, dropout_rate=0., num_experts=1, f_activation = nn.ReLU(), kernel_size=15, cross_att = False,\n",
    "#                 dual_cross_att=False):\n",
    "        \n",
    "#         super(TransformerRegression, self).__init__()\n",
    "#         self.cross_att = cross_att\n",
    "#         self.dual_cross_att = dual_cross_att\n",
    "        \n",
    "#         print(f'Cross Attention: {cross_att}')\n",
    "#         print(f'Dual Cross Attention: {dual_cross_att}')\n",
    "\n",
    "#         self.embedding_dim = input_dim\n",
    "#         self.act = f_activation\n",
    "#         self.max_len = 3700 #lunghezza massima proteina\n",
    "#         out_channels = 128  #num filtri conv 1D\n",
    "#         kernel_size = 20\n",
    "#         padding = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae41c28-e7e6-4c53-ab9c-bb12e35e4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"JanusDDG_loss_train.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(l_tr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342216a-bd92-4b60-ba85-de5a6a321fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=False)\n",
    "\n",
    "# Primo sottografico: Pearson r per il set di Train\n",
    "sns.lineplot(data=p_tr, ax=axes[0], label='Train r')\n",
    "#sns.lineplot(data=p_val, ax=axes[0], label='Test r')\n",
    "sns.lineplot(data=p_te, ax=axes[0], label='Test r')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Pearson r Values for Train and Test Set')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Pearson r')\n",
    "#axes[0].text(10, 0.53, str(round(pearson_max_val,3)), fontsize=12, color='red')\n",
    "axes[0].text(10, 0.53, str(round(max(p_te),3)), fontsize=12, color='red')\n",
    "\n",
    "axes[0].axhline(y=0.545, color='r', linestyle='--', linewidth=2)\n",
    "\n",
    "# Secondo sottografico: Pearson r per il set di Test\n",
    "sns.lineplot(data=l_tr, ax=axes[1], label='Train Loss')\n",
    "sns.lineplot(data=l_te, ax=axes[1], label='Test Loss')\n",
    "\n",
    "axes[1].legend()\n",
    "axes[1].set_title('Loss Values for Train and Test Set')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Huber Loss')\n",
    "\n",
    "# Imposta il titolo generale per la figura\n",
    "fig.suptitle('JanusDDG Pearson r Values for Train and Test Sets', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73b1d9-b5b4-497e-baa4-af130826e9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74861c-0515-4b8f-82bb-51ab1d153b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08a31a-a4a5-4768-873b-3b59bdaf3acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf6c16-ab15-4ef7-9fbf-a446c602f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b2738-25e5-4dd5-b9dd-1119798ef5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred_dir, pred_inv, true):\n",
    "\n",
    "    #Dirette\n",
    "    print(f'Pearson test dirette: {pearsonr(true,pred_dir)[0]}')   \n",
    "    print(f'Spearmanr test dirette: {spearmanr(true,pred_dir)[0]}')    \n",
    "    print(f'RMSE dirette: {root_mean_squared_error(true,pred_dir)}')\n",
    "    print(f'MAE dirette: {mean_absolute_error(true,pred_dir)}\\n')\n",
    "    \n",
    "    #Inverse\n",
    "    print(f'Pearson test inverse: {pearsonr(-true,pred_inv)[0]}')   \n",
    "    print(f'Spearmanr test inverse: {spearmanr(-true,pred_inv)[0]}')    \n",
    "    print(f'RMSE inverse: {root_mean_squared_error(-true,pred_inv)}')\n",
    "    print(f'MAE inverse: {mean_absolute_error(-true,pred_inv)}\\n')\n",
    "    #Tot\n",
    "    \n",
    "    print(f'Pearson test tot: {pearsonr(pd.concat([true,-true],axis=0),pd.concat([pred_dir,pred_inv],axis=0))[0]}')   \n",
    "    print(f'Spearmanr test tot: {spearmanr(pd.concat([true,-true],axis=0),pd.concat([pred_dir,pred_inv],axis=0))[0]}')    \n",
    "    print(f'RMSE tot: {root_mean_squared_error(pd.concat([true,-true],axis=0),pd.concat([pred_dir,pred_inv],axis=0))}')\n",
    "    print(f'MAE tot: {mean_absolute_error(pd.concat([true,-true],axis=0),pd.concat([pred_dir,pred_inv],axis=0))}\\n')\n",
    "    \n",
    "    print(f'PCC d-r: {pearsonr(pred_dir,pred_inv)}\\n')\n",
    "    print(f'anti-symmetry bias: {np.mean(pred_dir + pred_inv)}\\n-----------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0bdcdc-5cbf-44fd-ac0f-5de90b55c836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42ae80-6c55-4d6e-a362-24d5af1bb279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def output_model_from_batch_inv(batch, model, device, train=True):\n",
    "\n",
    "#     '''Dato un modello pytorch e batch restituisce: output_modello, True labels'''\n",
    "    \n",
    "#     x_wild = batch['mut_type'].float().to(device)\n",
    "#     x_mut = batch['wild_type'].float().to(device)\n",
    "#     labels = -batch['ddg'].float().to(device)\n",
    "#     length = batch['length'].to(device)\n",
    "#     output_ddg = model(x_wild, x_mut, length, train = train)\n",
    "    \n",
    "#     return output_ddg, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb6056-ecb1-4295-a449-4be5219c5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_generation_pred(E_TYPE, test_path, batch_size = 128, dataloader_shuffle = True, inv= False):\n",
    "    \n",
    "    EMBEDDING_TYPE = E_TYPE\n",
    "    \n",
    "    if EMBEDDING_TYPE == 'ESM2':\n",
    "\n",
    "        '''train formato da s2648 + UnionV e DA; 1000 dei DA sono usati nel validation insieme a s669 DA\n",
    "        '''\n",
    "        \n",
    "        dim_embedding = 1280\n",
    "        \n",
    "        dataset_test = []\n",
    "        \n",
    "        for path in test_path:           \n",
    "            with open(path, 'rb') as f:\n",
    "                dataset_test += pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    dataset_test = DeltaDataset(dataset_test, dim_embedding, inv = inv)\n",
    "    \n",
    "    # Creazione DataLoader\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=dataloader_shuffle, collate_fn=collate_fn)#collate_fn_MULTIPLE\n",
    "\n",
    "    return dataloader_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138fa9c3-c6ac-49bb-ade2-f999a1d8fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def collate_fn_MULTIPLE(batch):\n",
    "#     max_len = max(sample['wild_type'].shape[0] for sample in batch)  # Max sequence length in batch   700\n",
    "#     max_features = max(sample['wild_type'].shape[1] for sample in batch)  # Max feature size\n",
    "\n",
    "#     padded_batch = {\n",
    "#         'id': [],\n",
    "#         'wild_type': [],\n",
    "#         'mut_type': [],\n",
    "#         'length': [],\n",
    "#         'ddg': [],\n",
    "#         #'alpha_vec': [],\n",
    "#         'pos_mut': []\n",
    "#     }\n",
    "\n",
    "#     for sample in batch:\n",
    "#         wild_type_padded = F.pad(sample['wild_type'], (0, max_features - sample['wild_type'].shape[1], \n",
    "#                                                        0, max_len - sample['wild_type'].shape[0]))\n",
    "#         mut_type_padded = F.pad(sample['mut_type'], (0, max_features - sample['mut_type'].shape[1], \n",
    "#                                                      0, max_len - sample['mut_type'].shape[0]))\n",
    "\n",
    "#         padded_batch['id'].append(sample['id'])  \n",
    "#         padded_batch['wild_type'].append(wild_type_padded)  \n",
    "#         padded_batch['mut_type'].append(mut_type_padded)  \n",
    "#         padded_batch['length'].append(sample['length'])#append(torch.tensor(sample['length'], dtype=torch.float32))  \n",
    "#         padded_batch['ddg'].append(sample['ddg'])#append(torch.tensor(float(sample['ddg']), dtype=torch.float32))  \n",
    "#         #padded_batch['alpha_vec'].append(sample['alpha_vec'])#append(torch.tensor(sample['alpha_vec'], dtype=torch.float32))  \n",
    "#         #padded_batch['pos_mut'].append(sample['pos_mut'])#append(torch.tensor(sample['pos_mut'], dtype=torch.int64))  \n",
    "\n",
    "#     # Convert list of tensors into a single batch tensor\n",
    "#     padded_batch['wild_type'] = torch.stack(padded_batch['wild_type'])  # Shape: (batch_size, max_len, max_features)\n",
    "#     padded_batch['mut_type'] = torch.stack(padded_batch['mut_type'])  \n",
    "#     padded_batch['length'] = torch.stack(padded_batch['length'])  \n",
    "#     padded_batch['ddg'] = torch.stack(padded_batch['ddg'])  \n",
    "#     #padded_batch['alpha_vec'] = torch.stack(padded_batch['alpha_vec'])  \n",
    "#     #padded_batch['pos_mut'] = torch.stack(padded_batch['pos_mut'])  \n",
    "\n",
    "#     return padded_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf636760-6707-4522-b204-9bdadcb29efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "E_TYPE = 'ESM2'\n",
    "dataloader_test = dataloader_generation_pred(E_TYPE,  test_path=['s669_Castrense.pkl'],  batch_size = 6, dataloader_shuffle = False, inv= False)\n",
    "#['s669_Castrense.pkl']#'PTMUL_D.pkl'\n",
    "#'ddg_S2648_ESM2_ALL_LENGTH.pkl' #'s2450_fold_4.pkl' \n",
    "#['../DeltaDelta_BELLO/cdna117k_fold_1.pkl'] + ['../DeltaDelta_BELLO/cdna117k_fold_2.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2869da-2d53-4419-b0df-5417521bf81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def model_performance_test(model, dataloader_test, inv= False, train = False):\n",
    "    # Assicurati che il modello sia in modalità di valutazione\n",
    "    model.eval()\n",
    "    \n",
    "    # Lista per salvare tutte le predizioni\n",
    "    all_predictions_test = []\n",
    "    all_lables_test = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "       \n",
    "        for i, batch in enumerate(dataloader_test):\n",
    "\n",
    "            predictions_test, labels_test=output_model_from_batch(batch, model, device, train=False)\n",
    "\n",
    "            # Aggiungi le predizioni alla lista\n",
    "            all_predictions_test.append(predictions_test)\n",
    "            all_lables_test.append(labels_test)\n",
    "\n",
    "    return all_predictions_test, all_lables_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1677da-b1ba-41cc-968e-557714b358f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "input_dim = 1280\n",
    "\n",
    "transf_parameters={'input_dim':1280, 'num_heads':8,\n",
    "                    'dropout_rate':0.,}\n",
    "i=4\n",
    "best_model = torch.load('JanusDDG_300epochs.pth')#(f'JanusDDG_300epochs.pth')\n",
    "#torch.load(f'DDGemb_Cross_4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e296cf-51ce-421c-ab1b-fe6fc12a0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test, all_lables_test = model_performance_test(best_model,dataloader_test,\n",
    "                                                          inv=False, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee06d0-a0e6-4327-b20a-1bbc4ecddcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pearson test dirette: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), -torch.cat(all_lables_test, dim=0).cpu())}')   \n",
    "print(f'RMSE dirette: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(),- torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(),- torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee49360-e73b-4945-8f48-a3dc7addce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(torch.cat(all_predictions_test, dim=0).cpu()[:669],torch.cat(all_predictions_test, dim=0).cpu()[669:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ae410-5d3c-4e99-a754-775fdf4c9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.cat(all_predictions_test, dim=0).cpu()[:669] + torch.cat(all_predictions_test, dim=0).cpu()[669:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682cfa5-0193-46d4-8092-398a900d5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(torch.cat(all_predictions_test, dim=0).cpu()).to_pickle('DDGemb_cross_0_predictions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b188b42b-b44c-40c2-b63b-5ddd5742d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('indici_ordinati_s669.pkl').sort_values(by='index_castrense')['DDG'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8fd86-2002-4008-83c4-71e242fb371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indici_ordinati = pd.read_pickle('indici_ordinati_s669.pkl').sort_values(by='index_castrense')['index'].values\n",
    "pythia_s669 = pd.read_csv('../DeltaDeltaG/pythia_s669.csv').iloc[indici_ordinati,:]['Pythia_inv']\n",
    "info_mut = pd.read_csv('../DeltaDeltaG/pythia_s669.csv').iloc[indici_ordinati,:][['wildtype','mutation']]\n",
    "pred_janus= np.array(torch.cat(all_predictions_test, dim=0).cpu())\n",
    "true_ddg = pd.read_pickle('indici_ordinati_s669.pkl').sort_values(by='index_castrense')['DDG'].values#np.array(torch.cat(all_lables_test, dim=0).cpu())\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'wild':info_mut.iloc[:,0].values,\n",
    "    'mut':info_mut.iloc[:,1].values,\n",
    "    'pythia_s669': pythia_s669.values,  # Deve avere la stessa lunghezza di predictions\n",
    "    'predictions': pred_janus,  # Ogni riga avrà un valore scalare o un array se multidimensionale\n",
    "    'true_ddg':true_ddg\n",
    "})\n",
    "df.index = indici_ordinati\n",
    "print('Pearson s669: ', pearsonr(df['predictions'],df['true_ddg']))   \n",
    "\n",
    "\n",
    "indici_ordinati_s461  = pd.read_pickle('indici_ordinati_s669.pkl').dropna(subset='s461_pdb').sort_values(by='index_castrense')['index'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a56c97-8744-4a04-bdc9-be2ce4820a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0dbdd-0575-4644-9e16-54e5365361fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rose_csv = pd.read_csv('rose1985.csv')\n",
    "\n",
    "def rose_score(row):\n",
    "    wild_rose = rose_csv[rose_csv['Parameter']==row['wild']]['Rose1985'].values[0]\n",
    "    mut_rose = rose_csv[rose_csv['Parameter']==row['mut']]['Rose1985'].values[0]\n",
    "    return wild_rose - mut_rose\n",
    "    \n",
    "df['Rose'] = df.apply(rose_score,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f42c7-e87f-476d-9e68-1962b73fd0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "\n",
    "lr.fit(df[['pythia_s669','Rose']].values,df['true_ddg'].values)\n",
    "pearsonr(lr.predict(df[['pythia_s669','Rose']].values),df['true_ddg'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17326be-596b-4791-a894-0d1ffb5e188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(df['pythia_s669'].values * 0.11 - df['Rose'].values*0.0088,df['true_ddg'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5c78a-0e44-41b1-a867-993aeb12fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s461 = df.loc[indici_ordinati_s461,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6214df-0511-4eb9-bda5-693e62d6c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pearsonr(df_s461['pythia_s669']*0.11-df_s461['Rose']*0.0088 ,df_s461['true_ddg']))   \n",
    "print(pearsonr(df_s461['predictions'],df_s461['true_ddg']))   \n",
    "print('RMSE: ', root_mean_squared_error(df_s461['pythia_s669'],df_s461['true_ddg']))\n",
    "print('MAE: ', mean_squared_error(df_s461['pythia_s669'],df_s461['true_ddg']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac82868-6850-48b1-8658-ce454ecac9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869cf8c-309d-447e-9fe3-d9ff8aa1f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pearsonr(df_s461['predictions'],df_s461['true_ddg']))   \n",
    "print(pearsonr(df['predictions'],df['true_ddg']))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e10514-d351-4bf0-afae-6504b5aa23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pearson test dirette: {pearsonr(-0.07416111+ pythia_s669.values*0.09+np.array(torch.cat(all_predictions_test, dim=0).cpu()*0.57), torch.cat(all_lables_test, dim=0).cpu())}')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000aaa46-0caf-43fd-b2c6-ac151c5259a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(torch.cat(all_predictions_test, dim=0).cpu()).to_pickle('Janus_s669_fake.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f99bb4e-193a-4ec3-ab3a-60ffdcd8432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = torch.cat(all_predictions_test, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4fe39f-7e66-4fb2-a3c0-ccbd739828b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv = torch.cat(all_predictions_test, dim=0).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d582d9-99a7-432c-adf4-c35abd6c78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot =(dir-inv) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be3358-4f22-4040-8554-79546503f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec72677-0da4-4bf7-9d6b-5e47a2f2cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics( pd.DataFrame(tot),  pd.DataFrame(tot), pd.DataFrame(torch.cat(all_lables_test, dim=0).cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec5b3b-2bcb-4d82-8af0-b0433820646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_inv = all_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff7662-74e9-42df-b80a-82cde66052b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dirette = all_predictions_test\n",
    "true = all_lables_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b476662-dd1c-4e3a-ab34-e3af2c2a45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "metrics( pd.DataFrame(torch.cat(pred_dirette, dim=0).cpu()), pd.DataFrame(torch.cat(pred_inv, dim=0).cpu()), pd.DataFrame(torch.cat(true, dim=0).cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a9d69-6ba4-43e4-ae60-f92bbeafb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "50+49+53+51+51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdbec9-4b21-4531-ba01-8934204c449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    val_Set = [0,1,2,3,4]\n",
    "    val_Set.remove(i)\n",
    "    print(val_Set)\n",
    "    best_model = torch.load(f'DDGemb_Cross_{i}.pth')\n",
    "    \n",
    "    dataloader_val = dataloader_generation_pred(E_TYPE,  test_path=[f's2450_fold_{i}.pkl']+[f's2450_fold_{i}_inv.pkl'],\n",
    "                                                 batch_size = 6, dataloader_shuffle = False, inv= False)\n",
    "    \n",
    "    all_predictions_val, all_lables_val = model_performance_test(best_model,dataloader_val,\n",
    "                                                              inv=False, train=False)\n",
    "    \n",
    "    \n",
    "    print(f'Pearson test dirette: {pearsonr(torch.cat(all_predictions_val, dim=0).cpu(), torch.cat(all_lables_val, dim=0).cpu())}')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa390cb-681a-4d5a-82cb-46cc2fa384bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecfb7c-e7f0-459e-b576-5e6b0d9d0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesi = np.array([0.2,0.21,0.2,0.19,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abdfd3-0129-4413-8aa0-bb5e72467c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tot = []\n",
    "\n",
    "for i in range(5):\n",
    "    best_model = torch.load(f'DDGemb_Cross_{i}.pth')\n",
    "    \n",
    "    all_predictions_test, all_lables_test = model_performance_test(best_model,dataloader_test,\n",
    "                                                          inv=False, train=False)\n",
    "    \n",
    "    pred_tot.append(pd.Series(torch.cat(all_predictions_test, dim=0).cpu()))#.to_pickle('DDGemb_Cross_0_predictions_s669.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765fbd4-053c-412f-9190-a991bb5b2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_inv = pred_tot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9876e-b6e6-4fba-b529-efc556d6cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = pred_tot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6205aa5-01dc-4431-b1bc-535f0ab9cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3707d1d-d3a7-465a-932d-d50fd9580a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "somma_pesata_dir = sum(w * v for w, v in zip(pesi, pred_tot))\n",
    "\n",
    "print(somma_pesata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274aa870-2e17-4408-8b37-e7dced6d6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492fc24c-9dc2-44c1-bc3a-23c8a675d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "somma_pesata_inv = sum(w * v for w, v in zip(pesi, pred_inv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddc96f-7db1-4cee-b761-8145de71a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pred_tot[0]+pred_tot[1]+pred_tot[2]+pred_tot[3]+pred_tot[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b87d4-0dbb-4384-bb30-20f1711a37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "print(f'Pearson test dirette: {pearsonr(somma_pesata, torch.cat(all_lables_test, dim=0).cpu())}')   \n",
    "print(f'Spearmanr test dirette: {spearmanr(somma_pesata, torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(somma_pesata, torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(somma_pesata, torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21931a-b3bf-4369-840f-a054a051eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "somma_pesata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fe318-ef98-40d8-8070-897967820ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "somma_pesata_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e1404-8d4c-4849-abda-7df1c6045fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(pd.DataFrame(somma_pesata_dir),pd.DataFrame(somma_pesata_inv),-pd.DataFrame(torch.cat(all_lables_test, dim=0).cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab21d1-dd90-40de-8372-bba1ac3d6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([386,342,187,497,105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2c633-7cea-4235-870c-277482660be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793be0cd-e2d2-4936-914b-d5760adddabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(f'DDGemb_Cross_4.pth')\n",
    "all_predictions_test, all_lables_test = model_performance(best_model,dataloader_test,\n",
    "                                                          dataloader_train=None,dataloader_validation=None,inv=True, train=False)\n",
    "\n",
    "pred_tot.append(pd.Series(torch.cat(all_predictions_test, dim=0).cpu()))#.to_pickle('DDGemb_Cross_0_predictions_s669.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b68981-6975-495f-b097-aebe0a727f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "print(f'Pearson test dirette: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')   \n",
    "print(f'Spearmanr test dirette: {spearmanr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f84dea-1498-489e-a115-e83651c553ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63ef67-190b-4822-8178-1a0592a753dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed1a72-757b-43a3-8783-29d054d95e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138de61-a8eb-449d-b187-818ba64fe978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295bed0-f130-4813-8eb5-06a966923c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce649183-a266-431c-833a-6d704e6e2a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455a826-dbe4-402e-97f0-c146853a0d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015cdf3e-44c1-4a1a-8a90-4eab7b2a147e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a015d-7710-42e8-b972-4fbdf49e4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOOTSTRAPPING\n",
    "\n",
    "results_naive=[]\n",
    "for i in range(1000):\n",
    "    dataloader_test = dataloader_generation_pred(E_TYPE,  test_path='ddg_s669_ESM2_HYDRA_LITE.pkl',  batch_size = 128, dataloader_shuffle = False, inv= False)\n",
    "\n",
    "    all_predictions_test, all_lables_test = model_performance(best_model,\n",
    "                                                                                                                                           dataloader_test,\n",
    "                                                                                                                                           dataloader_train=None,\n",
    "                                                                                                                                           dataloader_validation=None,         \n",
    "                                                                                                                                           inv=False,\n",
    "                                                                                                                                           train=True)\n",
    "    results_naive.append(pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())[0]) \n",
    "\n",
    "pd.DataFrame(results_naive).to_pickle('result_bootstrapping_MLP_27_01_2025.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869eca65-1ecd-4ae1-874e-96e50352be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('MLP_HYDRA_MEAN_29_01_2025.pth')#('MLP_HYDRA_MEAN_29_01_2025.pth')#'MLP_27_01_2025.pth')#('MPL_HYDRA_model_7W_256_256.pth')\n",
    "best_model.hydra=True\n",
    "results_hydra=[]\n",
    "for i in range(1000):\n",
    "    dataloader_test = dataloader_generation_pred(E_TYPE,  test_path='ddg_s669_ESM2_HYDRA_LITE.pkl',  batch_size = 128, dataloader_shuffle = False, inv= False)\n",
    "\n",
    "    all_predictions_test, all_lables_test = model_performance(best_model,\n",
    "                                                                                                                                           dataloader_test,\n",
    "                                                                                                                                           dataloader_train=None,\n",
    "                                                                                                                                           dataloader_validation=None,         \n",
    "                                                                                                                                           inv=False,\n",
    "                                                                                                                                           train=True)\n",
    "    results_hydra.append(pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())[0]) \n",
    "\n",
    "pd.DataFrame(results_hydra).to_pickle('result_bootstrapping_MLP_HYDRA_MEAN_29_01_2025.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b7fb2-6a9a-442a-8f83-332dc5f55ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(results_hydra,kde=True,color='blue')\n",
    "sns.histplot(results_naive,kde=True,color='red')\n",
    "from scipy.stats import ks_2samp\n",
    "ks_2samp(results_hydra,results_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c7802-9ca4-4097-85e7-685cf7901750",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(results_hydra,kde=True)\n",
    "sns.histplot(results_naive,kde=True)\n",
    "from scipy.stats import ks_2samp\n",
    "ks_2samp(results_hydra,results_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424bec2-19c5-4010-a42d-6222c6c20271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "print(f'Pearson test dirette: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')   \n",
    "print(f'Spearmanr test dirette: {spearmanr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3828a81b-024a-414b-8e5e-6ba90ee5075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(torch.cat(all_predictions_test, dim=0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a65ae-51b9-4f60-9316-9af940d97886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(torch.cat(all_predictions_train, dim=0).cpu()).to_pickle('MLP_HYDRA_7W_256_256_predictions_esm2_train.pkl')\n",
    "pd.Series(torch.cat(all_predictions_test, dim=0).cpu()).to_pickle('MLP_HYDRA_MEAN_29_01_2025.pkl')\n",
    "#pd.Series(torch.cat(all_predictions_validation, dim=0).cpu()).to_pickle('MLP_HYDRA_7W_256_256_predictions_esm2_val.pkl')\n",
    "# pd.Series(torch.cat(all_lables_train, dim=0).cpu()).to_pickle('MLP_all_lables_train.pkl')\n",
    "# pd.Series(torch.cat(all_lables_test, dim=0).cpu()).to_pickle('MLP_all_lables_test.pkl')\n",
    "# pd.Series(torch.cat(all_lables_validation, dim=0).cpu()).to_pickle('MLP_all_lables_val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e0ef4-d52f-4d60-8d75-e64b0f8e25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####NOMI \n",
    "#'MLP_HYDRA_7W_256_256_predictions_esm2_train.pkl'\n",
    "\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b33ed-c344-46dd-91ae-6055e8523dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(torch.cat(all_predictions_test, dim=0).cpu()).to_pickle('MLP_HYDRA_7W_256_256_predictions_Protherm_Doubles_pdbnum_2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8d7e1-f460-49ca-8210-45949461934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "print(f'Pearson test d-r: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_predictions_test_inv, dim=0).cpu())}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87dcd48-a7fd-473c-b1fe-e87ac8963672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "print(f'Pearson test dirette: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')   \n",
    "print(f'Spearmanr test dirette: {spearmanr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "\n",
    "\n",
    "# Pearson test dirette: PearsonRResult(statistic=0.49811210357961794, pvalue=3.0676935334080987e-43)\n",
    "# Pearson test dirette: SignificanceResult(statistic=0.5209411003368696, pvalue=8.229743816346383e-48)\n",
    "# RMSE dirette: 1.4306520223617554\n",
    "# MAE dirette: 1.0020039081573486\n",
    "\n",
    "# Pearson test dirette: PearsonRResult(statistic=0.4984754880369162, pvalue=2.6104865839652876e-43)\n",
    "# Pearson test dirette: SignificanceResult(statistic=0.5219817530960136, pvalue=4.996030095808217e-48)\n",
    "# RMSE dirette: 1.4335042238235474\n",
    "# MAE dirette: 1.0032145977020264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b8682-79ad-4a78-b29e-b7abe1e0c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pearson test dirette: {spearmanr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bba584-dbcf-4a89-9d90-7bc9d3aede00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pearson test inverse: {pearsonr(torch.cat(all_predictions_test_inv, dim=0).cpu(), torch.cat(all_lables_test_inv, dim=0).cpu())}')    \n",
    "print(f'RMSE inve: {root_mean_squared_error(torch.cat(all_predictions_test_inv, dim=0).cpu(), torch.cat(all_lables_test_inv, dim=0).cpu())}')\n",
    "print(f'MAE inve: {mean_absolute_error(torch.cat(all_predictions_test_inv, dim=0).cpu(), torch.cat(all_lables_test_inv, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c026492-0b77-4ae9-912c-35632ee628df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test.extend(all_predictions_test_inv)\n",
    "all_lables_test.extend(all_lables_test_inv)\n",
    "print(f'Pearson test dirette+inverse: {pearsonr(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')    \n",
    "print(f'RMSE dirette+inverse: {root_mean_squared_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')\n",
    "print(f'MAE dirette+inverse: {mean_absolute_error(torch.cat(all_predictions_test, dim=0).cpu(), torch.cat(all_lables_test, dim=0).cpu())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b1761-2941-44ed-95be-20772e977033",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286b45b-f9f4-4dd2-b2ea-60993c2c1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855b4db-5922-4b5d-a421-1a632f2b591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_esm2_train = pd.read_pickle('MLP_all_predictions_esm2_s2648.pkl')\n",
    "y_esm2_test = pd.read_pickle('MLP_all_predictions_esm2_s669.pkl')\n",
    "\n",
    "y_ohe_train = pd.read_pickle('MLP_all_predictions_ohe_s2648.pkl')\n",
    "y_ohe_test = pd.read_pickle('MLP_all_predictions_ohe_s669.pkl')\n",
    "\n",
    "y_true_test = pd.DataFrame(torch.cat(all_lables_test, dim=0).cpu())\n",
    "y_true_train = pd.DataFrame(torch.cat(all_lables_train, dim=0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f120e-4350-4b51-a81f-4af0feafad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train = pd.concat([y_esm2_train,y_ohe_train],axis=1)\n",
    "X_test = pd.concat([y_esm2_test,y_ohe_test],axis=1) \n",
    "y_train = y_true_train\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "prediction_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cacfa9-a935-458c-a693-bc147810073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lr.reshape(669).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3083a-a2a1-4043-9bdc-7a76079cd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a4524-38b8-4eb0-aa56-bff0006459de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assicurati che entrambe le variabili siano monodimensionali\n",
    "y_true_test_flat = y_true_test.values.ravel()  # Oppure .flatten()\n",
    "prediction_lr_flat = prediction_lr.reshape(669).ravel()  # Oppure np.squeeze()\n",
    "\n",
    "print(f'Pearson test dirette: {pearsonr(y_true_test_flat, prediction_lr_flat)}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(y_true_test_flat, prediction_lr_flat)}')  \n",
    "print(f'MAE dirette: {mean_absolute_error(y_true_test_flat, prediction_lr_flat)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0307e-7a1c-4b38-ab34-4210aa56dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pearson test dirette: {pearsonr(y_true_test_flat, X_test.iloc[:,0])}')    \n",
    "print(f'RMSE dirette: {root_mean_squared_error(y_true_test_flat, X_test.iloc[:,0])}')  \n",
    "print(f'MAE dirette: {mean_absolute_error(y_true_test_flat, X_test.iloc[:,0])}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610c903-b8b7-48ab-9c84-9d4130b997f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950ae10-31f9-47a1-aaec-de171423eca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c68ce-c0b7-4def-b967-8d3570324d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################FINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ddd4a-880f-4362-98d8-f5fccfbf1cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ddg_s2648_ESM2.pkl', 'rb') as f:\n",
    "    dataset_s2648 = pickle.load(f)\n",
    "\n",
    "with open('ddg_s669_ESM2.pkl', 'rb') as f:\n",
    "    dataset_s669 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1ae21-c521-4c46-9470-374043e613d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Creiamo il dataset\n",
    "dataset_train = ProteinDataset(dataset_s2648, threshold=5.0)\n",
    "dataset_test = ProteinDataset(dataset_s669, threshold=5.0)\n",
    "\n",
    "\n",
    "# Creazione di un DataLoader\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1d9c2-7b09-45a3-b47d-669c5c145ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_embedding_aa = {}\n",
    "ordine_aa = {}\n",
    "\n",
    "aa_a = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "ind_pos = 0\n",
    "for name_aa in [*aa_a]:\n",
    "    dic_embedding_aa[name_aa] = np.zeros(20)\n",
    "    dic_embedding_aa[name_aa][ind_pos] = 1\n",
    "\n",
    "    ordine_aa[name_aa] = ind_pos\n",
    "\n",
    "    ind_pos+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84332f-a5c7-4bb1-aede-ccf84b9e7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_letters_test = []\n",
    "info_letters_train = []\n",
    "info_mutation_test = []\n",
    "info_mutation_train = []\n",
    "\n",
    "for batch in dataloader_test:\n",
    "    info_letters_test = []\n",
    "    \n",
    "    for batch_i in range(len(batch['id'])):\n",
    "        vector_w = [1 if i == 1 else 0 for i in batch['V'][batch_i][:]]\n",
    "        vector_m = [1 if i == -1 else 0 for i in batch['V'][batch_i][:]]\n",
    "        letter_w = next((k for k, v in dic_embedding_aa.items() if (v == vector_w).all()), None)\n",
    "        letter_m = next((k for k, v in dic_embedding_aa.items() if (v == vector_m).all()), None)\n",
    "        letters = [letter_w + letter_m]\n",
    "        info_letters_test.extend(letters)\n",
    "\n",
    "    info_mutation_test.extend([id+l[0]+str(pos)+l[-1] for id, pos, l in zip(batch['id'],batch['position'].tolist(), info_letters_test)])\n",
    "    \n",
    "\n",
    "for batch in dataloader_train:\n",
    "    info_letters_train = []\n",
    "    for batch_i in range(len(batch['id'])):\n",
    "        vector_w = [1 if i == 1 else 0 for i in batch['V'][batch_i][:]]\n",
    "        vector_m = [1 if i == -1 else 0 for i in batch['V'][batch_i][:]]\n",
    "        letter_w = next((k for k, v in dic_embedding_aa.items() if (v == vector_w).all()), None)\n",
    "        letter_m = next((k for k, v in dic_embedding_aa.items() if (v == vector_m).all()), None)\n",
    "        letters = [letter_w + letter_m]\n",
    "        info_letters_train.extend(letters)\n",
    "\n",
    "    info_mutation_train.extend([id+l[0]+str(pos)+l[-1] for id, pos, l in zip(batch['id'],batch['position'].tolist(), info_letters_train)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab45e0f-fa6a-4086-935e-3938844ac5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(info_mutation_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c3c7f-3d36-4e11-a170-735f19109641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assicurati che il modello sia in modalità di valutazione\n",
    "DDG_model.eval()\n",
    "\n",
    "# Lista per salvare tutte le predizioni\n",
    "all_predictions_test = []\n",
    "all_predictions_train = []\n",
    "\n",
    "all_lables_train = []\n",
    "all_lables_test = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for i, batch in enumerate(dataloader_test):\n",
    "        graph_x = batch.x.float().to(device)   #wild_type\n",
    "        graph_edge_index = batch.edge_index.to(device)   #wild_type\n",
    "        graph_batch = batch.batch.to(device)   #wild_type\n",
    "        labels = batch.y.float().float().to(device)\n",
    "        position_wild = position_adj(batch.position, batch.ptr)\n",
    "        position_mut = position_mut_adj(batch.position, batch.ptr)\n",
    "        intervallo_wild = edge_index_wild(graph_x, batch.ptr)\n",
    "        intervallo_mut = edge_index_mut(graph_x, batch.ptr)\n",
    "        V = batch.V\n",
    "        edge_weights = batch.edge_weight\n",
    "        predictions = DDG_model(graph_x, graph_edge_index, labels,position_wild,position_mut, V,\n",
    "                        edge_weights = edge_weights,intervallo_wild= intervallo_wild,intervallo_mut =intervallo_mut)  \n",
    "            \n",
    "        all_predictions_test.append(predictions)\n",
    "        all_lables_test.append(labels)\n",
    "\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        graph_x = batch.x.float().to(device)   #wild_type\n",
    "        graph_edge_index = batch.edge_index.to(device)   #wild_type\n",
    "        graph_batch = batch.batch.to(device)   #wild_type\n",
    "        labels = batch.y.float().float().to(device)\n",
    "        position_wild = position_adj(batch.position, batch.ptr)\n",
    "        position_mut = position_mut_adj(batch.position, batch.ptr)\n",
    "        intervallo_wild = edge_index_wild(graph_x, batch.ptr)\n",
    "        intervallo_mut = edge_index_mut(graph_x, batch.ptr)\n",
    "        V = batch.V\n",
    "        edge_weights = batch.edge_weight\n",
    "        predictions = DDG_model(graph_x, graph_edge_index, labels,position_wild,position_mut, V,\n",
    "                        edge_weights = edge_weights,intervallo_wild= intervallo_wild,intervallo_mut =intervallo_mut) \n",
    "\n",
    "        \n",
    "        # Aggiungi le predizioni alla lista\n",
    "        all_predictions_train.append(predictions)\n",
    "        all_lables_train.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2fdd0e-7e05-4a49-8c1b-db7b289aabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test = torch.cat([i[0] for i in all_predictions_test], dim=0)\n",
    "all_predictions_train = torch.cat([i[0] for i in all_predictions_train], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf0cb5-633f-4d78-9876-0f7bdae04629",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_train_dir = all_predictions_train[:int(all_predictions_train.shape[0]/2)]\n",
    "all_predictions_train_inv = all_predictions_train[int(all_predictions_train.shape[0]/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f7404-755a-457e-a7da-12cdabf9ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test_dir = all_predictions_test[:int(all_predictions_test.shape[0]/2)]\n",
    "all_predictions_test_inv = all_predictions_test[int(all_predictions_test.shape[0]/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea3b27-42aa-4459-bfd2-442daa99c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(all_predictions_test_dir.cpu(),all_predictions_test_inv.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91955d77-5468-454e-9dd5-0243644751da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(all_predictions_train.cpu(),torch.cat(all_lables_train, dim=0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8279c-1a5a-4acf-9358-f7e7b2386ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_guido_test = pd.DataFrame(np.array([info_mutation_test, all_predictions_test.cpu().numpy()]).T)\n",
    "prediction_guido_train = pd.DataFrame(np.array([info_mutation_train, all_predictions_train.cpu().numpy()]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1eaf7-3939-49e9-a135-182b84877a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_guido_train.drop_duplicates(subset=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45bb2a-f484-415f-9cb0-8e8e7df58621",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_guido_train.iloc[:,1] = prediction_guido_train.iloc[:,1].map(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9603ed6-2d54-4c63-9b1d-13f439cd118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_guido_train.iloc[:,1] = -prediction_guido_train.iloc[:,1]\n",
    "prediction_guido_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1dfe35-a149-4670-893e-80f0b6528db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f229fc-9b26-47fc-9389-2e01966301ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_guido_test.to_pickle('prediction_guido_test.pkl')\n",
    "prediction_guido_train.to_pickle('prediction_guido_train.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3895da-089f-43c5-9c2c-33e3badf6a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../DeltaDeltaG/prediction_guido_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43239c-2d26-4eae-b139-18d5a1897470",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DDG_model.state_dict(), 'model_weights_pyhzia.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b4661-f10c-4b38-b79e-769d6b5a4763",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9d111-4bfa-4e3e-b486-5f2b42bd4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([float(d['ddg']) for d in dataset_s2648])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88b791-ef3b-4be2-84c9-b18ff69f337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c46748-a8fb-4a2c-93b8-3ac269b3c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(x=[float(d['ddg']) for d in dataset_s669] + all_predictions_test.tolist(),\n",
    "            hue=([\"true\"] * len(dataset_s669) + [\"pred\"] * len(dataset_s669)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27693c02-73ae-4ca2-ad48-2ab665a0e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(x=[float(d['ddg']) for d in dataset_s2648] + prediction_guido_train[1].tolist(),\n",
    "            hue=([\"true\"] * len(dataset_s2648) + [\"pred\"] * len(dataset_s2648)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255bc5e-eeb9-4be3-8849-dd588580165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction_guido_train[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4aa8a7-39d6-49e8-9301-5408400d0ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot([float(true['ddg']) for true in dataset_s669])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fab0ab-0458-4c09-ae1d-433d23fc8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(prediction_guido_test.iloc[:,1].values.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ffe30b-2757-4275-99d6-94ef547f3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "sns.scatterplot(y = prediction_guido_test.iloc[:,1].values.astype(float), x= [float(true['ddg']) for true in dataset_s669])\n",
    "sns.scatterplot(y =  [float(true['ddg']) for true in dataset_s669],x= [float(true['ddg']) for true in dataset_s669])\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Pred')\n",
    "plt.savefig('prova.pdf')\n",
    "\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbbdeb-f8d5-4f73-aeac-669adad74b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(prediction_guido_test.iloc[:,1].values.astype(float),[float(true['ddg']) for true in dataset_s669])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264a83d-b099-4fa5-9e02-3c4f6cec19e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
